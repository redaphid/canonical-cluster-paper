{
  "doc": {
    "logo": "images/interview-icon.png",
    "title": "Written Interview",
    "miro": "https://miro.com/app/board/uXjVMeJA43k=",
    "repo": "https://github.com/redaphid/resume-canonical",
    "intro": "#### Structure of this Document\nWhen responding to these prompts, I found answering them in a discrete fashion made the discussion too fragmentary at times. I have therefore organized some responses into descriptions of concrete solutions in my career that utilize the concepts in question. I provide detailed descriptions porkbarreled in to answering the questions as I encounter them, and refer to these descriptions in subsequent prompts. I have also included some diagrams to help visualize the concepts I am trying to communicate.\n\nI hope this format makes sense to you, though I imagine I won't be able to clarify anything, due to the anonymity of this process.\n\nIf you're interested in seeing how I think, looking at the commits in [this document's repo](https://github.com/redaphid/resume-canonical) is seeing me in my natural setting. I didn't plan on bringing attention to it until now.\n\nThe diagrams were made in Miro. [Here is the link to the board](https://miro.com/app/board/uXjVMeJA43k=)\n",
    "main": [
      {
        "title": "Identity and Authorization Technology",
        "prompts": [
          {
            "prompt": "Describe your experience with authorization systems, specifically Open Policy Agent and OAuth.",
            "answers": [
              {
                "response": "I have extensive, varied experience when it comes to authorization and authentication systems.\nI will give some brief bullet points of my experiences over time with various authorization systems, and then I will give some more detailed examples of my experience with Open Policy Agent and OAuth.\n\n* The earliest \"exotic\" auth system I encountered in my career was a strange one called [OACC](http://oaccframework.org). It is a open-source system created by a colleague, and I remember it to be something like RBAC, with some permission delegation mechanism.\n\n* Next, I worked for Unicon and used [Shiboboleth](https://www.shibboleth.net/) (a SAML-based identity provider)\n\n* After this, I began work on the open-source Internet of Things platform \"Meshblu\", and the closed source application that ran on top of it called \"Octoblu\". Octoblu was later acquired by Citrix.\n\n#### Experience with Meshblu\nMeshblu is capable of configuring and messaging hardware or software \"devices\" hardware being things like hue bulbs, or DIY Arduino projects, and software being things like web applications. My team inherited Meshblu with a [permission system](https://meshblu.readme.io/reference/whitelists) that was baked directly in to the json document that represented the configuration of the device. This system was a simple one - arrays of concrete uuids that are allowed to perform certain actions. It was only capable of dealing with concrete entities - no roles, groups, etc. My colleagues and I eventually replaced it with a [more flexible system](https://meshblu.readme.io/reference/whitelists-2-0) that is still very simple and easy to understand.\n\nIn short, it is a 2-dimensional \"permission matrix\", where the first dimension is the actions that can be taken on a device {broadcast, discover, configure, message}, and the second dimension usually has 2-3 permissions that were relevant to the action domain. Critically, we added an 'as' permission, allowed for impersonation, which makes groups possible without being baked in to the system.\n\nOn top of this, we have many microservices that exist in \"userland\" with respect to Meshblu; they services could implement different authorization systems on top of Meshblu if necessary or desired. For example, we had a microservice that translated between OAuth and our permission system.\n\nWe had many authn/authz systems of our own to do this permission translation. For example, a microcontroller may need to authenticate via MQTT, the protocol it is using to communicate with Meshblu. Often communications happened over http, and we allowed for basic auth if necessary. Since we had many protocols, I ended up having doing this translation a lot.\n\nWe had many microservices that would \"assume the identity\" of a device, and bridge the gap between Meshblu and other services, which often required the use of OAuth. In some cases, I remember having to do the \"OAuth handshake\" between 4 different oauth providers (including at least 1 of ours), though the details escape me at the moment.\n"
              },
              {
                "response": "Below describes some of my experience  with authentication and authorization systems. I will describe the architecture of an experimental cluster design to illustrate my knowledge and expertise in this problem domain. I know this is probably overkill, but bear with me!\n"
              },
              {
                "response": "#### The \"Totally Naive Services\" Cluster & Open Policy Agent Experience\nI later designed an experimental cluster to be used in environments with complex and unknown-to-us security requirements, potentially including governmental or military applications. The cluster would be deployed in both single-tenant, on-prem environments and multitenant cloud environments. The cluster design factored out multitenancy with respect to the services it contained; e.g. the underlying services did not have a concept of organizations, or really even users (from a security standpoint). I used Open Policy Agent to the point of absurdity when it came to this cluster design, with the expectation that we'd eventually hit a wall of impracticality after which the services inside the cluster would have to become more complex as they became more aware of their environment.\n\nThis approach to cluster development allowed us to build the cluster in parallel to the teams developing the services, and the development roadmaps of these services could be organized such that there was no wasted code if some of the more novel ideas did not work out.\n\n_aside: I wrote a wrote a Javascript poc that used OPA's ability to return a partially-solved abstract syntax tree from a policy, and then execute it using function composition with [Ramda](https://ramdajs.com/). Should you do this? No. But it was interesting!_\n\n#### Cluster Design from a Security Standpoint\nSeeing how I now have a captive audience (dear reader), I'll use the cluster as an object lesson in how I design things when in \"Research and Development mode\", while minimizing risk during the development of production code.\nI am recalling this from memory, and recreating the diagrams, so I may be wrong about some of the details.\n\nOne of the goals of this architecture was to separate all authentication and authorization concerns from the services themselves, and end up with a \"naive service\", which I will here define as:\n\n1. trusts that actions requested to be performed on a resource are allowed.\n1. trusts that any information requested is allowed to be returned.\n"
              },
              {
                "response": "At the highest level, the cluster is divided into 3 concentric circles of trust:\n"
              },
              {
                "response": "1. No trust (outside world)\n1. Authentication trust (within cluster)\n1. Authorization trust. \"Naive\" (within pod)\n",
                "figure": {
                  "figure": "naive-cluster-overview",
                  "miro": "3458764548628056239"
                }
              },
              {
                "response": "The barriers between these circles were enforced by OPA.\nOutside of the authentication barrier, we are in a no-trust environment.\nOnce we pass through the authentication barrier, communications between the services can trust that the request was from who it says it is.\n"
              },
              {
                "response": "#### Authentication Barrier\nAn OPA service was installed alongside Envoy at the ingress gateway of the cluster. OPA would evaluate the request after obtaining information necessary to reason about authentication legitimacy. This information could be provided as a result of an OIDC workflow, some kind of Active Directory authentication, or a variety of other authorization methods we supported via an adapter pattern described [later in this document](#authorization-adapters). This policy could also be a passthrough, and assume whatever security handshake that happened earlier in the request flow validated user identity correctly.\n\n_note: the authorization data in the diagram is separated logically from the request, but the information in practice would likely come from the request._\n",
                "figure": {
                  "figure": "authentication-barrier",
                  "miro": "3458764548747400083"
                }
              },
              {
                "response": "#### Authentication Workflow\nThis workflow diagram llustrates the flow of data through the authentication barrier. Note the 'Envoy' nodes represent the same Envoy service on the edge of the cluster.\n",
                "figure": {
                  "figure": "authentication-workflow",
                  "miro": "3458764548844659047"
                }
              },
              {
                "response": "In the initial implementation, I used Envoy to insert an \"Identity\" header into the request after the request was validated through whatever mechanism the client wanted to use. We offered a Keycloak instance as an IdP  (Identity Provider) with OIDC capabilities, but we also would accept OIDC credentials from a trusted 3rd party; or many other authentication mechanisms - we followed an adapter pattern for normalizing auth requirements and data, which I talk about [later on](#authorization-adapters).\n\nRegardless of the mechanism used, in the end an OPA policy always gated access to the cluster. The client's authentication data was evaluated against OPA policies, which had the final say. This, like all autoinjected systems in the cluster, it could be opted out of via configuration.\n\nThis implementation, however, has a potential vulnerability in that we would need to scrub an \"Identity\" header off of the incoming request. Since all requests were coming in through the ingress controller that Envoy was manipulating the headers from, in theory this wouldn't be a problem - but I would have rather encapsulated the request in an envelope larger than the request itself. I had learned with Meshblu that separation between data and metadata is very important, especially with regard to security.\n\nIn addition, this header could be manipulated by a malicious service within the cluster. The header solution was good enough for a proof of concept, but further research would be needed to create an envelope for the request that sufficiently separated \"userland\" requests and cluster metadata. Anyway, that's a mini self-audit of this implementation.\n\nPast this barrier, the services could trust that the request was from who it said it was, but they could not (yet) trust that the request was authorized to do what it was trying to do.\n"
              },
              {
                "response": "#### Authorization Barrier\nAfter the authentication barrier is passed, any subsequent OPA policy evaluation can assume that the identity presented to the policy for evaluation is accurate, which allows for simple, decoupled policies to govern authorization on a per-service pod basis. You may notice below the similarity between the cluster-protecting authentication barrier, and this authorization barrier.\n"
              },
              {
                "response": "An OPA service was injected into each Kubernetes pod, alongside Envoy. Work was being done on embedding OPA as a plugin to Envoy (also injected by default in Istio).\nAlso by default, the OPA service would only do the authorization gating on ingress traffic - though evolutions of this design allowed for OPA to filter egress traffic as well. But we'll get to that later.\n",
                "figure": {
                  "figure": "authorization-barrier",
                  "miro": "3458764548747400083"
                }
              },
              {
                "response": "#### Authorization Workflow\nThis diagram shows the flow of data through the authorization barrier. Note that all 'Envoy' nodes refer to the same service.\n\nAt this point, the request is coming from inside the cluster, with the rewritten identity header.\n",
                "figure": {
                  "figure": "authorization-workflow",
                  "miro": "3458764548844659047"
                }
              },
              {
                "response": "Past this point, we have what I'd consider \"half\" of a naive service: the service can trust that actions requested to be performed on a resource are allowed. If this is as far as I could take the cluster, it still provides a lot of value. But the service still needs to do some logic as to what a particular user can see, which is unsatisfying. But let's take a little break from these barriers, go depth-first, and talk about the origin of the authz/n information.\n"
              },
              {
                "response": "#### Authorization Adapters\nOne of the goals of the cluster experiment was to support existing security infrastructure transparently. OPA was used as the only gate for both trust boundaries, but the clients may need information from their existing IdP (identity provider) in order to reason about access control. This entails using data or existing rules from Active Directory, Okta, or various other IdPs within the logic of a policy.\n"
              },
              {
                "response": "I accomplished this by making several microservices that used the adapter pattern to ETL (extract,transform,load) any information necessary for policy evaluation from various IdPs to OPA's internal, ephermal datastore.\n\nI preferred this method to OPA policies doing http requests to our adapters, as I feel doing http requests and parsing the results make policies much more difficult to understand, as the policy now has to know and deal with implementation details, which defeats the purpose of having a restricted policy language.\n",
                "figure": {
                  "figure": "identity-provider-data-to-opa",
                  "miro": "3458764548738949056"
                }
              },
              {
                "response": "#### Delegating Authorization Decision-Making\nOPA policies can be designed to proxy the authn/z decision making to an existing security service. For example, for a naive implementation, I made a microservice that could query Okta's decision-making engine and treat Okta's response as authoritative. A simple http service internal to the cluster that takes data from the post body, queries Okta, and returns 200s if Okta says things are ok, 400s otherwise.\n\nIn a production system, I would use/make a plugin for OPA that abstracts the communication to an external identity provider behind a simple construct accessible to the Rego language, keeping the policy language again unaware of the specifics of http. But this effort could be done in parallel to others working on the cluster, with the security model held constant. The http parsing would just be removed from the policy later, and that interim microservice would be removed. In the meantime, if OPA speaks rest, and I can make a microservice that speaks rest and Okta, we can close the loop early.\n\nThrough this, enterprise customers could keep all their rules for authentication and authorization in Okta, which they may already be using for other purposes. Keycloak has a similar mechanism, with a different scheme when it comes to rule execution. It's a common feature of identity providers.\n\nThe eventual removal of http parsing from the policy is a good example of how the cluster can bear the burden of transporting security information and allow the policies to more closely resemble their \"platonic ideal\": a simple, declarative statement of what is allowed and what is not. The cluster attempts to factor everything else out, continuing to separate responsibilities.\n"
              },
              {
                "response": "#### Authorization Egress Filters\n_warning: we are entering the area in which performance could be a concern. Using the OPA service is a good way to reason about this approach, but other methods can be used to accomplish the same effect. An example of this is [generating SQL from OPA policies](https://blog.openpolicyagent.org/write-policy-in-opa-enforce-policy-in-sql-d9d24db93bf4), but I am not a fan of this as it requires services to have special knowledge re: both OPA and the database. But still, it's better than nothing!_\n\n\nYou may have noticed an asymmetry in the [authorization barrier](#authorization-barrier). While the request to perform an action is gated by an OPA policy, the response (egress from the pod) is not. As a result, a service within a pod may need to make assumptions regarding the security model.\n"
              },
              {
                "response": "One way to think about policies is that they are 'gates' that control access to a resource. This is the most common use case of an OPA policy.\n"
              },
              {
                "figure": {
                  "figure": "opa-policy-gating-a-single-resource",
                  "miro": "3458764548858771011"
                }
              },
              {
                "response": "We can, however, use the same gating policy over the set of all resources. This returns all the resources a user is allowed to see, based on the 'gating' policy via [partial evaluation](https://blog.openpolicyagent.org/partial-evaluation-162750eaf422)\n"
              },
              {
                "figure": {
                  "figure": "opa-policy-filtering-resources",
                  "miro": "3458764548861602424"
                }
              },
              {
                "response": "_this, of course, has implications when it comes to pagination, but I had some ideas that are outside of the scope of the experiment_.\n"
              },
              {
                "response": "With the addition of this filtering, the authorization barrier is now symmetric, with OPA both gating access to resources and filtering the response. At this point the pod architecture allows for a service to be fully naive.\n",
                "figure": {
                  "figure": "authorization-barrier-with-filter",
                  "miro": "3458764548866168330"
                }
              },
              {
                "response": "We can even run this policy as a filter on a different microservice, as long as enough variables are applied via partial evaluation. This is a bit trickier though, and this answer is already ludicrously long. Just keep in mind that the resources returned by this microservice + selected policy would filtered __again__ by the egress filter on the microservice pod.\n"
              },
              {
                "response": null,
                "figure": {
                  "figure": "opa-policy-filtering-other-domains",
                  "miro": "3458764548863225249"
                }
              },
              {
                "response": "Finally, the OPA policy that filters responses doesn't have to be the same policy that gates the requests. They can be totally unrelated. I reused the gate as a filter in the above examples to demonstrate how much mileage you can get out of a single policy using this architecture.\n"
              },
              {
                "response": "#### Ok, well hopefully that was enough\nI hope that the above description of my experimental cluster design is enough to demonstrate some of my security engineering experience - particularly with regards to OPA. I plan on referring to this answer in subsequent prompts.\n\nI hope somebody took the time to read this, and that it was at least somewhat understandable. It was a lot of work recreating this from memory! Thanks for giving me a soapbox.\n\n\nI am obviously interested in doing more research in this area.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience integrating OpenID Connect providers or using OpenID Connect libraries in your projects.",
            "answers": [
              {
                "response": "I both consumed OIDC responses from external identity providers, and run Keycloak as an OIDC provider in the cluster described above. Besides that, I can't think offhand of other work I've done regarding OIDC except to study the workflow in order to build the cluster.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with container technologies such as Docker, LXD and Kubernetes.",
            "answers": [
              {
                "response": "I have used Docker extensively throughout my personal and professional life. I'm running a Minecraft server, a duckdns image, and a Plex media server on this machine as I type this! I also have Podman, but ironically I have to run Podman with sudo, and I can run Docker without. Which, come to think of it, is a pretty big security vulnerability on my home machine...\n\nI often set up development environments for people that use docker compose liberally if we're working on a nontrivial project that requires multiple services. I also use Docker to run tests in CI/CD pipelines. In fact, I often use the same docker-compose.yml file for both development and CI/CD, spinning up the entire stack for testing before deployment.\n\nI have a few personal repos that at least demonstrate my interest in Docker that I might as well list off:\n - [here's a repo](https://github.com/redaphid/docker-compose-ecs-test) of me messing around with Docker Compose deploying to AWS! There is almost literally nothing in it, but if you're curious you can go through the commits and see what I was trying out.\n - [here's an older repo](https://github.com/redaphid/jad) of me using Docker in various ways on a home server. In the `services/plex` directory you can see me do a little fanciness where I encrypt my Plex server preferences so I don't leak my token and can still have a public repo.\n - [here's an older repo](https://github.com/redaphid/docker-ntp-server) Where I was running a very accurate ntp server in this effort to make \"indoor gps\" via ultrasonic emitters and detectors.\n\n  _tl;dr: I think it was 3 Raspberry Pi's that each had their own ultrasonic frequency. I was trying to triangulate the position of a device in a room that had an ultrasonic detector. It didn't work. But I learned a lot about how bats use ultrasonic frequencies to navigate!_\n\nIt looks like after a while I started using ZFS and relied on automatic snapshots to keep my Docker configs at home.\n\n\nAs you probably can imagine from the cluster experiment, I am at least somewhat familiar with Kubernetes. To be honest I usually have to spin up and look in to using Kubernetes whenever I get my hands dirty with the copius YAML config. Kubernetes is the pragmatic choice for all enterprise development right now, but personally I find it to grossly violate the Unix Philosophy with it's complexity. That said, Kubernetes is important. I have used it in the past, and will again. Personal disagreements notwithstanding.\n\nI prefer Docker Swarm, but I know I'm a minority at this point.\n\nRegarding LXD: I just looked in to it, and it sounds pretty awesome! I haven't tried it, but it sounds like it might be helpful for me.\n\nI (like any normal person) run a gaming vm inside my Linux box. Just because I don't want to have a \"real\" machine in my house that runs windows.\n\n```\n<username>@mortal ~> sudo virsh list --all\nId   Name          State\n------------------------------\n1    nx3           running\n2    bf3           running\n3    win7-2009     running\n-    bt2_testbed   shut off\n-    face          shut off\n-    face2         shut off\n\n```\nThe little I just read about LXD sounds like it might be a rabbithole I should avoid until I fill out the rest of this document.\n"
              },
              {
                "response": null
              }
            ]
          }
        ]
      },
      {
        "title": "General Software Engineering Experience",
        "prompts": [
          {
            "prompt": "What kinds of software projects have you worked on before? Which operating systems, development environments, languages, databases?",
            "answers": [
              {
                "response": "I pride myself in being a generalist. To that effect, I have worked on a huge variety of different systems, languages, etc. ranging from microcontrollers, to the open-source Internet of Things platform [Meshblu](https://github.com/octoblu) I talk about a little [here](#experience-with-meshblu), to giant fancy clusters that you may remember me describing in a previous response :).\n\nI helped out with developing a new  closed-source browser recently, which was interesting. [Here](https://github.com/loqwai/experiments-messageport-react) is a repo where I experiment with distributing state via Message Ports to be rendered with React. Electron has some crazy way of fusing node-side and browser-side MessagePorts together, which is important, because when making a browser you have to deal with a lot of different processes, and you really need to have a structured way for them to communicate with each other.\n\n\nI like working on platforms when I get the chance - though this is no means required. Working at Octoblu on the IoT platform and the flow-based execution engine that ran on top of it was by far the most satisfying work of my career so far.\n\nI have a feeling I'll need to talk about Octoblu and Meshblu a lot more further on in this document, so I won't go in to this any more right now. But [here](https://www.youtube.com/watch?v=g9sLUn_lPaQ&list=PLugWVgJZBNjY_jtZBLZnQmtwr402bC6We) is a YouTube playlist featuring the flow execution engine if you're interested in seeing it in action.\n\nAs far as languages go, I've used a ton of them. I'm a language nerd. I soapbox about Lisp when I see an opening in conversations.\n\nThese days I'm doing a lot of Typescript, but:\n\n- [here](https://github.com/loqwai/juniper-gardens-twig-debug)'s a repo where I helped an open source project by fixing a bug in their implementation of the ESPHome protocol that allows esp32 microcontrollers to get wifi credentials over BLE. (Javascript. On a microcontroller. This was not my idea)\n- [here](https://github.com/loqwai/extra-color-perception)'s a repo where I make hardware pendants that each glow a different color, and that color changes depending on how physically close you are to someone else wearing a pendant with a different color. (C++ on esp32s, using platform.io).\n- [here](https://github.com/loqwai/carbonaria)'s a repo where I learn Rust by making a game with my friend!\n- [here](https://github.com/redaphid/fish-shell-stuff)'s a repo of all my Fish shell functions I clone down when I set up a new machine or VM.\n- [here](https://github.com/redaphid/resume-canonical)'s the repo I used to generate this document. Mostly using Javascript, HTML, Github Actions, and madness.\n\nI've used Go somewhat recently for professional, closed-source work. I'm catching up with Python to learn more about the latest AI craze. I coded in Java and C# in the distant past, when I didn't know any better.\n\nAs far as databases are concerned, I've used Firebase, Postgres, and MongoDB recently-ish. If Redis counts, I've done some of that. But it probably doesn't.\n\nThe particulars of a given database haven't affected my career much so far. I try to get to the point where I'm abstracting the details as soon as possible, and rarely think about it again unless some problem crops up.\n\nRegarding development environments, I'm all-in on vscode and Linux. I bought Red Hat 6.0 at a Gamestop in high school by saving up my lunch money. I had to buy a Macbook for the aforementioned browser project, and I'll use it occasionally because the hardware is fantastic. Please someone update the Linux kernel to support this hardware! I want out of Mac-land.\n"
              }
            ]
          },
          {
            "prompt": "Would you describe yourself as a high quality coder? Why?",
            "answers": [
              {
                "response": "Yes, I would describe myself as a high quality coder.\nI think it all boils down to philosophy, with some experience mixed in.\n\n#### Philosophy\nI have a syncretic coding philosophy that I've developed over the years, and love to talk about. Not much of my philosophy is novel, but I steal a lot from others. To wit:\n* Unix Philosophy\n* Worse is better\n* The Zen of Python (with the exception of 'flat is better than nested')\n* Pragmatic Programmer\n* Some of Uncle Bob's Clean code.\n* Test Driven Development\n  - I have a game that makes this fun, I swear!\n* Strong opinions, weakly held.\n* Some of \"xtreme programming\"\n* \"premature optimization is the root of all evil\"\n* Ping-Pong pairing.\n* \"The best code is no code at all\"\n* The \"[Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise)\" as a heuristic for code complexity.\n* The Principle of Least Surprise\n* \"Branching by Abstraction\"\n* \"Make it work, make it right, make it fast\"\n\n...and many others.\n\nI simplify interfaces exposed to whatever is consuming my code as much as possible, exposing the bare surface area necessary for it to be functional. I try to use the simplest data structure for inputs and outputs.\n\nWhen it comes to code complexity, I believe \"There are either obviously no errors, or no obvious errors\". Defects hide in complexity. Keep your code as simple as possible - but no simpler.\n\nI apply the scientific method to my coding philosophy, and allow it to evolve over time, based on new information and experiences.\n\nI try to use the Socratic Method whenever possible during architectural or procedural debates.\n\nI really value collaboration. My brain probably can't come up with the best solution to a problem on its own. I like to bounce ideas off of other people.\n\nI like finding out I was wrong about something! I enjoy well-intentioned critique of my work, and am happy to do it for others as well. I foster a culture where people \"like it when they are wrong\", and are open and eager to hear criticism. Because how else can we evolve?\n\nI feel a major part of our job is the fight against entropy, and finding the simplest way to accomplish our goals.\n\nI think critically about the solution I am developing, and focus on developing what we actually need by checking my assumptions regularly.\n\nI commit constantly, as you can see in any repo I've ever worked on. I make the features I'm working on as small and atomic as possible.\n\nI've mentioned a few times that I pride myself in being a generalist. And one of the reasons for this is I think it's good for you. It keeps the mind flexible, and just knowing what's out there in our huge field of work gives you more \"tools\" in your mental toolbox. I don't have to remember how to do a binary search, but I know that it exists, and what it's for. Knowing things like that helps you to design things better.\nFor example, learning about Rust's \"match arms\" and \"match guards\" has caused me to think about switch statements and guard clauses in other languages in a different way. I know if I see a problem that looks like something a match arm would make easier, there might be a development pattern I can use to imitate them, even if I'm not in Rust.\n\n#### The Game\nThe first thing that always comes to mind when I think about my good developer habits is this \"game\" I play that was originally for pairing. But I \"play it\" with myself when I can't pair, assuming both roles.\n\nThe game goes like this:\n1. \"Member 1\" of the pair writes the simplest possible test that fails, given the current codebase.\n1. \"Member 2\" of the pair writes the simplest possible code that will cause the test to pass - even if it's a function that returns a constant present in the test!\n1. The roles in the pair are reversed, and the loop continues.\n\nAs a heuristic for what constitutes the \"simplest possible code\", I use the [Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise), moving from simplest->most complex code on this scale, increasing in complexity only when forced by a test to do so.\n"
              }
            ]
          },
          {
            "prompt": "Would you describe yourself as an architect of resilient software? If so, why, and in which sorts of applications?",
            "answers": [
              {
                "response": "Yes, I would describe myself as an architect of resilient software.\n\nThe statements re: being a [high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder?-why?) apply here. I'm guessing that is a given, but I figured I'd refer to it just in case.\n\nOne way characterize architectural resiliency is by how well the system recovers from failure. And failures will happen.\n\nThe domain in which I deal with the importance of resiliency the most is (perhaps obviously) cloud services. But I think the principles I refer to below apply to any system.\n\nOne of the things I do is paradoxically make sure a system crashes out early if some critical expectation is not met. By making things crash as soon as possible, you an expectation that things may be crashing all the time, which causes resiliency to be a concern from the beginning.\n\nDetecting failures and replaying any actions attempted during the failure helps - as long as the actions don't have side effects that can't be undone.\n\nThings like blue/green deployments are great for resiliency, along with easy rollbacks to previous versions of subsets of the system.\n\nI'm excited by the idea of [Principles of Chaos](http://principlesofchaos.org/).\n\nThings like [circuit breakers](https://martinfowler.com/bliki/CircuitBreaker.html) help.\n\nI think of resiliency not as a \"feature\" of a system, but rather a property of the system. It's not easy to add to a system after the fact. It's something you have to design into the system from the beginning.\n"
              }
            ]
          },
          {
            "prompt": "What is your most senior role in a software engineering organisation? Describe your span of control, and the diversity of products, functions and teams you led.",
            "answers": [
              {
                "response": "I was Staff at Citrix for 3 years, during which time I would sometimes manage multiple teams in order to accomplish a goal. At Octoblu (startup acquired by Citrix), I co-architected and managed a few different initiatives - the major ones being the Meshblu 2.0 permission system referenced earlier [here](#experience-with-meshblu), and the \"Flow Engine\" - a flow-based execution engine that ran on top of our open source, free-to-use IoT platform Meshblu. We deployed the flows at large scale, using redis queues and workers, taking advantage of any asynchronicity we could to make the system as fast as possible. In the beginning, the system used a Docker container for each flow, but as you can imagine, this did not scale.\n\nThese were difficult, highly technical problems to solve, and we were executed them very effectively, in a short timeframe.\n\nI also assumed some leadership roles for the front end of the flow system, in which people could drag-and-drop nodes representing devices and wire them together.\n\nI later was team lead for an R&D focused team within Magic Leap, which had a lot fewer responsibilities, but was still a lot of fun.\n"
              }
            ]
          },
          {
            "prompt": "What is your proudest success as an engineering leader?",
            "answers": [
              {
                "response": "I'm sounding like a broken record now, but I am extremely proud of the work I did on the Meshblu platform. It's architecture and design is great, the system was functional and processing 4 million messages a day. Translating the configuration and messaging systems of hundreds of types of devices and services into one \\*ahem\\* **canonical** system and having the system work well was a huge success.\nThe flow engine was a very technical challenge, and the whole team working on it were very effective at executing on it. It was a great experience.\n"
              }
            ]
          },
          {
            "prompt": "Outline your thoughts on open source software development. What is important to get right in open source projects? What open source projects have you worked on? Have you been an open source maintainer, on which projects, and what was your role?"
          },
          {
            "prompt": "Describe your experience building large systems with many services - web front ends, REST APIs, data stores, event processing and other kinds of integration between components. What are the key things to think about in regard to architecture, maintainability, and reliability in these large systems?",
            "answers": [
              {
                "response": "Meshblu, again, is a great example of my experience with this. We had hundreds of **unique** services, translating millions of messages between many protocols, to many devices and services. Most of these services were themselves using Meshblu to handle requests, responses, configuration, and data storage, because they appeared to be a \"device\" within the system. This allowed us to delegate all event processing, data storage, and integrations into Meshblu itself.\n\nThough not everyone is fortunate enough to have this sort of infrastructure built in in a cloud-spanning way, the principles of the system are the same. The services should be as simple and decoupled as possible, exposing a small, sensible API. We should strive to delegate as much of this complexity as possible to the environment around the service. I'm starting to notice a theme in my responses here.\n\nI don't believe in forcing uniformity upon any single service in the system as long as it is small - and services should be, unless there is a good reason to make them larger.\nRather, we should make the path to uniformity so simple and easy that people want to walk it. Libraries, generators, templates, guides, and things of this nature should be provided to make service development as easy as possible - but not forced upon anyone. This is an evolutionary approach to making good tools that I find to be very effective.\n\nSmall services that have other responsibilities pushed as far out to the edge as possible are easy to maintain, and easy to replace. The heuristic I mentioned earlier that software either \"obviously has no bugs\" or \"has no obvious bugs\" is a good one when thinking about maintainability and reliability.\n\nArchitectures of large systems should provide environments that help the services within push those responsibilities out. I don't know if you remember the [way too long](#the-totally-naive-services-cluster--open-policy-agent-experience) description of my experimental cluster architecture earlier, but I think it's a good example of how to think about responsibility delegation in a large system.\n"
              }
            ]
          },
          {
            "prompt": "How comprehensive would you say your knowledge of a Linux distribution is, from the kernel up? How familiar are you with low-level system architecture, runtimes and Linux distro packaging? How have you gained this knowledge?",
            "answers": [
              {
                "response": "This is an interesting thing to try to quantify.\nI think the best way I can answer this is via a few personal anecdotes, and some reflection as to what that means.\n\nI first used Linux when I bought Red Hat 6.0 from a Gamestop by saving my lunch money in high school. It didn't work for teenage me. I couldn't get the installation to finish. I had to retreat.\n\nFreshman year in college I decided to get Gentoo (stage 2) running on an old P4 with 128mb of ram, because I was trying to use it as a DVR, and needed all the performance I could get. It compiled for easily over 12 hours. The beeping from the speaker in the motherboard all night I'm sure haunts my then-roommate to this day. But I'm sure all the effort I put in to removing all printing capabilities from all the software on that distro via some kind of flag system emerge used was \"worth it\". And it worked.\n\nDuring this time I had a laptop whose cd-rom had failed, and the bios wouldn't detect the hard drive. I used Puppy Linux on a floppy to get the laptop to a usable state. And I used it quite a bit.\n\nI've tried to use Alpine as a \"hypervisor\" that just ran virsh and everything else would be a container, or VM, but that failed because Alpine is a huge pain with it's state management when it's not in a container. So I returned, once again, to Ubuntu. And that machine has been fairly problem-free.\n\nI tried Void Linux close to 2 years ago, because I wanted to use a \"bootloader\" that allowed me to select from different ZFS datasets to use as the root filesystem for the machine. I wanted to use [ZFSBootMenu](https://docs.zfsbootmenu.org/en/latest/) to do this, and the tutorials all used Void. I got this to work, but it turns out systemd is pretty important these days, and I had trouble using Docker (well, Rancher) with it. Also the Docker daemon hates ZFS.\n\nI know, for example, that ZFSBootmenu is not an \"actual bootloader\", but a Linux kernel that later (I believe) `kexecs` somehow to replace itself. Unfortunately I was trying to use VFIO to pass through a GPU to a VM and it didn't work. I had suspicions it had something to do with this. So I used Ubuntu again, and that's where the VM is hosted to this day.\n\nI've accidentally been in the initrd environment a few times, and was confused and amazed by what was going on. Then did research in to how it works. I think it loads the binary with this minimal set of tools directly in to ram before continuing to start the system? I have messed around there to try to save machines in the past.\n\nI've been curious re: musl, but sadly think it'll never catch on.\n\nI like the \"everything is a file\" stuff. I've dd'd my own hard drives on a few occasions.\n\nI have a use an old machine in my home network as a bastion host for ssh'ing into my VM server. Running Ubuntu, of course.\n\nI like the \"everything is a file\" system, and yearn for an alternate timeline where Plan 9 became the dominant OS.\n\nMy `/etc/apt/sources.list.d` looks like this:\n```\n<username>@mortal ~> ls -1 /etc/apt/sources.list.d\n1password.list\ncuda-ubuntu2204-12-0-local.list\ndocker.list\nfish-shell-ubuntu-release-3-jammy.list\ngoogle-chrome.list\npop-os-apps.sources\npop-os-release.sources\nsignal-xenial.list\nspotify.list\nsteam.list\nsystem.sources\nvscode.list\n```\n_yes, it is Pop :/_\n\nI'm not sure how these anecdotes roll up to a quantitative measure of my Linux knowledge, but I think you get the gist.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with large-scale IT operations, SAAS, or other running services, in a devops or IS or system administration capacity",
            "answers": [
              {
                "response": "I talk about large-scale SaaS operations a bit [in the Meshblu section](#experience-with-meshblu), but I wouldn't characterize myself as having system administration responsibilities.\nWhen it comes to devops, I've set them up for a few projects, and I often become the \"devops expert\" for a project because I know a little about it. I am not claiming I'm an __actual__ devops expert. Just painting a picture of my experiences.\n\nIt's not much, bit I set up the repo I use to write this document to use CI to generate a pdf and publish a [Github release](https://github.com/redaphid/resume-canonical/releases/latest) on a push to main. You can see the Github Actions workflow used to create this document [here](https://github.com/redaphid/resume-canonical/blob/main/.github/workflows/main.yml). I am (obviously) used to more sophisticated devops scenarios, but I did this one for myself, for fun. That might say something about the level of familiarity I have with devops.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with public cloud based operations - how well do you understand large-scale public cloud estate management and developer experience?",
            "answers": [
              {
                "response": "I am somewhat experienced when it comes to public cloud operations. I've used AWS off and on for a long time, but I do have to refresh my memory on how to do certain things whenever I touch it. But things change so fast, it's worth reading up on it every time I use it.\n\nI know about reducing the surface area of attack for the \"cloud estate\" at a theoretical level, and have done it before in the less-hand-holding Amazon resources (e.g. EC2 clusters).\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with enterprise infrastructure and application management, either as a user running enterprise operations, or as a vendor targeting the enterprise market",
            "answers": [
              {
                "response": "I talk about my understanding and experience when it comes to some enterprise infrastructure while talking about the cluster designed to integrate with enterprise infrastructure [here](#the-totally-naive-services-cluster--open-policy-agent-experience).\nMore specifically, when I talk about [making authorization adapters](#authorization-adapters) and [delegating authority](#delegating-authorization-decision-making).\n"
              }
            ]
          },
          {
            "prompt": "Outline your thoughts on quality in software development. What practices are most effective to drive improvements in quality?",
            "answers": [
              {
                "response": "Hm. If we're talking about how to write quality code, I've answered the question pretty thoroughly [when talking about why I think I'm a high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder--why-). This must be a different kind of quality.\n\n If we're talking about improving code quality at a team level, I usually focus on collaboration and enthusiasm. Coding is fun! I have an open discussion with whoever is interested re: heuristics when it comes to \"quality code\". We can agree or disagree about them, and revisit them from time to time. But we at least know they exist, and have a lexicon for them.\n\nA culture of collective ownership when it comes to code is important, so people don't end up in silos, disconnected from the rest of the project.\n"
              }
            ]
          },
          {
            "prompt": "Outline your thoughts on documentation in large software projects. What practices should teams follow? What are great examples of open source docs?"
          },
          {
            "prompt": "Outline your thoughts on user experience, usability and design in software. How do you lead teams to deliver outstanding user experience?",
            "answers": [
              {
                "response": "I took a few design classes in my graduate studies, but I wouldn't consider myself an expert in user experience. I do, however, have a few \"tools in the toolbox\" that can guide me along.\n\nThe first thing I think about when I think about design is what I took away from reading Klaus Krippendorf's \"[The Semantic Turn](https://www.amazon.com/Semantic-Turn-New-Foundation-Design/dp/0415322200)\". I won't pretend I've read all of it, but the concept of \"affordances\" made such an impact on me that it is the first thing I think about when I think about design.\n\nMy understanding of affordances is that they are \"properties of an object that broadcast what can be done with it, and how\". To that effect, I think the primary consideration when designing user experiences is to make the affordances of the system clear.\n\nThis applies to humans and machines alike.\n\nOf course, in practice, this cannot be the only concern. We can't have a UI that's just a series of labeled components in a big, gray list. Style is important, and much more than affordances are communicated with a good design.\n\nHaving a consistent design language is nice, for graphic design as well as UX. They are both striving to convey information in non-verbal ways. I like to think about design as a language, and I think it's important to have a consistent vocabulary. But it also can't be like \"newspeak\" from 1984. We need some variation to communicate nuance.\n\nAnother source that springs to mind every time I think about design is [The Humane Interface](https://www.amazon.com/Humane-Interface-Directions-Designing-Interactive/dp/0201379376), by Jef Raskin. I read this book long after it came out, but the principles are still relevant. I am famous amongst coworkers for being very against modals, because this book made such a strong argument.\n\nI also like the idea of a ZUI (\"Zoomable User Interface\") from that book a lot, and I find keeping that idea in mind can inform some design decisions - even if there is no obvious \"zooming\" involved. And also universal undo/redoing.\n\nI was passionate enough about these principles that I led the design of Octoblu's visual editor for the Flow Engine so we would follow these 3 principles, as well as many others. You can see an example of how the UI works [here](https://youtu.be/g9sLUn_lPaQ?t=267). You may notice the lack of modals during configuration, the infinite, zooming canvas, and the universal undo/redo visible in the upper right in this video.\n\nI use these principles to make design decisions. But more than that, I emphasize the usage of principles in general when it comes to leading graphic design. As is a repeating theme throughout this document, I like to have a few heuristics to use as metrics for how \"usable\" a design is. And the heuristics themselves must evolve over time, and - following the scientific method - be open to revision.\n\nI'd imagine someone who is interested in communication via design would take something like a style guide in to account. Say, read up on the colors, tints, font weights, and visual hierarchies. Try to understand the information this design pattern is trying to communicate.\n\nAnd then make a ton of new, unique diagrams using that style guide's color scheme and visual hierarchy. Maybe create a whole new document writing system to experiment with theming documents in the spirit of said style guide while still allowing for exceptions to the theme via modification of the code. Spend time thinking about how to communicate at least __some__ information in that document visually, and semantically. Accidentally cause the semantics of \\<em\\>(emphasis) tags to become ambiguous, and then _not be able to use emphasis in markdown without looking like asides now._\n\nBut who am I kidding - no one is interested enough about design to put all that effort in to a written interview.\n"
              }
            ]
          },
          {
            "prompt": "Outline your thoughts on performance in software engineering. How do you ensure that your product is fast?",
            "answers": [
              {
                "response": "It must be measured. And we have to define what \"fast enough\" is. According to Donald Knuth, \"Premature optimization is the root of all evil\".\n\nOften people will see suboptimal things like nested `for(` loops, and make the assumption that they are the cause of poor performance. But these loops may often run a few times. A colleague of mine once told me that \"the average loop in code runs like 6 times\". I don't know if that's true, and searching for \"average loop iterations computer science\" looks like a chore to go through. But it's an interesting thought to consider while analyzing the code by hand. If it's an extremely inefficient loop that runs 6 times, I'd much rather we use the slower, more semantically accurate code if those 6 interations aren't very complex.\n\nBut more importantly, you shouldn't be doing that optimizing by hand if you can help it. Write the code that is easy to understand first. Premature optimization is the root of all evil. Then, if you have a performance problem, profile it. Find the bottleneck. Then optimize that. Don't optimize the code you haven't written yet, unless it's fairly easy to do.\n\nI tell a story [later on](#what-did-you-achieve-at-university-that-you-consider-exceptional-) about how I got an automatic A in assembly class without writing any assembly. This is because people misunderstood how to be performant in assembly, vs a C optimizing compiler that's specific to the CPU architecture you are targeting. By writing C code, I was able to explain to the compiler what I wanted to achieve semantically, and the compiler knew how to optimize for that specific CPU better than most humans could. The compiler knew about fancy and obscure assembly instructions and how to use them. Computers are good at memorizing how to do stuff like that.\n\nThat said: does performance matter? Yes. It can become an issue. In my opinion you should mitigate risk of performance issues the same way you mitigate risk with everything in software: contain things in very small, well-defined interfaces. If you've done things right, you should be able to replace the implementation of the interface with a faster one without the rest of the code knowing about it. I do this even in microcontroller work where performance is often an issue.\n\nIf we're dealing with something that can horizontally scale, then that is a great way to create a stopgap while working on a more performant solution, which again reduces performance risk.\n\nI also ascribe to the whole \"Make it work, make it right, make it fast\" mantra.\n"
              }
            ]
          },
          {
            "prompt": "Outline your thoughts on security in software engineering. How do you lead your engineers to improve their security posture and awareness?"
          },
          {
            "prompt": "Outline your thoughts on devops and devsecops. Which practices are effective, and which are overrated?"
          }
        ]
      },
      {
        "title": "Industry leadership experience",
        "prompts": [
          {
            "prompt": "Describe your speaking experience at industry events and conferences",
            "answers": [
              {
                "response": "I've spoken at a few conferences, and ran quite a few hackathons. I think my biggest talk was at NodeConf, where I spoke negatively about Kubernetes. haha. I have since conceded the victory to Kubernetes.\n\nI was on the local speaking circuit a bit w/regards to IoT, and ran a few workshops sponsored by Intel to get the word out about their IoT offerings.\n\nPersonally, I would teach Javascript in a monthly meetup, and go to local hackerspaces to teach some people to use the tools I was using, and they taught me theirs. I would also go to local meetups and talk about the things I was working on. I also taught kids to program a couple of times, using [Processing](https://processing.org/).\n"
              }
            ]
          },
          {
            "prompt": "Are you a thought leader in any particular area of technology?",
            "answers": [
              {
                "response": "I have a lot of thoughts about software development practices, but I don't tell many people about them, and I don't know how unique they are. tl;dr \"no\".\n"
              }
            ]
          },
          {
            "prompt": "Describe any experience working with startups. What did you draw from that experience that would be relevant for this application?"
          },
          {
            "prompt": "Describe any experience working in a public company. What is important for your colleagues to know about being a public company?"
          }
        ]
      },
      {
        "title": "Education",
        "prompts": [
          {
            "prompt": "How did you fare in high school mathematics, physical sciences and computing? Which were strengths and which most enjoyable? How did you rank, competitively, in them?",
            "answers": [
              {
                "response": "I was a good student in high school. I was an 'A' student in physical sciences. They were fun! I wasn't at the time a fan of math; I was too busy writing calculator games on my ti-89 (I was fancy) in Basic in class. My family had a kind of 'anti-math' bias. I was taught at the time that it was useless. I have since became pretty frustrated about this, and read up on math theory in Wikipedia sometimes.\n\nI was, however, very good at programming.\n\nThe computer science curriculum in high school coincidentally began my freshman year. Well, they had \"Basic Computer Training\" before then.\nI was under the mistaken impression I had to take this class to graduate. A classmate overheard me complaining about this, and told me to take \"Visual Basic\" instead. I wish I could let them know what a difference that made in my life.\n\nI was part of the first generation to take computer science classes in the school. There weren't many of us. But we were all there because we wanted to be.\n\nI don't know if I ever got anything less than 100% on any assignment. It was so much fun. I had arms races with my friends as to who could make the most annoying program. I learned DirectX in VB6. I made programs to \"spy\" on the library computers. I couldn't get enough. I took literally every CS class offered in the school, every year. As electives.\n"
              }
            ]
          },
          {
            "prompt": "What sort of high school student were you? Outside of class, what were your interests and hobbies?  What would your high school peers remember you for, if we asked them?",
            "answers": [
              {
                "response": "I was a good student in high school - people thought of me as an academic. I was extremely in to computers, and so I went to LAN parties a lot with my fellow techies. I liked paintball. I took Wing Chun Kung Fu. I worked as a programmer after school at a place I [talk about later](#describe-some-high-school-achievements-considered-exceptional-by-peers-and-staff). I went to punk rock shows. I liked cars.\n\nMy high school peers remember me as a funny, good-natured, intelligent person who occasionally wrote very deep poetry in his notebooks. And obviously, yes, they remember me as a computer nerd. But they remember my attitude first, which I appreciate.\n"
              }
            ]
          },
          {
            "prompt": "In languages and the arts at high school, what were your strongest subjects and how did you rank in those among your school peers?",
            "answers": [
              {
                "response": "I did well in language arts in high school. I was picked to be the \"Best at English\" for the yearbook my graduation year, and have been able to brag about it at parties ever since. I got 5s in my AP English classes, and my teacher in Honors English actually remembered me when I ran into her years later.\nI was an 'A' student in anything language arts. But if \"Best at English\" in a high school yearbook means anything, then I was the best at English.\n\n(Honestly, though: This was chosen by the English teachers. I was actually the top student with regards to English).\n"
              }
            ]
          },
          {
            "prompt": "Describe some high school achievements considered exceptional by peers and staff",
            "answers": [
              {
                "response": "Did I mention I was the best at English?\n\nThe English teachers really were pretty impressed by me. As were, of course, the Computer Science teachers. They employed me to do networking in classrooms and help various staff members with their computers.\n\nI was recruited by a subcontractor to Honeywell who was working on flight management systems for Airbus A320 aircraft and looking for high school kids he could pay $11/hr to to do some of his work for him when I was 16. This was awesome.\nThe staff were pretty impressed, as were my friends and parents. I had a great time, learned a lot, and made a lot of mistakes that I hope were caught by somebody.\n"
              }
            ]
          },
          {
            "prompt": "Which university courses did you perform best at? How did you rank in your degree?",
            "answers": [
              {
                "response": "I did the best in computer science. I went to a liberal arts school, but I think I took every CS class they offered. It is a small school, and I bet the teachers remember me. I insisted on taking 3d graphics my first semester freshman year, and the school was flexible enough to allow me to do it. It was pretty rough going in to that class without taking any algorithm or datastructure classes yet, but I had been programming for fun and work long enough that I did pretty well.\n\n I don't think there were 'rankings' for my degree.\n"
              }
            ]
          },
          {
            "prompt": "Outside of class, what were your interests and where did you spend your time?",
            "answers": [
              {
                "response": "I spent a lot of time driving around, getting lost in the Pacific Northwest, and - obviously at this point - messing with computers.\nMy school had a program in which they would give you a \\*house\\* instead of a dorm room if you had an academic theme and put on a few events related to that theme per semester.\n\nThe program was similar to the fraternity/sorority system.\n\nMy friends and I founded \"Robothouse\", with 8 of us living in a smallish 2-story house, with me in the basement. We had a permanent LAN party, and sometimes big ones that would fill the house when friends would stop by.\n\nI also worked part-time for the school, writing software that interfaced with smart routers via SNMP. This was during the age when viruses were running wild in Windows-world. All the students on campus were connected to the same LAN, and were all pirating software for years. When the semester started, thousands of these computers would plug in at once and viruses spread like wildfire.\n\nMy job was to work on this software that would scan each computer when they signed up to use the network for the first time using Nessus. If Nessus found any critical vulnerabilities, we'd lock their computer in to a vlan where they could only access Windows Update (and maybe a few other services). Then they could update, try again, and the cycle would potentially repeat.\n"
              }
            ]
          },
          {
            "prompt": "What did you achieve at university that you consider exceptional?",
            "answers": [
              {
                "response": "I took the hardest possible computer science classes first, then wrapped around and took the 1-200 levels to finish my degree when I needed a break. Founding Robothouse was pretty cool, and it went on for a couple of years after I graduated. Which made me pretty happy.\n\nI received an automatic 'A' in assembly class, without writing much assembly. My professor was making a homemade image manipulation program, and one of the class's assignments was to write the algorithm for some subset of that program. I think it was color normalization for my year.\n\nThere was a competition where, if your algorithm was faster than the teacher's, you would get an automatic 'A'. So I made a faster algorithm in C, using tons of compiler flags, and optimization levels for the particular CPU we were running it on. I got an 'A' without writing too much\nassembly.\n\nI then made a header file and a compiler script, and sent it to everyone in the class, saying they could use it if they kept my name in the comments of the header\n"
              }
            ]
          },
          {
            "prompt": "At high school or university, what leadership roles did you take on?",
            "answers": [
              {
                "response": "Besides founding Robothouse, nothing readily comes to mind. I preferred organizational structures that didn't require an explicit 'leader' role. I liked to have leadership be an emergent property of the group, rather than something that is explicitly assigned.\n"
              }
            ]
          }
        ]
      },
      {
        "title": "Context",
        "prompts": [
          {
            "prompt": "Outline your thoughts on the mission of Canonical. What is it about the company's purpose and goals which is most appealing to you? What do you see as risky or unappealing?"
          },
          {
            "prompt": "Who are Canonical's key competitors, and how should Canonical set about winning?",
            "draft": true
          },
          {
            "prompt": "Why do you most want to work for Canonical?"
          },
          {
            "prompt": "What would you most want to change about Canonical?"
          },
          {
            "prompt": "What gets you most excited about this role?",
            "answers": [
              {
                "response": "I don't know if you've noticed the [huge description of this cluster I worked on](#the-totally-naive-services-cluster--open-policy-agent-experience) where I talk a lot about my interest in the theory of security in distributed systems. I don't know how to elaborate on this much more. I'm clearly interested in the work.\n"
              }
            ]
          }
        ]
      }
    ]
  }
}
