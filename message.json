{
  "doc": {
    "logo": "images/interview-icon.png",
    "title": "Written Interview",
    "miro": "https://miro.com/app/board/uXjVMeJA43k=",
    "repo": "https://github.com/redaphid/resume-canonical",
    "intro": "#### Structure of this Document\nWhen responding to these prompts, I found that answering them in a discrete fashion was not always the most effective way to communicate experience and expertise in some technical matters. I have therefore organized some responses into descriptions of concrete solutions in my career that utilize the technologies in question. I provide a detailed description porkbarreled in to answering the questions as I encounter them, and refer to these descriptions in subsequent prompts. I have also included some diagrams to help visualize the concepts I am trying to communicate.\n\nI hope this format makes sense to you, though I imagine I won't be able to clarify anything, due to the anonymity of this process.\n",
    "main": [
      {
        "title": "Identity and Authorization Technology",
        "prompts": [
          {
            "prompt": "Describe your experience with authorization systems, specifically Open Policy Agent and OAuth.",
            "answers": [
              {
                "response": "I have extensive, varied experience when it comes to authorization and authentication systems.\nI will give some brief bullet points of my experiences over time with various authorization systems, and then I will give some more detailed examples of my experience with Open Policy Agent and OAuth.\n\n* The earliest \"exotic\" auth system I encountered in my career was a strange one called [OACC](http://oaccframework.org). It is a open-source system created by a colleague, and I remember it to be something like RBAC, with some permission delegation mechanism.\n\n* Next, I worked for Unicon and used [Shiboboleth](https://www.shibboleth.net/) (a SAML-based identity provider)\n\n* After this, I began work on the open-source Internet of Things platform \"Meshblu\", and the closed source application that ran on top of it called \"Octoblu\". Octoblu was later acquired by Citrix.\n\n#### Experience with Meshblu\nMeshblu is capable of configuring and messaging hardware or software \"devices\" hardware being things like hue bulbs, or DIY Arduino projects, and software being things like web applications. My team inherited Meshblu with a [permission system](https://meshblu.readme.io/reference/whitelists) that was baked directly in to the json document that represented the configuration of the device. This system was a simple one - arrays of concrete uuids that are allowed to perform certain actions. It was only capable of dealing with concrete entities - no roles, groups, etc. My colleagues and I eventually replaced it with a [more flexible system](https://meshblu.readme.io/reference/whitelists-2-0) that is still very simple and easy to understand.\n\nIn short, it is a 2-dimensional \"permission matrix\", where the first dimension is the actions that can be taken on a device {broadcast, discover, configure, message}, and the second dimension usually has 2-3 permissions that were relevant to the action domain. Critically, we added an 'as' permission, allowed for impersonation, which makes groups possible without being baked in to the system.\n\nOn top of this, we have many microservices that exist in \"userland\" with respect to Meshblu; they services could implement different authorization systems on top of Meshblu if necessary or desired. For example, we had a microservice that translated between OAuth and our permission system.\n\nWe had many authn/authz systems of our own to do this permission translation. For example, a microcontroller may need to authenticate via MQTT, the protocol it is using to communicate with Meshblu. Often communications happened over http, and we allowed for basic auth if necessary. Since we had many protocols, I ended up having doing this translation a lot.\n\nWe had many microservices that would \"assume the identity\" of a device, and bridge the gap between Meshblu and other services, which often required the use of OAuth. In some cases, I remember having to do the \"OAuth handshake\" between 4 different oauth providers (including at least 1 of ours), though the details escape me at the moment.\n"
              },
              {
                "response": "Below describes some of my experience  with authentication and authorization systems. I will describe the architecture of an experimental cluster design to illustrate my knowledge and expertise in this problem domain. I know this is probably overkill, but bear with me!\n"
              },
              {
                "response": "#### The \"Totally Naive Services\" cluster & Open Policy Agent experience\nI later designed an experimental cluster to be used in environments with complex and unknown-to-us security requirements, potentially including governmental or military applications. The cluster would be deployed in both single-tenant, on-prem environments and multitenant cloud environments. The cluster design factored out multitenancy with respect to the services it contained; e.g. the underlying services did not have a concept of organizations, or really even users (from a security standpoint). I used Open Policy Agent to the point of absurdity when it came to this cluster design, with the expectation that we'd eventually hit a wall of impracticality after which the services inside the cluster would have to become more complex as they became more aware of their environment.\n\nThis approach to cluster development allowed us to build the cluster in parallel to the teams developing the services, and the development roadmaps of these services could be organized such that there was no wasted code if some of the more novel ideas did not work out.\n\n_aside: I wrote a wrote a Javascript poc that used OPA's ability to return a partially-solved abstract syntax tree from a policy, and then execute it using function composition with [Ramda](https://ramdajs.com/). Should you do this? No. But it was interesting!_\n\n#### Cluster Design from a Security Standpoint\nSeeing how I now have a captive audience (dear reader), I'll use the cluster as an object lesson in how I design things when in \"Research and Development mode\", while minimizing risk during the development of production code.\nI am recalling this from memory, and recreating the diagrams, so I may be wrong about some of the details.\n\nOne of the goals of this architecture was to separate all authentication and authorization concerns from the services themselves, and end up with a \"naive service\", which I will here define as:\n\n1. trusts that actions requested to be performed on a resource are allowed.\n1. trusts that any information requested is allowed to be returned.\n"
              },
              {
                "response": "At the highest level, the cluster is divided into 3 concentric circles of trust:\n"
              },
              {
                "response": "1. No trust (outside world)\n1. Authentication trust (within cluster)\n1. Authorization trust. \"Naive\" (within pod)\n",
                "figure": {
                  "figure": "naive-cluster-overview",
                  "miro": "3458764548628056239"
                }
              },
              {
                "response": "The barriers between these circles were enforced by OPA.\nOutside of the authentication barrier, we are in a no-trust environment.\nOnce we pass through the authentication barrier, communications between the services can trust that the request was from who it says it is.\n"
              },
              {
                "response": "#### Authentication Barrier\nAn OPA service was installed before Envoy at the ingress gateway of the cluster. OPA would evaluate the request after obtaining information necessary to reason about authentication legitimacy. This information could be provided as a result of an OIDC workflow, some kind of Active Directory authentication, or a variety of other authorization methods we supported via an adapter pattern described [later in this document](#authorization-adapters). This policy could also be a passthrough, and assume whatever security handshake that happened earlier in the request flow validated user identity correctly.\n\n_note: the authorization data in the diagram is separated logically from the request, but the information in practice would likely come from the request._\n",
                "figure": {
                  "figure": "authentication-barrier",
                  "miro": "3458764548747400083"
                }
              },
              {
                "response": "#### Authentication Workflow\nThis workflow diagram llustrates the flow of data through the authentication barrier. Note the 'Envoy' nodes represent the same Envoy service on the edge of the cluster.\n",
                "figure": {
                  "figure": "authentication-workflow",
                  "miro": "3458764548844659047"
                }
              },
              {
                "response": "In the proof of concept, I used Envoy to insert an \"Identity\" header into the request after the request was validated through whatever mechanism the client wanted to use. We offered a Keycloak instance as an IdP  (Identity Provider) with OIDC capabilities, but we also would accept OIDC credentials from a trusted 3rd party; or many other authentication mechanisms - we followed an adapter pattern for normalizing auth requirements and data, which I talk about [later on](#authorization-adapters).\n\nRegardless of the mechanism used, in the end an OPA policy always gated access to the cluster. The client's authentication data was evaluated against OPA policies, which had the final say. This, like all autoinjected systems in the cluster, could be opted out of via configuration.\n\nThis implementation, however, has a potential vulnerability in that we would need to scrub an \"Identity\" header off of the incoming request. Since all requests were coming in through the ingress controller that Envoy was manipulating the headers from, in theory this wouldn't be a problem - but I would have rather encapsulated the request in an envelope larger than the request itself. I had learned with Meshblu that separation between data and metadata is very important, especially with regard to security.\n\nIn addition, this header could be manipulated by a malicious service within the cluster. The header solution was good enough for a proof of concept, but further research would be needed to create an envelope for the request that sufficiently separated \"userland\" requests and cluster metadata. Anyway, that's a mini self-audit of this implementation.\n\nPast this barrier, the services could trust that the request was from who it said it was, but they could not (yet) trust that the request was authorized to do what it was trying to do.\n"
              },
              {
                "response": "#### Authorization Barrier\nAfter the authentication barrier is passed, any subsequent OPA policy evaluation can assume that the identity presented to the policy for evaluation is accurate, which allows for simple, decoupled policies to govern authorization on a per-service pod basis. You may notice below the similarity between the cluster-protecting authentication barrier, and this authorization barrier.\n"
              },
              {
                "response": "An OPA service was injected into each Kubernetes pod, layered above Envoy. Work was being done on embedding OPA as a plugin to Envoy (also injected by default in Istio).\nAlso by default, the OPA service would only do the authorization gating on ingress traffic - though evolutions of this design allowed for OPA to filter egress traffic as well. But we'll get to that later.\n",
                "figure": {
                  "figure": "authorization-barrier",
                  "miro": "3458764548747400083"
                }
              },
              {
                "response": "#### Authorization Workflow\nThis diagram shows the flow of data through the authorization barrier. Note that all 'Envoy' nodes refer to the same service.\n\nAt this point, the request is coming from inside the cluster, with the rewritten identity header.\n",
                "figure": {
                  "figure": "authorization-workflow",
                  "miro": "3458764548844659047"
                }
              },
              {
                "response": "Past this point, we have what I'd consider \"half\" of a naive service: the service can trust that actions requested to be performed on a resource are allowed. If this is as far as I could take the cluster, it still provides a lot of value. But the service still needs to do some logic as to what a particular user can see, which is unsatisfying. But let's take a little break from these barriers, go depth-first, and talk about the origin of the authz/n information.\n"
              },
              {
                "response": "#### Authorization Adapters\nOne of the goals of the cluster experiment was to support existing security infrastructure transparently. OPA was used as the only gate for both trust boundaries, but the clients may need information from their existing IdP (identity provider) in order to reason about access control. This entails using data or existing rules from Active Directory, Okta, or various other IdPs within the logic of a policy.\n"
              },
              {
                "response": "I accomplished this by making several microservices that used the adapter pattern to ETL (extract,transform,load) any information necessary for policy evaluation from various IdPs to OPA's internal, ephermal datastore.\n\n I preferred this method to OPA policies doing http requests to our adapters, as I feel doing http requests and parsing the results make policies much more difficult to understand, as the policy now has to know and deal with implementation details, which defeats the purpose of having a restricted policy language.\n",
                "figure": {
                  "figure": "identity-provider-data-to-opa",
                  "miro": "3458764548738949056"
                }
              },
              {
                "response": "#### Authorization Egress Filters\n_warning: we are entering the area in which performance could be a concern. Using the OPA service is a good way to reason about this approach, but other methods can be used to accomplish the same effect. An example of this is [generating SQL from OPA policies](https://blog.openpolicyagent.org/write-policy-in-opa-enforce-policy-in-sql-d9d24db93bf4), but I am not a fan of this as it requires services to have special knowledge re: both OPA and the database. But still, it's better than nothing!_\n\n\nYou may have noticed an asymmetry in the [authorization barrier](#authorization-barrier). While the request to perform an action is gated by an OPA policy, the response (egress from the pod) is not. As a result, a service within a pod may need to make assumptions regarding the security model.\n"
              },
              {
                "response": "One way to think about policies is that they are 'gates' that control access to a resource. This is the most common use case of an OPA policy.\n"
              },
              {
                "figure": {
                  "figure": "opa-policy-gating-a-single-resource",
                  "miro": "3458764548858771011"
                }
              },
              {
                "response": "We can, however, use the same gating policy over the set of all resources. This returns all the resources a user is allowed to see, based on the 'gating' policy via [partial evaluation](https://blog.openpolicyagent.org/partial-evaluation-162750eaf422)\n"
              },
              {
                "figure": {
                  "figure": "opa-policy-filtering-resources",
                  "miro": "3458764548861602424"
                }
              },
              {
                "response": "_this, of course, has implications when it comes to pagination, but I had some wild ideas that are outside of the scope of the experiment_.\n"
              },
              {
                "response": "With the addition of this filtering, the authorization barrier is now symmetric, with OPA both gating access to resources and filtering the response. At this point the pod architecture allows for a service to be fully naive.\n",
                "figure": {
                  "figure": "authorization-barrier-with-filter",
                  "miro": "3458764548866168330"
                }
              },
              {
                "response": "We can even run this policy as a filter on a different microservice, as long as enough variables are applied via partial evaluation. This is a bit trickier though, and this answer is already ludicrously long. Just keep in mind that the resources returned by this microservice + selected policy would filtered __again__ by the egress filter on the microservice pod.\n"
              },
              {
                "response": null,
                "figure": {
                  "figure": "opa-policy-filtering-other-domains",
                  "miro": "3458764548863225249"
                }
              },
              {
                "response": "Finally, the OPA policy that filters responses doesn't have to be the same policy that gates the requests. They can be totally unrelated. I reused the gate as a filter in the above examples to demonstrate how much mileage you can get out of a single policy using this architecture.\n"
              },
              {
                "response": "#### Ok, well hopefully that was enough\nI hope that the above description of my experimental cluster design is enough to demonstrate some of my security engineering experience - particularly with regards to OPA. I plan on referring to this answer in subsequent prompts.\n\nI hope somebody took the time to read this, and that it was at least somewhat understandable. It was a lot of work recreating this from memory! Thanks for giving me a soapbox.\n\n\nI am obviously interested in doing more research in this area.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience integrating OpenID Connect providers or using OpenID Connect libraries in your projects.",
            "answers": [
              {
                "response": "I both consumed OIDC responses from external identity providers, and run Keycloak as an OIDC provider in the cluster described above. Besides that, I can't think offhand of other work I've done regarding OIDC except to study the workflow in order to build the cluster.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with container technologies such as Docker, LXD and Kubernetes.",
            "answers": [
              {
                "response": "I have used Docker extensively throughout my personal and professional life. I'm running a Minecraft server, a duckdns image, and a Plex media server on this machine as I type this! I also have Podman, but ironically I have to run Podman with sudo, and I can run Docker without. Which, come to think of it, is a pretty big security vulnerability on my home machine...\n\nI often set up development environments for people that use docker compose liberally if we're working on a nontrivial project that requires multiple services. I also use Docker to run tests in CI/CD pipelines. In fact, I often use the same docker-compose.yml file for both development and CI/CD, spinning up the entire stack for testing before deployment.\n\nI have a few personal repos that at least demonstrate my interest in Docker that I might as well list off:\n - [here's a repo](https://github.com/redaphid/docker-compose-ecs-test) of me messing around with Docker Compose deploying to AWS! There is almost literally nothing in it, but if you're curious you can go through the commits and see what I was trying out.\n - [here's an older repo](https://github.com/redaphid/jad) of me using Docker in various ways on a home server. In the `services/plex` directory you can see me do a little fanciness where I encrypt my Plex server preferences so I don't leak my token and can still have a public repo.\n - [here's an older repo](https://github.com/redaphid/docker-ntp-server) Where I was running a very accurate ntp server in this effort to make \"indoor gps\" via ultrasonic emitters and detectors.\n\n  _tl;dr: I think it was 3 Raspberry Pi's that each had their own ultrasonic frequency. I was trying to triangulate the position of a device in a room that had an ultrasonic detector. It didn't work. But I learned a lot about how bats use ultrasonic frequencies to navigate!_\n\nIt looks like after a while I started using ZFS and relied on automatic snapshots to keep my Docker configs at home.\n\n\nAs you probably can imagine from the cluster experiment, I am at least somewhat familiar with Kubernetes. To be honest I usually have to spin up and look in to using Kubernetes whenever I get my hands dirty with the copius YAML config. Kubernetes is the pragmatic choice for all enterprise development right now, but personally I find it to grossly violate the Unix Philosophy with it's complexity. That said, Kubernetes is important. I have used it in the past, and will again. Personal disagreements notwithstanding.\n\nI prefer Docker Swarm, but I know I'm a minority at this point.\n\nRegarding LXD: I just looked in to it, and it sounds pretty awesome! I haven't tried it, but it sounds like it might be helpful for me.\n\nI (like any normal person) run a gaming vm inside my Linux box. Just because I don't want to have a \"real\" machine in my house that runs windows.\n\n```\n<username>@mortal ~> sudo virsh list --all\nId   Name          State\n------------------------------\n1    nx3           running\n2    bf3           running\n3    win7-2009     running\n-    bt2_testbed   shut off\n-    face          shut off\n-    face2         shut off\n\n```\nThe little I just read about LXD sounds like it might be a rabbithole I should avoid until I fill out the rest of this document.\n"
              },
              {
                "response": null
              }
            ]
          }
        ]
      },
      {
        "title": "General Software Engineering Experience",
        "prompts": [
          {
            "prompt": "What kinds of software projects have you worked on before? Which operating systems, development environments, languages, databases?",
            "answers": [
              {
                "response": "I pride myself in being a generalist. To that effect, I have worked on a huge variety of different systems, languages, etc. ranging from microcontrollers, to the open-source Internet of Things platform [Meshblu](https://github.com/octoblu) I talk about a little [here](#experience-with-meshblu), to giant fancy clusters that you may remember me describing in a previous response :).\n\nI helped out with developing a new  closed-source browser recently, which was interesting. [Here](https://github.com/loqwai/experiments-messageport-react) is a repo where I experiment with distributing state via Message Ports to be rendered with React. Electron has some crazy way of fusing node-side and browser-side MessagePorts together, which is important, because when making a browser you have to deal with a lot of different processes, and you really need to have a structured way for them to communicate with each other.\n\n\nI like working on platforms when I get the chance - though this is no means required. Working at Octoblu on the IoT platform and the flow-based execution engine that ran on top of it was by far the most satisfying work of my career so far.\n\nI have a feeling I'll need to talk about Octoblu and Meshblu a lot more further on in this document, so I won't go in to this any more right now. But [here](https://www.youtube.com/watch?v=g9sLUn_lPaQ&list=PLugWVgJZBNjY_jtZBLZnQmtwr402bC6We) is a YouTube playlist featuring the flow execution engine if you're interested in seeing it in action.\n\nAs far as languages go, I've used a ton of them. I'm a language nerd. I soapbox about Lisp when I see an opening in conversations.\n\nThese days I'm doing a lot of Typescript, but:\n\n- [here](https://github.com/loqwai/juniper-gardens-twig-debug)'s a repo where I helped an open source project by fixing a bug in their implementation of the ESPHome protocol that allows esp32 microcontrollers to get wifi credentials over BLE. (Javascript. On a microcontroller. This was not my idea)\n- [here](https://github.com/loqwai/extra-color-perception)'s a repo where I make hardware pendants that each glow a different color, and that color changes depending on how physically close you are to someone else wearing a pendant with a different color. (C++ on esp32s, using platform.io).\n- [here](https://github.com/loqwai/carbonaria)'s a repo where I learn Rust by making a game with my friend!\n- [here](https://github.com/redaphid/fish-shell-stuff)'s a repo of all my Fish shell functions I clone down when I set up a new machine or VM.\n- [here](https://github.com/redaphid/resume-canonical)'s the repo I used to generate this document. Mostly using Javascript, HTML, Github Actions, and madness.\n\nI've used Go somewhat recently for professional, closed-source work. I'm catching up with Python to learn more about the latest AI craze. I coded in Java and C# in the distant past, when I didn't know any better.\n\nAs far as databases are concerned, I've used Firebase, Postgres, and MongoDB recently-ish. If Redis counts, I've done some of that. But it probably doesn't.\n\nThe particulars of a given database haven't affected my career much so far. I try to get to the point where I'm abstracting the details as soon as possible, and rarely think about it again unless some problem crops up.\n\nRegarding development environments, I'm all-in on vscode and Linux. I bought Red Hat 6.0 at a Gamestop in high school by saving up my lunch money. I had to buy a Macbook for the aforementioned browser project, and I'll use it occasionally because the hardware is fantastic. Please someone update the Linux kernel to support this hardware! I want out of Mac-land.\n"
              }
            ]
          },
          {
            "prompt": "Would you describe yourself as a high quality coder? Why?",
            "answers": [
              {
                "response": "Yes, I would describe myself as a high quality coder.\nI think it all boils down to philosophy, with some experience mixed in.\n\n#### Philosophy\nI have a syncretic coding philosophy that I've developed over the years, and love to talk about. Not much of my philosophy is novel, but I steal a lot from others. To wit:\n* Unix Philosophy\n* Worse is better\n* The Zen of Python (with the exception of 'flat is better than nested')\n* Pragmatic Programmer\n* Some of Uncle Bob's Clean code.\n* Test Driven Development\n  - I have a game that makes this fun, I swear!\n* Strong opinions, weakly held.\n* Some of \"xtreme programming\"\n* \"premature optimization is the root of all evil\"\n* Ping-Pong pairing.\n* \"The best code is no code at all\"\n* The \"[Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise)\" as a heuristic for code complexity.\n\n...and many others.\n\nI think about minimizing the \"cost of code change\" constantly by simplifing interfaces exposed to whatever is consuming my code as much as possible, exposing the bare minimum surface area necessary for the consumer to do its job. I try to use the simplest data structure for inputs and outputs.\n\nWhen it comes to code complexity, I believe \"There are either obviously no errors, or no obvious errors\". Defects hide in complexity. Keep your code as simple as possible - but no simpler.\n\nI apply the scientific method to my coding philosophy, and allow it to evolve over time, based on new information and experiences.\n\nI try to use the Socratic Method whenever possible during architectural or procedural debates.\n\nI really value collaboration. My brain probably can't come up with the best solution to a problem on its own. I like to bounce ideas off of other people.\n\nI like finding out I was wrong about something! I enjoy well-intentioned critique of my work, and am happy to do it for others as well. I foster a culture where people \"like it when they are wrong\", and are open and eager to hear criticism. Because how else can we evolve?\n\nI feel a major part of our job is the fight against entropy, and finding the simplest way to accomplish our goals.\n\nI think critically about the solution I am developing, and focus on developing what we actually need by checking my assumptions regularly.\n\nI've mentioned a few times that I pride myself in being a generalist. And one of the reasons for this is I think it's good for you. It keeps the mind flexible, and just knowing what's out there in our huge field of work gives you more \"tools\" in your mental toolbox. I don't have to remember how to do a binary search, but I know that it exists, and what it's for. Knowing things like that helps you to design things better.\nFor example, learning about Rust's \"match arms\" and \"match guards\" has caused me to think about switch statements and guard clauses in other languages in a different way. I know if I see a problem that looks like something a match arm would make easier, there might be a development pattern I can use to imitate them, even if I'm not in Rust.\n\n#### The Game\nThe first thing that always comes to mind when I think about my good developer habits is this \"game\" I play that was originally for pairing. But I \"play it\" with myself when I can't pair, assuming both roles.\n\nThe game goes like this:\n1. \"Member 1\" of the pair writes the simplest possible test that fails, given the current codebase.\n1. \"Member 2\" of the pair writes the simplest possible code that will cause the test to pass - even if it's a function that returns a constant present in the test!\n1. The roles in the pair are reversed, and the loop continues.\n\nAs a heuristic for what constitutes the \"simplest possible code\", I use the [Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise), moving from simplest->most complex code on this scale, increasing in complexity only when forced by a test to do so.\n"
              }
            ]
          },
          {
            "prompt": "Would you describe yourself as an architect of resilient software? If so, why, and in which sorts of applications?",
            "answers": [
              {
                "response": "Yes, I would describe myself as an architect of resilient software.\n\nThe statements re: being a [high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder?-why?) apply here. I'm guessing that is a given, but I figured I'd refer to it just in case.\n\nOne way characterize architectural resiliency is by how well the system recovers from failure. And failures will happen.\n\nThe domain in which I deal with the importance of resiliency the most is (perhaps obviously) cloud services. But I think the principles I refer to below apply to any system.\n\nOne of the things I do is paradoxically make sure a system crashes out early if some critical expectation is not met. By making things crash as soon as possible, you an expectation that things may be crashing all the time, which causes resiliency to be a concern from the beginning.\n\nDetecting failures and replaying any actions attempted during the failure helps - as long as the actions don't have side effects that can't be undone.\n\nThings like blue/green deployments are great for resiliency, along with easy rollbacks to previous versions of subsets of the system.\n\nI'm excited by the idea of [Principles of Chaos](http://principlesofchaos.org/).\n\nThings like [circuit breakers](https://martinfowler.com/bliki/CircuitBreaker.html) help.\n\nI think of resiliency not as a \"feature\" of a system, but rather a property of the system. It's not easy to add to a system after the fact. It's something you have to design into the system from the beginning.\n"
              }
            ]
          },
          {
            "prompt": "What is your most senior role in a software engineering organisation? Describe your span of control, and the diversity of products, functions and teams you led.",
            "answers": [
              {
                "response": "I was Staff at Citrix for 3 years, during which time I would sometimes manage multiple teams in order to accomplish a goal. At Octoblu (startup acquired by Citrix), I co-architected and managed a few different initiatives - the major ones being the Meshblu 2.0 permission system referenced earlier [here](#experience-with-meshblu), and the \"Flow Engine\" - a flow-based execution engine that ran on top of our open source, free-to-use IoT platform Meshblu. We deployed the flows at large scale, using redis queues and workers, taking advantage of any asynchronicity we could to make the system as fast as possible. In the beginning, the system used a Docker container for each flow, but as you can imagine, this did not scale.\n\nThese were difficult, highly technical problems to solve, and we were executed them very effectively, in a short timeframe.\n\nI also assumed some leadership roles for the front end of the flow system, in which people could drag-and-drop nodes representing devices and wire them together.\n\nI later was team lead for an R&D focused team within Magic Leap, which had a lot fewer responsibilities, but was still a lot of fun.\n"
              }
            ]
          },
          {
            "prompt": "What is your proudest success as an engineering leader?",
            "answers": [
              {
                "response": "I'm sounding like a broken record now, but I am extremely proud of the work I did on the Meshblu platform. It's architecture and design is great, the system was functional and processing 4 million messages a day. Translating the configuration and messaging systems of hundreds of types of devices and services into one \\*ahem\\* **canonical** system and having the system work well was a huge success.\n"
              }
            ]
          },
          {
            "prompt": "Outline your thoughts on open source software development. What is important to get right in open source projects? What open source projects have you worked on? Have you been an open source maintainer, on which projects, and what was your role?"
          },
          {
            "prompt": "Describe your experience building large systems with many services - web front ends, REST APIs, data stores, event processing and other kinds of integration between components. What are the key things to think about in regard to architecture, maintainability, and reliability in these large systems?"
          },
          {
            "prompt": "How comprehensive would you say your knowledge of a Linux distribution is, from the kernel up? How familiar are you with low-level system architecture, runtimes and Linux distro packaging? How have you gained this knowledge?",
            "answers": [
              {
                "response": "This is an interesting thing to try to quantify.\nI think the best way I can answer this is via a few personal anecdotes, and some reflection as to what that means.\n\nI first used Linux when I bought Red Hat 6.0 from a Gamestop by saving my lunch money in high school. It didn't work for teenage me. I couldn't get the installation to finish. I had to retreat.\n\nFreshman year in college I decided to get Gentoo (stage 2) running on an old P4 with 128mb of ram, because I was trying to use it as a DVR, and needed all the performance I could get. It compiled for easily over 12 hours. The beeping from the speaker in the motherboard all night I'm sure haunts my then-roommate to this day. But I'm sure all the effort I put in to removing all printing capabilities from all the software on that distro via some kind of flag system emerge used was \"worth it\". And it worked.\n\nDuring this time I had a laptop whose cd-rom had failed, and the bios wouldn't detect the hard drive. I used Puppy Linux on a floppy to get the laptop to a usable state. And I used it quite a bit.\n\nI've tried to use Alpine as a \"hypervisor\" that just ran virsh and everything else would be a container, or VM, but that failed because Alpine is a huge pain with it's state management when it's not in a container. So I returned, once again, to Ubuntu. And that machine has been fairly problem-free.\n\nI tried Void Linux close to 2 years ago, because I wanted to use a \"bootloader\" that allowed me to select from different ZFS datasets to use as the root filesystem for the machine. I wanted to use [ZFSBootMenu](https://docs.zfsbootmenu.org/en/latest/) to do this, and the tutorials all used Void. I got this to work, but it turns out systemd is pretty important these days, and I had trouble using Docker (well, Rancher) with it. Also the Docker daemon hates ZFS.\n\nI know, for example, that ZFSBootmenu is not an \"actual bootloader\", but a Linux kernel that later (I believe) `kexecs` somehow to replace itself. Unfortunately I was trying to use VFIO to pass through a GPU to a VM and it didn't work. I had suspicions it had something to do with this. So I used Ubuntu again, and that's where the VM is hosted to this day.\n\nI've accidentally been in the initrd environment a few times, and was confused and amazed by what was going on. Then did research in to how it works. I think it loads the binary with this minimal set of tools directly in to ram before continuing to start the system? I have messed around there to try to save machines in the past.\n\nI've been curious re: musl, but sadly think it'll never catch on.\nI like the \"everything is a file\" stuff.\n\nI've dd'd my own hard drives on a few occasions.\n\nI have a use an old machine in my home network as a bastion host for ssh'ing into my VM server.\n\nI like the \"everything is a file\" system, and yearn for an alternate timeline where Plan 9 became the dominant OS.\n\nMy `/etc/apt/sources.list.d` looks like this:\n```\n<username>@mortal ~> ls -1 /etc/apt/sources.list.d\n1password.list\ncuda-ubuntu2204-12-0-local.list\ndocker.list\nfish-shell-ubuntu-release-3-jammy.list\ngoogle-chrome.list\npop-os-apps.sources\npop-os-release.sources\nsignal-xenial.list\nspotify.list\nsteam.list\nsystem.sources\nvscode.list\n```\n_yes, it is Pop :/_\n\nI'm not sure how these anecdotes roll up to a quantitative measure of my Linux knowledge, but I think you get the gist.\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with large-scale IT operations, SAAS, or other running services, in a devops or IS or system administration capacity",
            "answers": [
              {
                "response": "sadasd\n"
              }
            ]
          },
          {
            "prompt": "Describe your experience with public cloud based operations - how well do you understand large-scale public cloud estate management and developer experience?"
          },
          {
            "prompt": "Describe your experience with enterprise infrastructure and application management, either as a user running enterprise operations, or as a vendor targeting the enterprise market"
          },
          {
            "prompt": "Outline your thoughts on quality in software development. What practices are most effective to drive improvements in quality?"
          },
          {
            "prompt": "Outline your thoughts on documentation in large software projects. What practices should teams follow? What are great examples of open source docs?"
          },
          {
            "prompt": "Outline your thoughts on user experience, usability and design in software. How do you lead teams to deliver outstanding user experience?"
          },
          {
            "prompt": "Outline your thoughts on performance in software engineering. How do you ensure that your product is fast?"
          },
          {
            "prompt": "Outline your thoughts on security in software engineering. How do you lead your engineers to improve their security posture and awareness?"
          },
          {
            "prompt": "Outline your thoughts on devops and devsecops. Which practices are effective, and which are overrated?"
          }
        ]
      },
      {
        "title": "Industry leadership experience",
        "prompts": [
          {
            "prompt": "Describe your speaking experience at industry events and conferences",
            "answers": [
              {
                "response": "I've spoken at a few conferences, and ran quite a few hackathons. I think my biggest talk was at NodeConf, where I spoke negatively about Kubernetes. haha. I have since conceded the victory to Kubernetes.\n\nI was on the local speaking circuit a bit w/regards to IoT, and ran a few workshops sponsored by Intel to get the word out about their IoT offerings.\n\nPersonally, I would teach Javascript in a monthly meetup, and go to local hackerspaces to teach some people to use the tools I was using, and they taught me theirs. I would also go to local meetups and talk about the things I was working on.\n"
              }
            ]
          },
          {
            "prompt": "Are you a thought leader in any particular area of technology?",
            "answers": [
              {
                "response": "Wellll I have a lot of thoughts about software development practices, but I don't tell many people about them, and I don't know how unique they are. Except maybe that cluster design?  tl;dr \"no\".\n"
              }
            ]
          },
          {
            "prompt": "Describe any experience working with startups. What did you draw from that experience that would be relevant for this application?"
          },
          {
            "prompt": "Describe any experience working in a public company. What is important for your colleagues to know about being a public company?"
          }
        ]
      },
      {
        "title": "Education",
        "prompts": [
          {
            "prompt": "How did you fare in high school mathematics, physical sciences and computing? Which were strengths and which most enjoyable? How did you rank, competitively, in them?"
          },
          {
            "prompt": "What sort of high school student were you? Outside of class, what were your interests and hobbies?  What would your high school peers remember you for, if we asked them?"
          },
          {
            "prompt": "In languages and the arts at high school, what were your strongest subjects and how did you rank in those among your school peers?"
          },
          {
            "prompt": "Describe some high school achievements considered exceptional by peers and staff"
          },
          {
            "prompt": "Which degree and university did you choose, and why?"
          },
          {
            "prompt": "Which university courses did you perform best at? How did you rank in your degree?"
          },
          {
            "prompt": "Outside of class, what were your interests and where did you spend your time?"
          },
          {
            "prompt": "What did you achieve at university that you consider exceptional?"
          },
          {
            "prompt": "At high school or university, what leadership roles did you take on?"
          }
        ]
      },
      {
        "title": "Context",
        "prompts": [
          {
            "prompt": "Outline your thoughts on the mission of Canonical. What is it about the company's purpose and goals which is most appealing to you? What do you see as risky or unappealing?"
          },
          {
            "prompt": "Who are Canonical's key competitors, and how should Canonical set about winning?"
          },
          {
            "prompt": "Why do you most want to work for Canonical?"
          },
          {
            "prompt": "What would you most want to change about Canonical?"
          },
          {
            "prompt": "What gets you most excited about this role?"
          }
        ]
      }
    ]
  }
}
