doc:
  logo: images/interview-icon.png
  title: Written Interview
  miro: https://miro.com/app/board/uXjVMeJA43k=
  repo: https://github.com/redaphid/resume-canonical
  intro: |
    #### Structure of this Document
    When responding to these prompts, I found that answering them in a discrete fashion made the discussion too fragmentary at times. I have therefore organized some responses into descriptions of concrete solutions in my career that utilize the concepts in question. I provide detailed descriptions porkbarreled in to answering the questions as I encounter them, and refer to these descriptions in subsequent prompts. I have also included some diagrams to help visualize the concepts I am trying to communicate.

    I hope this format makes sense to you, though I imagine I won't be able to clarify anything, due to the anonymity of this process.
  main:
    - title: Identity and Authorization Technology
      prompts:
      - prompt: Describe your experience with authorization systems, specifically Open Policy Agent and OAuth.
        answers:
        - response: |
            I have extensive, varied experience when it comes to authorization and authentication systems.
            I will give some brief bullet points of my experiences over time with various authorization systems, and then I will give some more detailed examples of my experience with Open Policy Agent and OAuth.

            * The earliest "exotic" auth system I encountered in my career was a strange one called [OACC](http://oaccframework.org). It is a open-source system created by a colleague, and I remember it to be something like RBAC, with some permission delegation mechanism.

            * Next, I worked for Unicon and used [Shiboboleth](https://www.shibboleth.net/) (a SAML-based identity provider)

            * After this, I began work on the open-source Internet of Things platform "Meshblu", and the closed source application that ran on top of it called "Octoblu". Octoblu was later acquired by Citrix.

            #### Experience with Meshblu
            Meshblu is capable of configuring and messaging hardware or software "devices" hardware being things like hue bulbs, or DIY Arduino projects, and software being things like web applications. My team inherited Meshblu with a [permission system](https://meshblu.readme.io/reference/whitelists) that was baked directly in to the json document that represented the configuration of the device. This system was a simple one - arrays of concrete uuids that are allowed to perform certain actions. It was only capable of dealing with concrete entities - no roles, groups, etc. My colleagues and I eventually replaced it with a [more flexible system](https://meshblu.readme.io/reference/whitelists-2-0) that is still very simple and easy to understand.

            In short, it is a 2-dimensional "permission matrix", where the first dimension is the actions that can be taken on a device {broadcast, discover, configure, message}, and the second dimension usually has 2-3 permissions that were relevant to the action domain. Critically, we added an 'as' permission, allowed for impersonation, which makes groups possible without being baked in to the system.

            On top of this, we have many microservices that exist in "userland" with respect to Meshblu; they services could implement different authorization systems on top of Meshblu if necessary or desired. For example, we had a microservice that translated between OAuth and our permission system.

            We had many authn/authz systems of our own to do this permission translation. For example, a microcontroller may need to authenticate via MQTT, the protocol it is using to communicate with Meshblu. Often communications happened over http, and we allowed for basic auth if necessary. Since we had many protocols, I ended up having doing this translation a lot.

            We had many microservices that would "assume the identity" of a device, and bridge the gap between Meshblu and other services, which often required the use of OAuth. In some cases, I remember having to do the "OAuth handshake" between 4 different oauth providers (including at least 1 of ours), though the details escape me at the moment.

        - response: |
              Below describes some of my experience  with authentication and authorization systems. I will describe the architecture of an experimental cluster design to illustrate my knowledge and expertise in this problem domain. I know this is probably overkill, but bear with me!

        - response: |
            #### The "Totally Naive Services" Cluster & Open Policy Agent Experience
            I later designed an experimental cluster to be used in environments with complex and unknown-to-us security requirements, potentially including governmental or military applications. The cluster would be deployed in both single-tenant, on-prem environments and multitenant cloud environments. The cluster design factored out multitenancy with respect to the services it contained; e.g. the underlying services did not have a concept of organizations, or really even users (from a security standpoint). I used Open Policy Agent to the point of absurdity when it came to this cluster design, with the expectation that we'd eventually hit a wall of impracticality after which the services inside the cluster would have to become more complex as they became more aware of their environment.

            This approach to cluster development allowed us to build the cluster in parallel to the teams developing the services, and the development roadmaps of these services could be organized such that there was no wasted code if some of the more novel ideas did not work out.

            _aside: I wrote a wrote a Javascript poc that used OPA's ability to return a partially-solved abstract syntax tree from a policy, and then execute it using function composition with [Ramda](https://ramdajs.com/). Should you do this? No. But it was interesting!_

            #### Cluster Design from a Security Standpoint
            Seeing how I now have a captive audience (dear reader), I'll use the cluster as an object lesson in how I design things when in "Research and Development mode", while minimizing risk during the development of production code.
            I am recalling this from memory, and recreating the diagrams, so I may be wrong about some of the details.

            One of the goals of this architecture was to separate all authentication and authorization concerns from the services themselves, and end up with a "naive service", which I will here define as:

            1. trusts that actions requested to be performed on a resource are allowed.
            1. trusts that any information requested is allowed to be returned.

        - response: |
            At the highest level, the cluster is divided into 3 concentric circles of trust:

        - response: |
            1. No trust (outside world)
            1. Authentication trust (within cluster)
            1. Authorization trust. "Naive" (within pod)

          figure:
            figure: naive-cluster-overview
            miro: '3458764548628056239'

        - response: |
            The barriers between these circles were enforced by OPA.
            Outside of the authentication barrier, we are in a no-trust environment.
            Once we pass through the authentication barrier, communications between the services can trust that the request was from who it says it is.
        - response: |
            #### Authentication Barrier
            An OPA service was installed alongside Envoy at the ingress gateway of the cluster. OPA would evaluate the request after obtaining information necessary to reason about authentication legitimacy. This information could be provided as a result of an OIDC workflow, some kind of Active Directory authentication, or a variety of other authorization methods we supported via an adapter pattern described [later in this document](#authorization-adapters). This policy could also be a passthrough, and assume whatever security handshake that happened earlier in the request flow validated user identity correctly.

            _note: the authorization data in the diagram is separated logically from the request, but the information in practice would likely come from the request._
          figure:
            figure: authentication-barrier
            miro: '3458764548747400083'
        - response: |
            #### Authentication Workflow
            This workflow diagram llustrates the flow of data through the authentication barrier. Note the 'Envoy' nodes represent the same Envoy service on the edge of the cluster.

          figure:
            figure: authentication-workflow
            miro: '3458764548844659047'

        - response: |
            In the proof of concept, I used Envoy to insert an "Identity" header into the request after the request was validated through whatever mechanism the client wanted to use. We offered a Keycloak instance as an IdP  (Identity Provider) with OIDC capabilities, but we also would accept OIDC credentials from a trusted 3rd party; or many other authentication mechanisms - we followed an adapter pattern for normalizing auth requirements and data, which I talk about [later on](#authorization-adapters).

            Regardless of the mechanism used, in the end an OPA policy always gated access to the cluster. The client's authentication data was evaluated against OPA policies, which had the final say. This, like all autoinjected systems in the cluster, could be opted out of via configuration.

            This implementation, however, has a potential vulnerability in that we would need to scrub an "Identity" header off of the incoming request. Since all requests were coming in through the ingress controller that Envoy was manipulating the headers from, in theory this wouldn't be a problem - but I would have rather encapsulated the request in an envelope larger than the request itself. I had learned with Meshblu that separation between data and metadata is very important, especially with regard to security.

            In addition, this header could be manipulated by a malicious service within the cluster. The header solution was good enough for a proof of concept, but further research would be needed to create an envelope for the request that sufficiently separated "userland" requests and cluster metadata. Anyway, that's a mini self-audit of this implementation.

            Past this barrier, the services could trust that the request was from who it said it was, but they could not (yet) trust that the request was authorized to do what it was trying to do.

        - response: |
            #### Authorization Barrier
            After the authentication barrier is passed, any subsequent OPA policy evaluation can assume that the identity presented to the policy for evaluation is accurate, which allows for simple, decoupled policies to govern authorization on a per-service pod basis. You may notice below the similarity between the cluster-protecting authentication barrier, and this authorization barrier.

        - response: |
            An OPA service was injected into each Kubernetes pod, alongside Envoy. Work was being done on embedding OPA as a plugin to Envoy (also injected by default in Istio).
            Also by default, the OPA service would only do the authorization gating on ingress traffic - though evolutions of this design allowed for OPA to filter egress traffic as well. But we'll get to that later.
          figure:
            figure: authorization-barrier
            miro: '3458764548747400083'
        - response: |
            #### Authorization Workflow
            This diagram shows the flow of data through the authorization barrier. Note that all 'Envoy' nodes refer to the same service.

            At this point, the request is coming from inside the cluster, with the rewritten identity header.
          figure:
            figure: authorization-workflow
            miro: '3458764548844659047'

        - response: |
            Past this point, we have what I'd consider "half" of a naive service: the service can trust that actions requested to be performed on a resource are allowed. If this is as far as I could take the cluster, it still provides a lot of value. But the service still needs to do some logic as to what a particular user can see, which is unsatisfying. But let's take a little break from these barriers, go depth-first, and talk about the origin of the authz/n information.

        - response: |
            #### Authorization Adapters
            One of the goals of the cluster experiment was to support existing security infrastructure transparently. OPA was used as the only gate for both trust boundaries, but the clients may need information from their existing IdP (identity provider) in order to reason about access control. This entails using data or existing rules from Active Directory, Okta, or various other IdPs within the logic of a policy.

        - response: |
           I accomplished this by making several microservices that used the adapter pattern to ETL (extract,transform,load) any information necessary for policy evaluation from various IdPs to OPA's internal, ephermal datastore.

            I preferred this method to OPA policies doing http requests to our adapters, as I feel doing http requests and parsing the results make policies much more difficult to understand, as the policy now has to know and deal with implementation details, which defeats the purpose of having a restricted policy language.

          figure:
            figure: identity-provider-data-to-opa
            miro: '3458764548738949056'

        - response: |
            #### Authorization Egress Filters
            _warning: we are entering the area in which performance could be a concern. Using the OPA service is a good way to reason about this approach, but other methods can be used to accomplish the same effect. An example of this is [generating SQL from OPA policies](https://blog.openpolicyagent.org/write-policy-in-opa-enforce-policy-in-sql-d9d24db93bf4), but I am not a fan of this as it requires services to have special knowledge re: both OPA and the database. But still, it's better than nothing!_


            You may have noticed an asymmetry in the [authorization barrier](#authorization-barrier). While the request to perform an action is gated by an OPA policy, the response (egress from the pod) is not. As a result, a service within a pod may need to make assumptions regarding the security model.

        - response: |
            One way to think about policies is that they are 'gates' that control access to a resource. This is the most common use case of an OPA policy.
        - figure:
            figure: opa-policy-gating-a-single-resource
            miro: '3458764548858771011'
        - response: |
           We can, however, use the same gating policy over the set of all resources. This returns all the resources a user is allowed to see, based on the 'gating' policy via [partial evaluation](https://blog.openpolicyagent.org/partial-evaluation-162750eaf422)

        - figure:
            figure: opa-policy-filtering-resources
            miro: '3458764548861602424'

        - response: |
            _this, of course, has implications when it comes to pagination, but I had some wild ideas that are outside of the scope of the experiment_.
        - response: |
            With the addition of this filtering, the authorization barrier is now symmetric, with OPA both gating access to resources and filtering the response. At this point the pod architecture allows for a service to be fully naive.
          figure:
            figure: authorization-barrier-with-filter
            miro: '3458764548866168330'

        - response: |
            We can even run this policy as a filter on a different microservice, as long as enough variables are applied via partial evaluation. This is a bit trickier though, and this answer is already ludicrously long. Just keep in mind that the resources returned by this microservice + selected policy would filtered __again__ by the egress filter on the microservice pod.
        - response:
          figure:
            figure: opa-policy-filtering-other-domains
            miro: '3458764548863225249'
        - response: |
            Finally, the OPA policy that filters responses doesn't have to be the same policy that gates the requests. They can be totally unrelated. I reused the gate as a filter in the above examples to demonstrate how much mileage you can get out of a single policy using this architecture.
        - response: |
            #### Ok, well hopefully that was enough
            I hope that the above description of my experimental cluster design is enough to demonstrate some of my security engineering experience - particularly with regards to OPA. I plan on referring to this answer in subsequent prompts.

            I hope somebody took the time to read this, and that it was at least somewhat understandable. It was a lot of work recreating this from memory! Thanks for giving me a soapbox.


            I am obviously interested in doing more research in this area.

      - prompt: Describe your experience integrating OpenID Connect providers or using OpenID Connect libraries in your projects.
        answers:
        - response: |
            I both consumed OIDC responses from external identity providers, and run Keycloak as an OIDC provider in the cluster described above. Besides that, I can't think offhand of other work I've done regarding OIDC except to study the workflow in order to build the cluster.
      - prompt: Describe your experience with container technologies such as Docker, LXD and Kubernetes.
        answers:
        - response: |
            I have used Docker extensively throughout my personal and professional life. I'm running a Minecraft server, a duckdns image, and a Plex media server on this machine as I type this! I also have Podman, but ironically I have to run Podman with sudo, and I can run Docker without. Which, come to think of it, is a pretty big security vulnerability on my home machine...

            I often set up development environments for people that use docker compose liberally if we're working on a nontrivial project that requires multiple services. I also use Docker to run tests in CI/CD pipelines. In fact, I often use the same docker-compose.yml file for both development and CI/CD, spinning up the entire stack for testing before deployment.

            I have a few personal repos that at least demonstrate my interest in Docker that I might as well list off:
             - [here's a repo](https://github.com/redaphid/docker-compose-ecs-test) of me messing around with Docker Compose deploying to AWS! There is almost literally nothing in it, but if you're curious you can go through the commits and see what I was trying out.
             - [here's an older repo](https://github.com/redaphid/jad) of me using Docker in various ways on a home server. In the `services/plex` directory you can see me do a little fanciness where I encrypt my Plex server preferences so I don't leak my token and can still have a public repo.
             - [here's an older repo](https://github.com/redaphid/docker-ntp-server) Where I was running a very accurate ntp server in this effort to make "indoor gps" via ultrasonic emitters and detectors.

              _tl;dr: I think it was 3 Raspberry Pi's that each had their own ultrasonic frequency. I was trying to triangulate the position of a device in a room that had an ultrasonic detector. It didn't work. But I learned a lot about how bats use ultrasonic frequencies to navigate!_

            It looks like after a while I started using ZFS and relied on automatic snapshots to keep my Docker configs at home.


            As you probably can imagine from the cluster experiment, I am at least somewhat familiar with Kubernetes. To be honest I usually have to spin up and look in to using Kubernetes whenever I get my hands dirty with the copius YAML config. Kubernetes is the pragmatic choice for all enterprise development right now, but personally I find it to grossly violate the Unix Philosophy with it's complexity. That said, Kubernetes is important. I have used it in the past, and will again. Personal disagreements notwithstanding.

            I prefer Docker Swarm, but I know I'm a minority at this point.

            Regarding LXD: I just looked in to it, and it sounds pretty awesome! I haven't tried it, but it sounds like it might be helpful for me.

            I (like any normal person) run a gaming vm inside my Linux box. Just because I don't want to have a "real" machine in my house that runs windows.

            ```
            <username>@mortal ~> sudo virsh list --all
            Id   Name          State
            ------------------------------
            1    nx3           running
            2    bf3           running
            3    win7-2009     running
            -    bt2_testbed   shut off
            -    face          shut off
            -    face2         shut off

            ```
            The little I just read about LXD sounds like it might be a rabbithole I should avoid until I fill out the rest of this document.

        - response:
    - title: General Software Engineering Experience
      prompts:
      - prompt: What kinds of software projects have you worked on before? Which operating systems, development environments, languages, databases?
        answers:
        - response: |
            I pride myself in being a generalist. To that effect, I have worked on a huge variety of different systems, languages, etc. ranging from microcontrollers, to the open-source Internet of Things platform [Meshblu](https://github.com/octoblu) I talk about a little [here](#experience-with-meshblu), to giant fancy clusters that you may remember me describing in a previous response :).

            I helped out with developing a new  closed-source browser recently, which was interesting. [Here](https://github.com/loqwai/experiments-messageport-react) is a repo where I experiment with distributing state via Message Ports to be rendered with React. Electron has some crazy way of fusing node-side and browser-side MessagePorts together, which is important, because when making a browser you have to deal with a lot of different processes, and you really need to have a structured way for them to communicate with each other.


            I like working on platforms when I get the chance - though this is no means required. Working at Octoblu on the IoT platform and the flow-based execution engine that ran on top of it was by far the most satisfying work of my career so far.

            I have a feeling I'll need to talk about Octoblu and Meshblu a lot more further on in this document, so I won't go in to this any more right now. But [here](https://www.youtube.com/watch?v=g9sLUn_lPaQ&list=PLugWVgJZBNjY_jtZBLZnQmtwr402bC6We) is a YouTube playlist featuring the flow execution engine if you're interested in seeing it in action.

            As far as languages go, I've used a ton of them. I'm a language nerd. I soapbox about Lisp when I see an opening in conversations.

            These days I'm doing a lot of Typescript, but:

            - [here](https://github.com/loqwai/juniper-gardens-twig-debug)'s a repo where I helped an open source project by fixing a bug in their implementation of the ESPHome protocol that allows esp32 microcontrollers to get wifi credentials over BLE. (Javascript. On a microcontroller. This was not my idea)
            - [here](https://github.com/loqwai/extra-color-perception)'s a repo where I make hardware pendants that each glow a different color, and that color changes depending on how physically close you are to someone else wearing a pendant with a different color. (C++ on esp32s, using platform.io).
            - [here](https://github.com/loqwai/carbonaria)'s a repo where I learn Rust by making a game with my friend!
            - [here](https://github.com/redaphid/fish-shell-stuff)'s a repo of all my Fish shell functions I clone down when I set up a new machine or VM.
            - [here](https://github.com/redaphid/resume-canonical)'s the repo I used to generate this document. Mostly using Javascript, HTML, Github Actions, and madness.

            I've used Go somewhat recently for professional, closed-source work. I'm catching up with Python to learn more about the latest AI craze. I coded in Java and C# in the distant past, when I didn't know any better.

            As far as databases are concerned, I've used Firebase, Postgres, and MongoDB recently-ish. If Redis counts, I've done some of that. But it probably doesn't.

            The particulars of a given database haven't affected my career much so far. I try to get to the point where I'm abstracting the details as soon as possible, and rarely think about it again unless some problem crops up.

            Regarding development environments, I'm all-in on vscode and Linux. I bought Red Hat 6.0 at a Gamestop in high school by saving up my lunch money. I had to buy a Macbook for the aforementioned browser project, and I'll use it occasionally because the hardware is fantastic. Please someone update the Linux kernel to support this hardware! I want out of Mac-land.

      - prompt: Would you describe yourself as a high quality coder? Why?
        answers:
        - response: |
            Yes, I would describe myself as a high quality coder.
            I think it all boils down to philosophy, with some experience mixed in.

            #### Philosophy
            I have a syncretic coding philosophy that I've developed over the years, and love to talk about. Not much of my philosophy is novel, but I steal a lot from others. To wit:
            * Unix Philosophy
            * Worse is better
            * The Zen of Python (with the exception of 'flat is better than nested')
            * Pragmatic Programmer
            * Some of Uncle Bob's Clean code.
            * Test Driven Development
              - I have a game that makes this fun, I swear!
            * Strong opinions, weakly held.
            * Some of "xtreme programming"
            * "premature optimization is the root of all evil"
            * Ping-Pong pairing.
            * "The best code is no code at all"
            * The "[Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise)" as a heuristic for code complexity.
            * The Principle of Least Surprise
            * "Branching by Abstraction"
            * "Make it work, make it right, make it fast"

            ...and many others.

            I simplify interfaces exposed to whatever is consuming my code as much as possible, exposing the bare surface area necessary for it to be functional. I try to use the simplest data structure for inputs and outputs.

            When it comes to code complexity, I believe "There are either obviously no errors, or no obvious errors". Defects hide in complexity. Keep your code as simple as possible - but no simpler.

            I apply the scientific method to my coding philosophy, and allow it to evolve over time, based on new information and experiences.

            I try to use the Socratic Method whenever possible during architectural or procedural debates.

            I really value collaboration. My brain probably can't come up with the best solution to a problem on its own. I like to bounce ideas off of other people.

            I like finding out I was wrong about something! I enjoy well-intentioned critique of my work, and am happy to do it for others as well. I foster a culture where people "like it when they are wrong", and are open and eager to hear criticism. Because how else can we evolve?

            I feel a major part of our job is the fight against entropy, and finding the simplest way to accomplish our goals.

            I think critically about the solution I am developing, and focus on developing what we actually need by checking my assumptions regularly.

            I commit constantly, as you can see in any repo I've ever worked on. I make the features I'm working on as small and atomic as possible.

            I've mentioned a few times that I pride myself in being a generalist. And one of the reasons for this is I think it's good for you. It keeps the mind flexible, and just knowing what's out there in our huge field of work gives you more "tools" in your mental toolbox. I don't have to remember how to do a binary search, but I know that it exists, and what it's for. Knowing things like that helps you to design things better.
            For example, learning about Rust's "match arms" and "match guards" has caused me to think about switch statements and guard clauses in other languages in a different way. I know if I see a problem that looks like something a match arm would make easier, there might be a development pattern I can use to imitate them, even if I'm not in Rust.

            #### The Game
            The first thing that always comes to mind when I think about my good developer habits is this "game" I play that was originally for pairing. But I "play it" with myself when I can't pair, assuming both roles.

            The game goes like this:
            1. "Member 1" of the pair writes the simplest possible test that fails, given the current codebase.
            1. "Member 2" of the pair writes the simplest possible code that will cause the test to pass - even if it's a function that returns a constant present in the test!
            1. The roles in the pair are reversed, and the loop continues.

            As a heuristic for what constitutes the "simplest possible code", I use the [Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise), moving from simplest->most complex code on this scale, increasing in complexity only when forced by a test to do so.


      - prompt:  Would you describe yourself as an architect of resilient software? If so, why, and in which sorts of applications?
        answers:
        - response: |
            Yes, I would describe myself as an architect of resilient software.

            The statements re: being a [high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder?-why?) apply here. I'm guessing that is a given, but I figured I'd refer to it just in case.

            One way characterize architectural resiliency is by how well the system recovers from failure. And failures will happen.

            The domain in which I deal with the importance of resiliency the most is (perhaps obviously) cloud services. But I think the principles I refer to below apply to any system.

            One of the things I do is paradoxically make sure a system crashes out early if some critical expectation is not met. By making things crash as soon as possible, you an expectation that things may be crashing all the time, which causes resiliency to be a concern from the beginning.

            Detecting failures and replaying any actions attempted during the failure helps - as long as the actions don't have side effects that can't be undone.

            Things like blue/green deployments are great for resiliency, along with easy rollbacks to previous versions of subsets of the system.

            I'm excited by the idea of [Principles of Chaos](http://principlesofchaos.org/).

            Things like [circuit breakers](https://martinfowler.com/bliki/CircuitBreaker.html) help.

            I think of resiliency not as a "feature" of a system, but rather a property of the system. It's not easy to add to a system after the fact. It's something you have to design into the system from the beginning.

      - prompt:  What is your most senior role in a software engineering organisation? Describe your span of control, and the diversity of products, functions and teams you led.
        answers:
        - response: |
            I was Staff at Citrix for 3 years, during which time I would sometimes manage multiple teams in order to accomplish a goal. At Octoblu (startup acquired by Citrix), I co-architected and managed a few different initiatives - the major ones being the Meshblu 2.0 permission system referenced earlier [here](#experience-with-meshblu), and the "Flow Engine" - a flow-based execution engine that ran on top of our open source, free-to-use IoT platform Meshblu. We deployed the flows at large scale, using redis queues and workers, taking advantage of any asynchronicity we could to make the system as fast as possible. In the beginning, the system used a Docker container for each flow, but as you can imagine, this did not scale.

            These were difficult, highly technical problems to solve, and we were executed them very effectively, in a short timeframe.

            I also assumed some leadership roles for the front end of the flow system, in which people could drag-and-drop nodes representing devices and wire them together.

            I later was team lead for an R&D focused team within Magic Leap, which had a lot fewer responsibilities, but was still a lot of fun.

      - prompt: What is your proudest success as an engineering leader?
        answers:
          - response: |
              I'm sounding like a broken record now, but I am extremely proud of the work I did on the Meshblu platform. It's architecture and design is great, the system was functional and processing 4 million messages a day. Translating the configuration and messaging systems of hundreds of types of devices and services into one \*ahem\* **canonical** system and having the system work well was a huge success.
              The flow engine was a very technical challenge, and the whole team working on it were very effective at executing on it. It was a great experience.

      - prompt: Outline your thoughts on open source software development. What is important to get right in open source projects? What open source projects have you worked on? Have you been an open source maintainer, on which projects, and what was your role?


      - prompt: Describe your experience building large systems with many services - web front ends, REST APIs, data stores, event processing and other kinds of integration between components. What are the key things to think about in regard to architecture, maintainability, and reliability in these large systems?
        answers:
          - response: |
              Meshblu, again, is a great example of my experience with this. We had hundreds of **unique** services, translating millions of messages between many protocols, to many devices and services. Most of these services were themselves using Meshblu to handle requests, responses, configuration, and data storage, because they appeared to be a "device" within the system. This allowed us to delegate all event processing, data storage, and integrations into Meshblu itself.

              Though not everyone is fortunate enough to have this sort of infrastructure built in in a cloud-spanning way, the principles of the system are the same. The services should be as simple and decoupled as possible, exposing a small, sensible API. We should strive to delegate as much of this complexity as possible to the environment around the service. I'm starting to notice a theme in my responses here.

              I don't believe in forcing uniformity upon any single service in the system as long as it is small - and services should be, unless there is a good reason to make them larger.
              Rather, we should make the path to uniformity so simple and easy that people want to walk it. Libraries, generators, templates, guides, and things of this nature should be provided to make service development as easy as possible - but not forced upon anyone. This is an evolutionary approach to making good tools that I find to be very effective.

              Small services that have responsibilities pushed as far out to the edge as possible are easy to maintain, and easy to replace. The heuristic I mentioned earlier that software either "obviously has no bugs" or "has no obvious bugs" is a good one when thinking about maintainability and reliability.

              Architectures of large systems should provide environments that help the services within push those responsibilities out. I don't know if you remember the [way too long](#the-totally-naive-services-cluster--open-policy-agent-experience) description of my experimental cluster architecture earlier, but I think it's a good example of how to think about responsibility delegation in a large system.


      - prompt:  How comprehensive would you say your knowledge of a Linux distribution is, from the kernel up? How familiar are you with low-level system architecture, runtimes and Linux distro packaging? How have you gained this knowledge?
        answers:
        - response: |
            This is an interesting thing to try to quantify.
            I think the best way I can answer this is via a few personal anecdotes, and some reflection as to what that means.

            I first used Linux when I bought Red Hat 6.0 from a Gamestop by saving my lunch money in high school. It didn't work for teenage me. I couldn't get the installation to finish. I had to retreat.

            Freshman year in college I decided to get Gentoo (stage 2) running on an old P4 with 128mb of ram, because I was trying to use it as a DVR, and needed all the performance I could get. It compiled for easily over 12 hours. The beeping from the speaker in the motherboard all night I'm sure haunts my then-roommate to this day. But I'm sure all the effort I put in to removing all printing capabilities from all the software on that distro via some kind of flag system emerge used was "worth it". And it worked.

            During this time I had a laptop whose cd-rom had failed, and the bios wouldn't detect the hard drive. I used Puppy Linux on a floppy to get the laptop to a usable state. And I used it quite a bit.

            I've tried to use Alpine as a "hypervisor" that just ran virsh and everything else would be a container, or VM, but that failed because Alpine is a huge pain with it's state management when it's not in a container. So I returned, once again, to Ubuntu. And that machine has been fairly problem-free.

            I tried Void Linux close to 2 years ago, because I wanted to use a "bootloader" that allowed me to select from different ZFS datasets to use as the root filesystem for the machine. I wanted to use [ZFSBootMenu](https://docs.zfsbootmenu.org/en/latest/) to do this, and the tutorials all used Void. I got this to work, but it turns out systemd is pretty important these days, and I had trouble using Docker (well, Rancher) with it. Also the Docker daemon hates ZFS.

            I know, for example, that ZFSBootmenu is not an "actual bootloader", but a Linux kernel that later (I believe) `kexecs` somehow to replace itself. Unfortunately I was trying to use VFIO to pass through a GPU to a VM and it didn't work. I had suspicions it had something to do with this. So I used Ubuntu again, and that's where the VM is hosted to this day.

            I've accidentally been in the initrd environment a few times, and was confused and amazed by what was going on. Then did research in to how it works. I think it loads the binary with this minimal set of tools directly in to ram before continuing to start the system? I have messed around there to try to save machines in the past.

            I've been curious re: musl, but sadly think it'll never catch on.
            I like the "everything is a file" stuff.

            I've dd'd my own hard drives on a few occasions.

            I have a use an old machine in my home network as a bastion host for ssh'ing into my VM server. Running Ubuntu, of course.

            I like the "everything is a file" system, and yearn for an alternate timeline where Plan 9 became the dominant OS.

            My `/etc/apt/sources.list.d` looks like this:
            ```
            <username>@mortal ~> ls -1 /etc/apt/sources.list.d
            1password.list
            cuda-ubuntu2204-12-0-local.list
            docker.list
            fish-shell-ubuntu-release-3-jammy.list
            google-chrome.list
            pop-os-apps.sources
            pop-os-release.sources
            signal-xenial.list
            spotify.list
            steam.list
            system.sources
            vscode.list
            ```
            _yes, it is Pop :/_

            I'm not sure how these anecdotes roll up to a quantitative measure of my Linux knowledge, but I think you get the gist.

      - prompt: Describe your experience with large-scale IT operations, SAAS, or other running services, in a devops or IS or system administration capacity
      - prompt: Describe your experience with public cloud based operations - how well do you understand large-scale public cloud estate management and developer experience?
      - prompt: Describe your experience with enterprise infrastructure and application management, either as a user running enterprise operations, or as a vendor targeting the enterprise market
      - prompt: Outline your thoughts on quality in software development. What practices are most effective to drive improvements in quality?
        answers:
        - response: |
            Hm. If we're talking about how to write quality code, I've answered the question pretty thoroughly [when talking about why I think I'm a high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder--why-). This must be a different kind of quality.

             If we're talking about improving code quality at a team level, I usually focus on collaboration and enthusiasm. Coding is fun! I have an open discussion with whoever is interested re: heuristics when it comes to "quality code". We can agree or disagree about them, and revisit them from time to time. But we at least know they exist, and have a lexicon for them.

            A culture of collective ownership when it comes to code is important, so people don't end up in silos, disconnected from the rest of the project.


      - prompt: Outline your thoughts on documentation in large software projects. What practices should teams follow? What are great examples of open source docs?
      - prompt: Outline your thoughts on user experience, usability and design in software. How do you lead teams to deliver outstanding user experience?
        answers:
          - response: |
              I took a few design classes in my graduate studies, but I wouldn't consider myself an expert in user experience. I do, however, have a few "tools in the toolbox" that can guide me along.

              The first thing I think about when I think about design is what I took away from reading Klaus Krippendorf's "[The Semantic Turn](https://www.amazon.com/Semantic-Turn-New-Foundation-Design/dp/0415322200)". I won't pretend I've read all of it, but the concept of "affordances" made such an impact on me that it is the first thing I think about when I think about design.

              My understanding of affordances is that they are "properties of an object that broadcast what can be done with it, and how". To that effect, I think the primary consideration when designing user experiences is to make the affordances of the system clear.

              This applies to humans and machines alike.

              Of course, in practice, this cannot be the only concern. We can't have a UI that's just a series of labeled components in a big, gray list. Style is important, and much more than affordances are communicated with a good design.

              Having a consistent design language is nice, for graphic design as well as UX. They are both striving to convey information in non-verbal ways. I like to think about design as a language, and I think it's important to have a consistent vocabulary. But it also can't be like "newspeak" from 1984. We need some variation to communicate nuance.

              Another source that springs to mind every time I think about design is [The Humane Interface](https://www.amazon.com/Humane-Interface-Directions-Designing-Interactive/dp/0201379376), by Jef Raskin. I read this book long after it came out, but the principles are still relevant. I am famous amongst coworkers for being very against modals, because this book made such a strong argument.

              I also like the idea of a ZUI ("Zoomable User Interface") from that book a lot, and I find keeping that idea in mind can inform some design decisions - even if there is no obvious "zooming" involved. And also universal undo/redoing.

              I was passionate enough about these principles that I led the design of Octoblu's visual editor for the Flow Engine so we would follow these 3 principles, as well as many others. You can see an example of how the UI works [here](https://youtu.be/g9sLUn_lPaQ?t=267). You may notice the lack of modals during configuration, the infinite, zooming canvas, and the universal undo/redo visible in the upper right in this video.

              I use these principles to make design decisions. But more than that, I emphasize the usage of principles in general when it comes to leading graphic design. As is a repeating theme throughout this document, I like to have a few heuristics to use as metrics for how "usable" a design is. And the heuristics themselves must evolve over time, and - following the scientific method - be open to revision.

              I'd imagine someone who is interested in communication via design would take something like a style guide in to account. Say, read up on the colors, tints, font weights, and visual hierarchies. Try to understand the information this design pattern is trying to communicate.

              And then make a ton of new, unique diagrams using that style guide's color scheme and visual hierarchy. Maybe create a whole new document writing system to experiment with theming documents in the spirit of said style guide while still allowing for exceptions to the theme via modification of the code. Spend time thinking about how to communicate at least __some__ information in that document visually, and semantically. Accidentally cause the semantics of \<em\>(emphasis) tags to become ambiguous, and then _not be able to use emphasis in markdown without looking like asides now._

              But who am I kidding - no one is curious enough about design to put all that effort in to a written interview.

      - prompt: Outline your thoughts on performance in software engineering. How do you ensure that your product is fast?
        answers:
          - response: |
              It must be measured. And we have to define what "fast enough" is. According to Donald Knuth, "Premature optimization is the root of all evil".

              Often people will see suboptimal things like nested `for(` loops, and make the assumption that they are the cause of poor performance. But these loops may often run a few times. A colleague of mine once told me that "the average loop in code runs like 6 times". I don't know if that's true, and searching for "average loop iterations computer science" looks like a chore to go through. But it's an interesting thought to consider while analyzing the code by hand. If it's an extremely inefficient loop that runs 6 times, I'd much rather we use the slower, more semantically accurate code if those 6 interations aren't very complex.

              But more importantly, you shouldn't be doing that optimizing by hand if you can help it. Write the code that is easy to understand first. Premature optimization is the root of all evil. Then, if you have a performance problem, profile it. Find the bottleneck. Then optimize that. Don't optimize the code you haven't written yet, unless it's fairly easy to do.

              I tell a story [later on](#what-did-you-achieve-at-university-that-you-consider-exceptional-) about how I got an automatic A in assembly class without writing any assembly. This is because people misunderstood how to be performant in assembly, vs a C optimizing compiler that's specific to the CPU architecture you are targeting. By writing C code, I was able to explain to the compiler what I wanted to achieve semantically, and the compiler knew how to optimize for that specific CPU better than most humans could. The compiler knew about fancy and obscure assembly instructions and how to use them.

              Computers are pretty good at talking to other computers.

              That said: does performance matter? Yes. It can become an issue. In my opinion, you should mitigate risk of performance issues the same way you mitigate risk with everything in software: contain things in very small, well-defined interfaces. If you've done things right, you should be able to replace the implementation of the interface with a faster one without the rest of the code knowing about it. I do this even in microcontroller work where performance is often an

              I also ascribe to the whole "Make it work, make it right, make it fast" mantra.

      - prompt: Outline your thoughts on security in software engineering. How do you lead your engineers to improve their security posture and awareness?
      - prompt: Outline your thoughts on devops and devsecops. Which practices are effective, and which are overrated?
    - title: Industry leadership experience
      prompts:
      - prompt: Describe your speaking experience at industry events and conferences
        answers:
        - response: |
            I've spoken at a few conferences, and ran quite a few hackathons. I think my biggest talk was at NodeConf, where I spoke negatively about Kubernetes. haha. I have since conceded the victory to Kubernetes.

            I was on the local speaking circuit a bit w/regards to IoT, and ran a few workshops sponsored by Intel to get the word out about their IoT offerings.

            Personally, I would teach Javascript in a monthly meetup, and go to local hackerspaces to teach some people to use the tools I was using, and they taught me theirs. I would also go to local meetups and talk about the things I was working on. I also taught kids to program a couple of times, using [Processing](https://processing.org/).

      - prompt: Are you a thought leader in any particular area of technology?
        answers:
        - response: |
            I have a lot of thoughts about software development practices, but I don't tell many people about them, and I don't know how unique they are. tl;dr "no".
      - prompt: Describe any experience working with startups. What did you draw from that experience that would be relevant for this application?
      - prompt: Describe any experience working in a public company. What is important for your colleagues to know about being a public company?
    - title: Education
      prompts:
      - prompt: How did you fare in high school mathematics, physical sciences and computing? Which were strengths and which most enjoyable? How did you rank, competitively, in them?
        answers:
        - response: |
            I was a good student in high school. I was an 'A' student in physical sciences. They were fun! I wasn't at the time a fan of math; I was too busy writing calculator games on my ti-89 (I was fancy) in Basic in class. My family had a kind of 'anti-math' bias. I was taught at the time that it was useless. I have since became pretty frustrated about this, and read up on math theory in Wikipedia sometimes.

            I was, however, very good at programming.

            The computer science curriculum in high school coincidentally began my freshman year. Well, they had "Basic Computer Training".
            I was under the mistaken impression I had to take this class to graduate. A classmate overheard me complaining about this, and told me to take "Visual Basic" instead. I wish I could let them know what a difference that made in my life.

            I was part of the first generation to take computer science classes in the school. There weren't many of us. But we were all there because we wanted to be.

            I don't know if I ever got anything less than 100% on any assignment. It was so much fun. I had arms races with my friends as to who could make the most annoying program. I learned DirectX in VB6. I made programs to "spy" on the library computers. I couldn't get enough. I took literally every CS class offered in the school, every year. As electives.


      - prompt: What sort of high school student were you? Outside of class, what were your interests and hobbies?  What would your high school peers remember you for, if we asked them?
        answers:
        - response: |
            I was a good student in high school - people thought of me as an academic. I was extremely in to computers, and so I went to LAN parties a lot with my fellow techies. I liked paintball. I took Wing Chun Kung Fu. I worked as a programmer after school at a place I [talk about later](#describe-some-high-school-achievements-considered-exceptional-by-peers-and-staff). I went to punk rock shows. I liked cars.

            My high school peers remember me as a funny, good-natured, intelligent person who occasionally wrote very deep poetry in his notebooks. And obviously, yes, they remember me as a computer nerd. But they remember my attitude first, which I appreciate.
      - prompt: In languages and the arts at high school, what were your strongest subjects and how did you rank in those among your school peers?
        answers:
        - response: |
            I did well in language arts in high school. I was picked to be the "Best at English" for the yearbook my graduation year, and have been able to brag about it at parties ever since. I got 5s in my AP English classes, and my teacher in Honors English actually remembered me when I ran into her years later.
            I was an 'A' student in anything language arts. But if "Best at English" in a high school yearbook means anything, then I was the best at English.

            (Honestly, though: This was chosen by the English teachers. I was actually the top student with regards to English).
      - prompt: Describe some high school achievements considered exceptional by peers and staff
        answers:
        - response: |
            Did I mention I was the best at English?

            The English teachers really were pretty impressed by me. As were, of course, the Computer Science teachers. They employed me to do networking in classrooms and help various staff members with their computers.

            I was recruited by a subcontractor to Honeywell who was working on flight management systems for Airbus A320 aircraft and looking for high school kids he could pay $11/hr to to do some of his work for him when I was 16. This was awesome.
            The staff were pretty impressed, as were my friends and parents. I had a great time, learned a lot, and made a lot of mistakes that I sincerely hope were caught by somebody.

      - prompt: Which university courses did you perform best at? How did you rank in your degree?
        answers:
        - response: |
            I did the best in computer science. I went to a liberal arts school, but I think I took every CS class they offered. It is a small school, and I bet the teachers remember me. I insisted on taking 3d graphics my first semester freshman year, and the school was flexible enough to allow me to do it. It was pretty rough going in to that class without taking any algorithm or datastructure classes yet, but I had been programming for fun and work long enough that I did pretty well.

             I don't think there were 'rankings' for my degree.
      - prompt: Outside of class, what were your interests and where did you spend your time?
        answers:
        - response: |
            I spent a lot of time driving around, getting lost in the Pacific Northwest, and - obviously at this point - messing with computers.
            My school had a program in which they would give you a \*house\* instead of a dorm room if you had an academic theme and put on a few events related to that theme per semester.

            The program was similar to the fraternity/sorority system.

            My friends and I founded "Robothouse", with 8 of us living in a smallish 2-story house, with me in the basement. We had a permanent LAN party, and sometimes big ones that would fill the house when friends would stop by.

            I also worked part-time for the school, writing software that interfaced with smart routers via SNMP. This was during the age when viruses were running wild in Windows-world. All the students on campus were connected to the same LAN, and were all pirating software for years. When the semester started, 1000s of these computers would plug in at once and viruses spread like wildfire.

            My job was to work on this software that would scan each computer when they signed up to use the network for the first time using Nessus. If Nessus found any critical vulnerabilities, we'd lock their computer in to a vlan where they could only access Windows Update (and maybe a few other services). Then they could update, try again, and the cycle would potentially repeat.

      - prompt: What did you achieve at university that you consider exceptional?
        answers:
        - response: |
            I took the hardest possible computer science classes first, then wrapped around and took the 1-200 levels to finish my degree when I needed a break. Founding Robothouse was pretty cool, and it went on for a couple of years after I graduated. Which made me pretty happy.

            I received an automatic 'A' in assembly class, without writing much assembly. My professor was making a homemade image manipulation program, and one of the class's assignments was to write the algorithm for some subset of that program. I think it was color normalization for my year.

            There was a competition where, if your algorithm was faster than the teacher's, you got an automatic 'A'. So I made a faster algorithm in C, using tons of compiler flags, and optimization levels for the particular CPU we were running it on. I got an 'A' without writing too much
            assembly.

            I then made a header file and a compiler script, and sent it to everyone in the class, saying they could use it if they kept my name in the comments of the header

      - prompt: At high school or university, what leadership roles did you take on?
        answers:
        - response: |
            Besides founding Robothouse, nothing readily comes to mind. I preferred organizational structures that didn't require an explicit 'leader' role. Like I liked to have leadership be an emergent property of the group, rather than something that is explicitly assigned.
    - title: Context
      prompts:
        - prompt: Outline your thoughts on the mission of Canonical. What is it about the company's purpose and goals which is most appealing to you? What do you see as risky or unappealing?
        - prompt: Who are Canonical's key competitors, and how should Canonical set about winning?
        - prompt: Why do you most want to work for Canonical?
        - prompt: What would you most want to change about Canonical?
        - prompt: What gets you most excited about this role?
          draft: true
          answers:
          - response: |
              I don't know if you've noticed the [huge, 10 page long](#the-totally-naive-services-cluster--open-policy-agent-experience) description of this experimental cluster I worked on where I talk a lot about my interest in the theory of security in distributed systems.
