doc:
  logo: images/interview-icon.png
  title: Written Interview
  miro: https://miro.com/app/board/uXjVMeJA43k=
  repo: https://github.com/redaphid/resume-canonical
  intro: |
    #### Structure of this Document
    When responding to these prompts, I found answering them in a discrete fashion made the discussion too fragmentary at times. I have therefore organized some responses into descriptions of concrete solutions in my career that utilize the concepts in question. I provide detailed descriptions porkbarreled in to answering the questions as I encounter them, and refer to these descriptions in subsequent prompts. I have also included some diagrams to help visualize the concepts I am trying to communicate.

    I hope this format makes sense to you, though I won't be able to clarify anything due to the anonymity of this process.

    If you're interested in seeing how I think, looking at the commits in [this document's repo](https://github.com/redaphid/resume-canonical) is seeing me in my natural habitat. I didn't plan on bringing attention to it until now.

    The diagrams were made in Miro. [Here is the link to the board](https://miro.com/app/board/uXjVMeJA43k=)

  main:
    - title: Identity and Authorization Technology
      prompts:
      - prompt: Describe your experience with authorization systems, specifically Open Policy Agent and OAuth.
        answers:
        - response: |
            I have extensive, varied experience when it comes to authorization and authentication systems.
            I will give some brief bullet points of my experiences over time with various authorization systems, and then I will give some more detailed examples of my experience with Open Policy Agent and OAuth.

            * The earliest "exotic" auth system I encountered in my career was a strange one called [OACC](http://oaccframework.org). It is a open-source system created by a colleague, and I remember it to be something like RBAC, with some permission delegation mechanism.

            * Next, I worked for Unicon and used [Shiboboleth](https://www.shibboleth.net/) (a SAML-based identity provider)

            * After this, I began work on the open-source Internet of Things platform "Meshblu", and the closed source application that ran on top of it called "Octoblu". Octoblu was later acquired by Citrix.

            #### Experience with Meshblu
            Meshblu is capable of configuring and messaging hardware or software "devices" hardware being things like hue bulbs, or DIY Arduino projects, and software being things like web applications. My team inherited Meshblu with a [permission system](https://meshblu.readme.io/reference/whitelists) that was baked directly in to the json document that represented the configuration of the device. This system was a simple one - arrays of concrete uuids that are allowed to perform certain actions. It was only capable of dealing with concrete entities - no roles, groups, etc. My colleagues and I eventually replaced it with a [more flexible system](https://meshblu.readme.io/reference/whitelists-2-0) that is still very simple and easy to understand.

            In short, it is a 2-dimensional "permission matrix", where the first dimension is the actions that can be taken on a device {broadcast, discover, configure, message}, and the second dimension usually has 2-3 permissions that were relevant to the action domain. Critically, we added an 'as' permission, allowed for impersonation, which makes groups possible without being baked in to the system.

            On top of this, we have many microservices that exist in "userland" with respect to Meshblu; they services could implement different authorization systems on top of Meshblu if necessary or desired. For example, we had a microservice that translated between OAuth and our permission system.

            We had many authn/authz systems of our own to do this permission translation. For example, a microcontroller may need to authenticate via MQTT, the protocol it is using to communicate with Meshblu. Often communications happened over http, and we allowed for basic auth if necessary. Since we had many protocols, I ended up having doing this translation a lot.

            We had many microservices that would "assume the identity" of a device, and bridge the gap between Meshblu and other services, which often required the use of OAuth. In some cases, I remember having to do the "OAuth handshake" between 4 different oauth providers (including at least 1 of ours), though the details escape me at the moment.

        - response: |
              Below describes some of my experience  with authentication and authorization systems. I will describe the architecture of an experimental cluster design to illustrate my knowledge and expertise in this problem domain. I know this is probably overkill, but bear with me!

        - response: |
            #### The "Totally Naive Services" Cluster & Open Policy Agent Experience
            I later designed an experimental cluster to be used in environments with complex and unknown-to-us security requirements, potentially including governmental or military applications. The cluster would be deployed in both single-tenant, on-prem environments and multitenant cloud environments. The cluster design factored out multitenancy with respect to the services it contained; e.g. the underlying services did not have a concept of organizations, or really even users (from a security standpoint). I used Open Policy Agent to the point of absurdity when it came to this cluster design, with the expectation that we'd eventually hit a wall of impracticality after which the services inside the cluster would have to become more complex as they became more aware of their environment.

            This approach to cluster development allowed us to build the cluster in parallel to the teams developing the services, and the development roadmaps of these services could be organized such that there was no wasted code if some of the more novel ideas did not work out.

            _aside: I wrote a wrote a Javascript poc that used OPA's ability to return a partially-solved abstract syntax tree from a policy, and then execute it using function composition with [Ramda](https://ramdajs.com/). Should you do this? No. But it was interesting!_

            #### Cluster Design from a Security Standpoint
            Seeing how I now have a captive audience (dear reader), I'll use the cluster as an object lesson in how I design things while doing research, and also try  to minimize risk during the development of production code.
            I am recalling this from memory, and recreating the diagrams, so I may be wrong about some of the details.

            One of the goals of this architecture was to separate all authentication and authorization concerns from the services themselves, and end up with a "naive service", which I will here define as:

            1. trusts that actions requested to be performed on a resource are allowed.
            1. trusts that any information requested is allowed to be returned.

        - response: |
            At the highest level, the cluster is divided into 3 concentric circles of trust:

        - response: |
            1. No trust (outside world)
            1. Authentication trust (within cluster)
            1. Authorization trust. "Naive" (within pod)

          figure:
            figure: naive-cluster-overview
            miro: '3458764548628056239'

        - response: |
            The barriers between these circles were enforced by OPA.
            Outside of the authentication barrier, we are in a no-trust environment.
            Once we pass through the authentication barrier, communications between the services can trust that the request was from who it says it is.

        - response: |
            #### Authentication Barrier
            An OPA service was installed alongside Envoy at the ingress gateway of the cluster. OPA would evaluate the request after obtaining information necessary to reason about authentication legitimacy. This information could be provided as a result of an OIDC workflow, some kind of Active Directory authentication, or a variety of other authorization methods we supported via an adapter pattern described [later in this document](#authorization-adapters). This policy could also be a passthrough, and assume whatever security handshake that happened earlier in the request flow validated user identity correctly.

            _note: the authorization data in the diagram is separated logically from the request, but the information in practice would likely come from the request._
          figure:
            figure: authentication-barrier
            miro: '3458764548747400083'
        - response: |
            #### Authentication Workflow
            This workflow diagram llustrates the flow of data through the authentication barrier. Note the 'Envoy' nodes represent the same Envoy service on the edge of the cluster.

          figure:
            figure: authentication-workflow
            miro: '3458764548844659047'

        - response: |
            In the initial implementation, I used Envoy to insert an "Identity" header into the request after the request was validated through whatever mechanism the client wanted to use. We offered a Keycloak instance as an IdP  (Identity Provider) with OIDC capabilities, but we also would accept OIDC credentials from a trusted 3rd party; or many other authentication mechanisms - we followed an adapter pattern for normalizing auth requirements and data, which I talk about [later on](#authorization-adapters).

            Regardless of the mechanism used, in the end an OPA policy always gated access to the cluster. The client's authentication data was evaluated against OPA policies, which had the final say. This, like all autoinjected systems in the cluster, it could be opted out of via configuration.

            This implementation, however, has a potential vulnerability in that we would need to scrub an "Identity" header off of the incoming request. Since all requests were coming in through the ingress controller that Envoy was manipulating the headers from, in theory this wouldn't be a problem - but I would have rather encapsulated the request in an envelope larger than the request itself. I had learned with Meshblu that separation between data and metadata is very important, especially with regard to security.

            In addition, this header could be manipulated by a malicious service within the cluster. The header solution was good enough for a proof of concept, but further research would be needed to create an envelope for the request that sufficiently separated "userland" requests and cluster metadata. Anyway, that's a mini self-audit of this implementation.

            _Since writing the statements above, I've been reading up a little on Kafka. Kafka seems to have the metadata/data separation that I was looking for. It's streaming nature would probably make the authz egress filter I talk about later achievable with less friction than http. We might be able to map requests to responses better as well. Anyway. I probably should learn more._

            Past this barrier, the services could trust that the request was from who it said it was, but they could not (yet) trust that the request was authorized to do what it was trying to do.

        - response: |
            #### Authorization Barrier
            After the authentication barrier is passed, any subsequent OPA policy evaluation can assume that the identity presented to the policy for evaluation is accurate, which allows for simple, decoupled policies to govern authorization on a per-service pod basis. You may notice below the similarity between the cluster-protecting authentication barrier, and this authorization barrier.

        - response: |
            An OPA service was injected into each Kubernetes pod, alongside Envoy. Work was being done on embedding OPA as a plugin to Envoy (also injected by default in Istio).
            Also by default, the OPA service would only do the authorization gating on ingress traffic - though evolutions of this design allowed for OPA to filter egress traffic as well. But we'll get to that later.
          figure:
            figure: authorization-barrier
            miro: '3458764548747400083'
        - response: |
            #### Authorization Workflow
            This diagram shows the flow of data through the authorization barrier. Note that all 'Envoy' nodes refer to the same service.

            At this point, the request is coming from inside the cluster, with the rewritten identity header.
          figure:
            figure: authorization-workflow
            miro: '3458764548844659047'

        - response: |
            Past this point, we have what I'd consider "half" of a naive service: the service can trust that actions requested to be performed on a resource are allowed. If this is as far as I could take the cluster, it still provides a lot of value. But the service still needs to do some logic as to what a particular user can see, which is unsatisfying. But let's take a little break from these barriers, go depth-first, and talk about the origin of the authz/n information.

        - response: |
            #### Authorization Adapters
            One of the goals of the cluster experiment was to support existing security infrastructure transparently. OPA was used as the only gate for both trust boundaries, but the clients may need information from their existing IdP (identity provider) in order to reason about access control. This entails using data or existing rules from Active Directory, Okta, or various other IdPs within the logic of a policy.

        - response: |
           I accomplished this by making several microservices that used the adapter pattern to ETL (extract,transform,load) any information necessary for policy evaluation from various IdPs to OPA's internal, ephermal datastore.

           I preferred this method to OPA policies doing http requests to our adapters, as I feel doing http requests and parsing the results make policies much more difficult to understand, as the policy now has to know and deal with implementation details, which defeats the purpose of having a restricted policy language.
          figure:
            figure: identity-provider-data-to-opa
            miro: '3458764548738949056'

        - response: |
              #### Delegating Authorization Decision-Making
              OPA policies can be designed to proxy the authn/z decision making to an existing security service. For example, for a naive implementation, I made a microservice that could query Okta's decision-making engine and treat Okta's response as authoritative. A simple http service internal to the cluster that takes data from the post body, queries Okta, and returns 200s if Okta says things are ok, 400s otherwise.

              In a production system, I would use/make a plugin for OPA that abstracts the communication to an external identity provider behind a simple construct accessible to the Rego language, keeping the policy language again unaware of the specifics of http. This would work similarly to the [algorithm primitives I injected in to a line of Python](#describe-any-experience-working-with-startups--what-did-you-draw-from-that-experience-that-would-be-relevant-for-this-application-) I talk about later on. But this effort could be done in parallel to others working on the cluster, with the security model held constant. The http parsing would just be removed from the policy later and that interim microservice removed. In the meantime, if OPA speaks rest, and I can make a microservice that speaks rest and Okta, we can close the loop early.

              Through this, enterprise customers could keep all their rules for authentication and authorization in Okta, which they may already be using for other purposes. Keycloak has a similar mechanism, with a different scheme when it comes to rule execution. It's a common feature of identity providers.

              The eventual removal of http parsing from the policy is a good example of how the cluster can bear the burden of transporting security information and allow the policies to more closely resemble their "platonic ideal": a simple, declarative statement of what is allowed and what is not. The cluster attempts to factor everything else out, continuing to separate responsibilities.


        - response: |
            #### Authorization Egress Filters
            _warning: we are entering the area in which performance could be a concern. Using the OPA service is a good way to reason about this approach, but other methods can be used to accomplish the same effect. An example of this is [generating SQL from OPA policies](https://blog.openpolicyagent.org/write-policy-in-opa-enforce-policy-in-sql-d9d24db93bf4), but I am not a fan of this as it requires services to have special knowledge re: both OPA and the database. But still, it's better than nothing!_


            You may have noticed an asymmetry in the [authorization barrier](#authorization-barrier). While the request to perform an action is gated by an OPA policy, the response (egress from the pod) is not. As a result, a service within a pod may need to make assumptions regarding the security model.

        - response: |
            One way to think about policies is that they are 'gates' that control access to a resource. This is the most common use case of an OPA policy.
        - figure:
            figure: opa-policy-gating-a-single-resource
            miro: '3458764548858771011'
        - response: |
           We can, however, use the same gating policy over the set of all resources. This returns all the resources a user is allowed to see, based on the 'gating' policy via [partial evaluation](https://blog.openpolicyagent.org/partial-evaluation-162750eaf422)

        - figure:
            figure: opa-policy-filtering-resources
            miro: '3458764548861602424'

        - response: |
            _this would have implications when it comes to pagination. A streaming model with backpressure would support this kind of design. Is that what Kafka is? I'll have to look into that. Envoy has a filter for Kafka, and Envoy also has a plugin for gating request via opa policies._
        - response: |
            With the addition of this filtering, the authorization barrier is now symmetric, with OPA both gating access to resources and filtering the response. At this point the pod architecture allows for a service to be fully naive.
          figure:
            figure: authorization-barrier-with-filter
            miro: '3458764548866168330'

        - response: |
            We can even run this policy as a filter on a different microservice, as long as enough variables are applied via partial evaluation. This is a bit trickier though, and this answer is already ludicrously long. But as a hint: this microservice + selected policy would filtered __again__ by the egress filter on the microservice pod.
        - response:
          figure:
            figure: opa-policy-filtering-other-domains
            miro: '3458764548863225249'
        - response: |
            Finally, the OPA policy that filters responses doesn't have to be the same policy that gates the requests. They can be totally unrelated. I reused the gate as a filter in the above examples to demonstrate how much mileage you can get out of a single policy using this architecture.
        - response: |
            #### Ok, well hopefully that was enough
            I hope that the above description of my experimental cluster design is enough to demonstrate some of my security engineering experience - particularly with regards to OPA. I plan on referring to this answer in subsequent prompts.

            I hope somebody took the time to read this, and that it was at least somewhat understandable. It was a lot of work recreating this from memory! Thanks for giving me a soapbox.


            I am obviously interested in doing more research in this area.

      - prompt: Describe your experience integrating OpenID Connect providers or using OpenID Connect libraries in your projects.
        answers:
        - response: |
            I both consumed OIDC responses from external identity providers, and run Keycloak as an OIDC provider in the cluster described above. Besides that, I can't think offhand of other work I've done regarding OIDC except to study the workflow in order to build the cluster.
      - prompt: Describe your experience with container technologies such as Docker, LXD and Kubernetes.
        answers:
        - response: |
            I have used Docker extensively throughout my personal and professional life. I'm running a Minecraft server, a duckdns image, and a Plex media server on this machine as I type this! I also have Podman, but ironically I have to run Podman with sudo, and I can run Docker without. Which, come to think of it, is a pretty big security vulnerability on my home machine...

            I talk in more depth about my professional Docker experience [in the devops section](#describe-your-experience-with-large-scale-it-operations--saas--or-other-running-services--in-a-devops-or-is-or-system-administration-capacity) of this document.

            I have a few personal repos that at least demonstrate my interest in Docker that I might as well list off:
             - [here's a repo](https://github.com/redaphid/docker-compose-ecs-test) of me messing around with Docker Compose deploying to AWS! There is almost literally nothing in it, but if you're curious you can go through the commits and see what I was trying out.
             - [here's an older repo](https://github.com/redaphid/jad) of me using Docker in various ways on a home server. In the `services/plex` directory you can see me do a little fanciness where I encrypt my Plex server preferences so I don't leak my token and can still have a public repo.
             - [here's an older repo](https://github.com/redaphid/docker-ntp-server) Where I was running a very accurate ntp server in this effort to make "indoor gps" via ultrasonic emitters and detectors.

              _tl;dr: I think it was 3 Raspberry Pi's that each had their own ultrasonic frequency. I was trying to triangulate the position of a device in a room that had an ultrasonic detector. It didn't work. But I learned a lot about how bats use ultrasonic frequencies to navigate!_

            It looks like after a while I started using ZFS and relied on automatic snapshots to keep my Docker configs at home.


            As you probably can imagine from the cluster experiment, I am at least somewhat familiar with Kubernetes. To be honest I usually have to spin up and look in to using Kubernetes whenever I get my hands dirty with the copius YAML config. Kubernetes is the pragmatic choice for all enterprise development right now.

            Regarding LXD: I just looked in to it, and it sounds pretty awesome! I haven't tried it, but it sounds like it might be helpful for me.

            I (like any normal person) run a gaming vm inside my Linux box. Just because I don't want to have a "real" machine in my house that runs windows.

            ```
            <username>@mortal ~> sudo virsh list --all
            Id   Name          State
            ------------------------------
            1    nx3           running
            2    bf3           running
            3    win7-2009     running
            -    bt2_testbed   shut off
            -    face          shut off
            -    face2         shut off

            ```
            The little I just read about LXD sounds like it might be a rabbithole I should avoid until I fill out the rest of this document.

        - response:
    - title: General Software Engineering Experience
      prompts:
      - prompt: What kinds of software projects have you worked on before? Which operating systems, development environments, languages, databases?
        answers:
        - response: |
            I pride myself in being a generalist. To that effect, I have worked on a huge variety of different systems, languages, etc. ranging from microcontrollers, to the open-source Internet of Things platform [Meshblu](https://github.com/octoblu) I talk about a little [here](#experience-with-meshblu), to giant fancy clusters that you may remember me describing in a previous response :).

            I helped out with developing a new  closed-source browser recently, which was interesting. [Here](https://github.com/loqwai/experiments-messageport-react) is a repo where I experiment with distributing state via Message Ports to be rendered with React. Electron has some crazy way of fusing node-side and browser-side MessagePorts together, which is important, because when making a browser you have to deal with a lot of different processes, and you really need to have a structured way for them to communicate with each other.


            I like working on platforms when I get the chance - though this is no means required. Working at Octoblu on the IoT platform and the flow-based execution engine that ran on top of it was by far the most satisfying work of my career so far.

            I have a feeling I'll need to talk about Octoblu and Meshblu a lot more further on in this document, so I won't go in to this any more right now. But [here](https://www.youtube.com/watch?v=g9sLUn_lPaQ&list=PLugWVgJZBNjY_jtZBLZnQmtwr402bC6We) is a YouTube playlist featuring the flow execution engine if you're interested in seeing it in action.

            As far as languages go, I've used a ton of them. I'm a language nerd. I soapbox about Lisp when I see an opening in conversations.

            These days I'm doing a lot of Typescript, but:

            - [here](https://github.com/loqwai/juniper-gardens-twig-debug)'s a repo where I helped an open source project by fixing a bug in their implementation of the ESPHome protocol that allows esp32 microcontrollers to get wifi credentials over BLE. (Javascript. On a microcontroller. This was not my idea)
            - [here](https://github.com/loqwai/extra-color-perception)'s a repo where I make hardware pendants that each glow a different color, and that color changes depending on how physically close you are to someone else wearing a pendant with a different color. (C++ on esp32s, using platform.io).
            - [here](https://github.com/loqwai/carbonaria)'s a repo where I learn Rust by making a game with my friend!
            - [here](https://github.com/redaphid/fish-shell-stuff)'s a repo of all my Fish shell functions I clone down when I set up a new machine or VM.
            - [here](https://github.com/redaphid/resume-canonical)'s the repo I used to generate this document. Mostly using Javascript, HTML, Github Actions, and madness.

            I've used Go somewhat recently for professional, closed-source work. I'm catching up with Python to learn more about the latest AI craze. I coded in Java and C# in the distant past, when I didn't know any better.

            As far as databases are concerned, I've used Firebase, Postgres, and MongoDB recently-ish. If Redis counts, I've done some of that. But it probably doesn't.

            The particulars of a given database haven't affected my career much so far. I try to get to the point where I'm abstracting the details as soon as possible, and rarely think about it again unless some problem crops up.

            Regarding development environments, I'm all-in on vscode and Linux. I bought Red Hat 6.0 at a Gamestop in high school by saving up my lunch money. I had to buy a Macbook for the aforementioned browser project, and I'll use it occasionally because the hardware is fantastic. Please someone update the Linux kernel to support this hardware! I want out of Mac-land.

      - prompt: Would you describe yourself as a high quality coder? Why?
        draft: primary
        answers:
        - response: |
            Yes, I would describe myself as a high quality coder.
            I think it all boils down to philosophy, with some experience mixed in.

            #### Philosophy
            I have a syncretic coding philosophy that I've developed over the years, and love to talk about. Not much of my philosophy is novel, but I steal a lot from others. To wit:
            * Unix Philosophy
            * Worse is better
            * The Zen of Python (with the exception of 'flat is better than nested')
            * Pragmatic Programmer
            * Some of Uncle Bob's Clean code.
            * Test Driven Development
              - I have a game that makes this fun, I swear!
            * Strong opinions, weakly held.
            * Some of "xtreme programming"
            * "premature optimization is the root of all evil"
            * Ping-Pong pairing.
            * "The best code is no code at all"
            * The "[Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise)" as a heuristic for code complexity.
            * The Principle of Least Surprise
            * "Branching by Abstraction"
            * "Make it work, make it right, make it fast"

            ...and many others.

            I simplify interfaces exposed to whatever is consuming my code as much as possible, exposing the minimal surface area necessary for it to be functional. I try to use the simplest data structure for inputs and outputs.

            When it comes to code complexity, I believe "There are either obviously no errors, or no obvious errors". Defects hide in complexity. Keep your code as simple as possible - but no simpler.

            I apply the scientific method to my coding philosophy, and allow it to evolve over time, based on new information and experiences.

            I try to use the Socratic Method whenever possible during architectural or procedural debates.

            I really value collaboration. My brain probably can't come up with the best solution to a problem on its own. I like to bounce ideas off of other people.

            I like finding out I was wrong about something! I enjoy well-intentioned critique of my work, and am happy to do it for others as well. I foster a culture where people "like it when they are wrong", and are open and eager to hear criticism. Because how else can we evolve?

            I feel a major part of our job is the fight against entropy, and finding the simplest way to accomplish our goals.

            I think critically about the solution I am developing, and focus on developing what we actually need by checking my assumptions regularly.

            I commit constantly, as you can see in any repo I've ever worked on. I make the features I'm working on as small and atomic as possible.

            I've mentioned a few times that I pride myself in being a generalist. And one of the reasons for this is I think it's good for you. It keeps the mind flexible, and just knowing what's out there in our huge field of work gives you more "tools" in your mental toolbox. I don't have to remember how to do a binary search, but I know that it exists, and what it's for. Knowing things like that helps you to design things better.
            For example, learning about Rust's "match arms" and "match guards" has caused me to think about switch statements and guard clauses in other languages in a different way. I know if I see a problem that looks like something a match arm would make easier, there might be a development pattern I can use to imitate them, even if I'm not in Rust.

            #### The Game
            The first thing that always comes to mind when I think about my good developer habits is this "game" I play that was originally for pairing. But I "play it" with myself when I can't pair, assuming both roles.

            The game goes like this:
            1. "Member 1" of the pair writes the simplest possible test that fails, given the current codebase.
            1. "Member 2" of the pair writes the simplest possible code that will cause the test to pass - even if it's a function that returns a constant present in the test!
            1. The roles in the pair are reversed, and the loop continues.

            As a heuristic for what constitutes the "simplest possible code", I use the [Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise), moving from simplest->most complex code on this scale, increasing in complexity only when forced by a test to do so.


      - prompt:  Would you describe yourself as an architect of resilient software? If so, why, and in which sorts of applications?
        answers:
        - response: |
            Yes, I would describe myself as an architect of resilient software.

            The statements re: being a [high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder?-why?) apply here. I'm guessing that is a given, but I figured I'd refer to it just in case.

            One way characterize architectural resiliency is by how well the system recovers from failure. And failures will happen.

            The domain in which I deal with the importance of resiliency the most is (perhaps obviously) cloud services. But I think the principles I refer to below apply to any system.

            One of the things I do is paradoxically make sure a system crashes out early if some critical expectation is not met. By making things crash as soon as possible, you an expectation that things may be crashing all the time, which causes resiliency to be a concern from the beginning.

            Detecting failures and replaying any actions attempted during the failure helps - as long as the actions don't have side effects that can't be undone.

            Things like blue/green deployments are great for resiliency, along with easy rollbacks to previous versions of subsets of the system.

            I'm excited by the idea of [Principles of Chaos](http://principlesofchaos.org/).

            Things like [circuit breakers](https://martinfowler.com/bliki/CircuitBreaker.html) help.

            I think of resiliency not as a "feature" of a system, but rather a property of the system. It's not easy to add to a system after the fact. It's something you have to design into the system from the beginning.

      - prompt:  What is your most senior role in a software engineering organisation? Describe your span of control, and the diversity of products, functions and teams you led.
        answers:
        - response: |
            I was Staff at Citrix for 3 years, during which time I would sometimes manage multiple teams in order to accomplish a goal. At Octoblu (startup acquired by Citrix), I co-architected and managed a few different initiatives - the major ones being the Meshblu 2.0 permission system referenced earlier [here](#experience-with-meshblu), and the "Flow Engine" - a flow-based execution engine that ran on top of our open source, free-to-use IoT platform Meshblu. We deployed the flows at large scale, using redis queues and workers, taking advantage of any asynchronicity we could to make the system as fast as possible. In the beginning, the system used a Docker container for each flow, but as you can imagine, this did not scale.

            These were difficult, highly technical problems to solve, and we were executed them very effectively, in a short timeframe.

            I also assumed some leadership roles for the front end of the flow system, in which people could drag-and-drop nodes representing devices and wire them together.

            I later was team lead for an R&D focused team within Magic Leap, which had a lot fewer responsibilities, but was still a lot of fun.

      - prompt: What is your proudest success as an engineering leader?
        answers:
          - response: |
              I'm sounding like a broken record now, but I am extremely proud of the work I did on the Meshblu platform. It's architecture and design is great, the system was functional and processing 4 million messages a day. Translating the configuration and messaging systems of hundreds of types of devices and services into one \*ahem\* **canonical** system and having the system work well was a huge success.
              The flow engine was a very technical challenge, and the whole team working on it were very effective at executing on it. It was a great experience.

      - prompt: Outline your thoughts on open source software development. What is important to get right in open source projects? What open source projects have you worked on? Have you been an open source maintainer, on which projects, and what was your role?
        answers:
          - response: |
              As I imagine most people working at Canonical would say, I think open source software is one of the most powerful forces in the world today. It's a manifestation of the very human impulse to share and help each other. It's a complex mind meld that is growing and evolving under it's own volition. Humanity's collective ability to think and help, manifest.

              I know they're doing it for probably most developers, but when I received an email that my code was in the [Arctic Code Vault](https://archiveprogram.github.com/arctic-vault/) my eyes watered.

              I've talked about [Meshblu](#experience-with-meshblu) a lot, but it was a really fun, challenging, and varied project to work on. As for my role as a maintainer, Octoblu's usual pseudo-leaderless organization makes my 'role' in developing it hard to qualify. I pushed straight to master on any of the 1.2k repos we were working on whenever I finished something that wouldn't break the code. But everyone had that freedom.

              That said, this was my job. Many people used Meshblu, but few contributed to it outside of those paid to do so. But at least a few adapters were made by members of the community to bring some weird device or service we didn't support into the platform. We had engagement in hackathons for hobbyist developers. And tons of throughput from people using it. And all of those were satisfying.


      - prompt: Describe your experience building large systems with many services - web front ends, REST APIs, data stores, event processing and other kinds of integration between components. What are the key things to think about in regard to architecture, maintainability, and reliability in these large systems?
        answers:
          - response: |
              Meshblu, again, is a great example of my experience with this. We had hundreds of **unique** services, translating millions of messages between many protocols, to many devices and services. Most of these services were themselves using Meshblu to handle requests, responses, configuration, and data storage, because they appeared to be a "device" within the system. This allowed us to delegate all event processing, data storage, and integrations into Meshblu itself.

              Though not everyone is fortunate enough to have this sort of infrastructure built in in a cloud-spanning way, the principles of the system are the same. The services should be as simple and decoupled as possible, exposing a small, sensible API. We should strive to delegate as much of this complexity as possible to the environment around the service. I'm starting to notice a theme in my responses here.

              I don't believe in forcing uniformity upon any single service in the system as long as it is small - and services should be, unless there is a good reason to make them larger.
              Rather, we should make the path to uniformity so simple and easy that people want to walk it. Libraries, generators, templates, guides, and things of this nature should be provided to make service development as easy as possible - but not forced upon anyone. This is an evolutionary approach to making good tools that I find to be very effective.

              Small services that have other responsibilities pushed as far out to the edge as possible are easy to maintain, and easy to replace. The heuristic I mentioned earlier that software either "obviously has no bugs" or "has no obvious bugs" is a good one when thinking about maintainability and reliability.

              Architectures of large systems should provide environments that help the services within push those responsibilities out. I don't know if you remember the [way too long](#the-totally-naive-services-cluster--open-policy-agent-experience) description of my experimental cluster architecture earlier, but I think it's a good example of how to think about responsibility delegation in a large system.


      - prompt:  How comprehensive would you say your knowledge of a Linux distribution is, from the kernel up? How familiar are you with low-level system architecture, runtimes and Linux distro packaging? How have you gained this knowledge?
        answers:
        - response: |
            This is an interesting thing to try to quantify.
            I think the best way I can answer this is via a few personal anecdotes, and some reflection as to what that means.

            I first used Linux when I bought Red Hat 6.0 from a Gamestop by saving my lunch money in high school. It didn't work for teenage me. I couldn't get the installation to finish. I had to retreat.

            Freshman year in college I decided to get Gentoo (stage 2) running on an old P4 with 128mb of ram, because I was trying to use it as a DVR, and needed all the performance I could get. It compiled for easily over 12 hours. The beeping from the speaker in the motherboard all night I'm sure haunts my then-roommate to this day. But I'm sure all the effort I put in to removing all printing capabilities from all the software on that distro via some kind of flag system emerge used was "worth it". And it worked.

            During this time I had a laptop whose cd-rom had failed, and the bios wouldn't detect the hard drive. I used Puppy Linux on a floppy to get the laptop to a usable state. And I used it quite a bit.

            I've tried to use Alpine as a "hypervisor" that just ran virsh and everything else would be a container, or VM, but that failed because Alpine is a huge pain with it's state management when it's not in a container. So I returned, once again, to Ubuntu. And that machine has been fairly problem-free.

            I tried Void Linux close to 2 years ago, because I wanted to use a "bootloader" that allowed me to select from different ZFS datasets to use as the root filesystem for the machine. I wanted to use [ZFSBootMenu](https://docs.zfsbootmenu.org/en/latest/) to do this, and the tutorials all used Void. I got this to work, but it turns out systemd is pretty important these days, and I had trouble using Docker (well, Rancher) with it. Also the Docker daemon hates ZFS.

            I know, for example, that ZFSBootmenu is not an "actual bootloader", but a Linux kernel that later (I believe) `kexecs` somehow to replace itself. Unfortunately I was trying to use VFIO to pass through a GPU to a VM and it didn't work. I had suspicions it had something to do with this. So I used Ubuntu again, and that's where the VM is hosted to this day.

            I've accidentally been in the initrd environment a few times, and was confused and amazed by what was going on. Then did research in to how it works. I think it loads the binary with this minimal set of tools directly in to ram before continuing to start the system? I have messed around there to try to save machines in the past.

            I've been curious re: musl, but sadly think it'll never catch on.

            I like the "everything is a file" stuff. I've dd'd my own hard drives on a few occasions.

            I have a use an old machine in my home network as a bastion host for ssh'ing into my VM server. Running Ubuntu, of course.

            I like the "everything is a file" system, and yearn for an alternate timeline where Plan 9 became the dominant OS.

            I used Warty Warthog!

            I'm not sure how these anecdotes roll up to a quantitative measure of my Linux knowledge, but I think you get the gist.

      - prompt: Describe your experience with large-scale IT operations, SAAS, or other running services, in a devops or IS or system administration capacity
        answers:
          - response: |
              I talk about large-scale SaaS operations a bit [in the Meshblu section](#experience-with-meshblu), but I wouldn't characterize myself as having system administration responsibilities.

              When it comes to devops, I've set up a couple of complicated workflows in [Concourse](https://concourse-ci.org/) and simpler ones in [Github Actions](https://github.com/features/actions). In both cases, I have used Docker-in-Docker images [like this one for Concourse](https://hub.docker.com/r/cycloid/concourse-dind). I use this to spin up whatever is being tested in docker-compose, as well as any relevant supporting software, using "real" Docker images whenever possible. The whole thing runs inside the CI environment, for both unit and integration testing.

              I have a practice of making what I call "integration-style" tests. These are a fusion of unit tests with some integration testing. Set up correctly, it's often __easier__ for developers to write more integration-y tests than it is to mock things - as long as you are disciplined enough to not store state via volumes or the like. I also have strategies in which a developer can run the system in question while storing state if they find that helpful, and also have docker-compose spin up an ephermal one for testing.

              This is a little in the weeds, but I usually have the service being tested run inside docker-compose without exposing a port, with it's own set of supporting services. A developer can run the service they are working on and maintain state if they'd like, while the testing instance of the service runs in the background with their own set of supporting services, without port or state conflicts. Depending on how resource intensive these tests are, it is often possible to run them on saving a file. Once a service has started in docker-compose it's pretty easy to restore it to a stateless state. For performance reasons, you might want to be able to interrupt currently running tests on save as well. This makes run-on-save "integration-ish" testing easier on the system.

              In all instances in which I've used this method of testing, my team has been able to use the same `docker-compose.yml` on both their local machines and CI. This method also helps to avoid mocking when possible. I believe it's a good practice to work with "real" systems during development and testing whenever possible.

              I used to evangelize Docker Swarm because the homogeneity of configuration could be extended to deploys as well. I have a mostly-empty repo that is a poc of deploying swarm config to AWS [here](https://github.com/redaphid/docker-compose-ecs-test).

      - prompt: Describe your experience with public cloud based operations - how well do you understand large-scale public cloud estate management and developer experience?
        answers:
          - response: |
              I am somewhat experienced when it comes to public cloud operations. I've used AWS off and on for a long time, but I do have to refresh my memory on how to do certain things whenever I touch it. But things change so fast, it's worth reading up on it every time I use it.

              I know about reducing the surface area of attack for the "cloud estate" at a theoretical level, and have done it before in the less-managed Amazon resources (e.g. EC2 clusters).

      - prompt: Describe your experience with enterprise infrastructure and application management, either as a user running enterprise operations, or as a vendor targeting the enterprise market
        answers:
        - response: |
            I talk about my understanding and experience when it comes to some enterprise infrastructure while talking about the cluster designed to integrate with enterprise infrastructure [here](#the-totally-naive-services-cluster--open-policy-agent-experience).
            More specifically, when I talk about [making authorization adapters](#authorization-adapters) and [delegating authority](#delegating-authorization-decision-making).

      - prompt: Outline your thoughts on quality in software development. What practices are most effective to drive improvements in quality?
        answers:
        - response: |
            Hm. If we're talking about how to write quality code, I've answered the question pretty thoroughly [when talking about why I think I'm a high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder--why-). This must be a different kind of quality.

             If we're talking about improving code quality at a team level, I usually focus on collaboration and enthusiasm. Coding is fun! I have an open discussion with whoever is interested re: heuristics when it comes to "quality code". We can agree or disagree about them, and revisit them from time to time. But we at least know they exist, and have a lexicon for them.

            A culture of collective ownership when it comes to code is important, so people don't end up in silos, disconnected from the rest of the project.


      - prompt: Outline your thoughts on documentation in large software projects. What practices should teams follow? What are great examples of open source docs?
        answers:
          - response: |
                When it comes to documentation, my first concern is making sure they don't go out of sync with the code. It's difficult to keep documentation up to date, and it can be difficult to tell if they are out of date. I've been learning some Rust lately, and the concept of a "doctest" might help keep the documentation and code in sync. Python has something similar. If a language has features that incentivize keeping documentation and code in sync, these features should be used because they will likely be the most friction-free way to get started.

                These features alone don't guarantee that the __intention__ of the code is documented, though.
                Having a culture that celebrates documentation is probably the most effective way to keep the docs up to date, and accurate. Perhaps a project generator that generates a markdown file next to code files, or to make a team's practice to write documentation in markdown files in the same directory as the code they are documenting.
                I'm imagining a scenario similar to recent code organization philosophy of "code that changes together, lives together", and adding documentation to the mix.
                For example, a team might have a directory structure like this:

                ```
                /src/components/
                  /IceCream
                    IceCream.ts
                    IceCream.test.ts
                    IceCream.css
                    IceCream.md
                  /Tamale
                    Tamale.ts
                    Tamale.test.ts
                    Tamale.css
                    Tamale.md
                ```

                This __could__ be enforce by CI. As a developer, I'm not sure how happy that would make me.

                It would also be interesting to have a warning in CI if the code changed, but the documentation didn't. Not all code changes require documentation changes, but making something nag the developer about the doc file not being modified might be enough to get them to think about it. A balance must be struck here that would involve some experimentation.

                As a developer, One of the things that makes me dread updating documentation is when it is a huge, monolithic series of intimidating files. Breaking down documentation into small, atomic pieces like this would make things more approachable. You would need higher-level documentation as you move up the abstraction ladder, but perhaps a similar idea could be used recursively.

                As far as good examples of open source docs, Arch Linux's [wiki](https://wiki.archlinux.org/) is famously amazing when it comes to anything Linux. I suspect their culture is one which celebrates documentation.

                I've been using an [Entity Component System](https://en.wikipedia.org/wiki/Entity_component_system) library called [Bevy](https://bevyengine.org/) for a while now, and The "[Bevy Unofficial Cheatbook](https://bevy-cheatbook.github.io/)" is fantastic. I read it just for fun. And that's saying a lot.


      - prompt: Outline your thoughts on user experience, usability and design in software. How do you lead teams to deliver outstanding user experience?
        answers:
          - response: |
              I took a few design classes in my graduate studies, but I wouldn't consider myself an expert in user experience. I do, however, have a few "tools in the toolbox" that can guide me along.

              The first thing I think about when I think about design is what I took away from reading Klaus Krippendorf's "[The Semantic Turn](https://www.amazon.com/Semantic-Turn-New-Foundation-Design/dp/0415322200)". I won't pretend I've read all of it, but the concept of "affordances" made such an impact on me that it is the first thing I think about when I think about design.

              My understanding of affordances is that they are "properties of an object that broadcast what can be done with it, and how". To that effect, I think the primary consideration when designing user experiences is to make the affordances of the system clear.

              This applies to humans and machines alike.

              Of course, in practice, this cannot be the only concern. We can't have a UI that's just a series of labeled components in a big, gray list. Style is important, and much more than affordances are communicated with a good design.

              Having a consistent design language is nice, for graphic design as well as UX. They are both striving to convey information in non-verbal ways. I like to think about design as a language, and I think it's important to have a consistent vocabulary. But it also can't be like "newspeak" from 1984. We need some variation to communicate nuance.

              Another source that springs to mind every time I think about design is [The Humane Interface](https://www.amazon.com/Humane-Interface-Directions-Designing-Interactive/dp/0201379376), by Jef Raskin. I read this book long after it came out, but the principles are still relevant. I am famous amongst coworkers for being very against modals, because this book made such a strong argument.

              I also like the idea of a ZUI ("Zoomable User Interface") from that book a lot, and I find keeping that idea in mind can inform some design decisions - even if there is no obvious "zooming" involved. And also universal undo/redoing.

              I was passionate enough about these principles that I led the design of Octoblu's visual editor for the Flow Engine so we would follow these 3 principles, as well as many others. You can see an example of how the UI works [here](https://youtu.be/g9sLUn_lPaQ?t=267). You may notice the lack of modals during configuration, the infinite, zooming canvas, and the universal undo/redo visible in the upper right in this video.

              I use these principles to make design decisions. But more than that, I emphasize the usage of principles in general when it comes to leading graphic design. As is a repeating theme throughout this document, I like to have a few heuristics to use as metrics for how "usable" a design is. And the heuristics themselves must evolve over time, and - following the scientific method - be open to revision.

              I'd imagine someone who is interested in communication via design would take something like a style guide in to account. Say, read up on the colors, tints, font weights, and visual hierarchies. Try to understand the information this design pattern is trying to communicate.

              And then make a ton of new, unique diagrams using that style guide's color scheme and visual hierarchy. Maybe create a whole new document writing system to experiment with theming documents in the spirit of said style guide while still allowing for exceptions to the theme via modification of the code. Spend time thinking about how to communicate at least __some__ information in that document visually, and semantically. Accidentally cause the semantics of \<em\>(emphasis) tags to become ambiguous, and then _not be able to use emphasis in markdown without looking like asides now._

              But who am I kidding - no one is interested enough about design to put all that effort in to a written interview.

      - prompt: Outline your thoughts on performance in software engineering. How do you ensure that your product is fast?
        answers:
          - response: |
              It must be measured. And we have to define what "fast enough" is. According to Donald Knuth, "Premature optimization is the root of all evil".

              Often people will see suboptimal things like nested `for(` loops, and make the assumption that they are the cause of poor performance. But these loops may often run a few times. A colleague of mine once told me that "the average loop in code runs like 6 times". I don't know if that's true, and searching for "average loop iterations computer science" looks like a chore to go through. But it's an interesting thought to consider while analyzing the code by hand. If it's an extremely inefficient loop that runs 6 times, I'd much rather we use the slower, more semantically accurate code if those 6 interations aren't very complex.

              But more importantly, you shouldn't be doing that optimizing by hand if you can help it. Write the code that is easy to understand first. Premature optimization is the root of all evil. Then, if you have a performance problem, profile it. Find the bottleneck. Then optimize that. Don't optimize the code you haven't written yet, unless it's fairly easy to do.

              I tell a story [later on](#what-did-you-achieve-at-university-that-you-consider-exceptional-) about how I got an automatic A in assembly class without writing any assembly. This is because people misunderstood how to be performant in assembly, vs a C optimizing compiler that's specific to the CPU architecture you are targeting. By writing C code, I was able to explain to the compiler what I wanted to achieve semantically, and the compiler knew how to optimize for that specific CPU better than most humans could. The compiler knew about fancy and obscure assembly instructions and how to use them. Computers are good at memorizing how to do stuff like that. I talk about the usefullness of abstractions like this some more when [discussing startups I have worked with](#describe-any-experience-working-with-startups--what-did-you-draw-from-that-experience-that-would-be-relevant-for-this-application-)

              That said: does performance matter? Yes. It can become an issue. In my opinion you should mitigate risk of performance issues the same way you mitigate risk with everything in software: contain things in very small, well-defined interfaces. If you've done things right, you should be able to replace the implementation of the interface with a faster one without the rest of the code knowing about it. I do this even in microcontroller work where performance is often an issue.

              If we're dealing with something that can horizontally scale, then that is a great way to create a stopgap while working on a more performant solution, which again reduces performance risk.

              I also ascribe to the whole "Make it work, make it right, make it fast" mantra.

      - prompt: Outline your thoughts on security in software engineering. How do you lead your engineers to improve their security posture and awareness?
        answers:
        - response: |
            The ideas I've been reading reading about in the DevSecOps-land have a lot of merit. I like thinking about security as a developer - to the surprise of no one reading this, I'm sure. But I also like developing a product, from beginning to end. Learning how to make secure software shouldn't be much different than learning REST. To extend the file system metaphor mentioned in [the documentation section](#outline-your-thoughts-on-documentation-in-large-software-projects--what-practices-should-teams-follow--what-are-great-examples-of-open-source-docs-), We could imagine the filesystem for a project looked like this:
            ```
            /src/components/
              /IceCream
                IceCream.ts
                IceCream.test.ts
                IceCream.css
                IceCream.md
                IceCream.rego
              /Tamale
                Tamale.ts
                Tamale.test.ts
                Tamale.css
                Tamale.md
                Tamale.rego
            ```
            This is a pretty unlikely folder structure in real life, but I think it represents the point I'm trying to get across.
            Developers working on the project would need to have a culture of celebrating security, in the same way they would need a culture of celebrating documentation. I am enthusiastic when I come up with an interesting insight or 'better' way to design or code something and am excited to talk about it. My fellow developers are the same. I think instilling a culture that extended this sort of attitude to documentation and security would help with both of these problems.

            Besides that, the usual education and training applies and would be helpful. It would be best if this was done in a fun and engaging way, and didn't feel like a series of unskippable videos.


      - prompt: Outline your thoughts on devops and devsecops. Which practices are effective, and which are overrated?
        answers:
        - response: |
            I talk about my thoughts regarding devops quite a bit  in the [Describe your experience with large-scale IT operations, SAAS, or other running services, in a devops or IS or system administration capacity](#describe-your-experience-with-large-scale-it-operations--saas--or-other-running-services--in-a-devops-or-is-or-system-administration-capacity) section of this document.
            #### Automated Systems
            In preparing to answer this question, I did a little research in to CI pipelinse for devsecops. I am concerned that this sort of automatic exploit-detecting pipeline would throw tons of warnings everywhere at first,which is understandable. It would take a long time - maybe in the order of months? - to configure things in such a way that it doesn't snow the team with warnings. But it stands a chance of being worth it in the long run.

            This reminds me of a problem I encounter often in C++ projects, where the compilation step will produce so many warnings an errors flying by the screen that the developers are blind to any actual issues. I call them "Ok Errors", and it's bad to acclimate the team to them.

            In both cases, a lot of intentional work needs to be done, specifically, to curate these warnings to the point where they merit attention.

            #### Organization and Culture
            I really like the idea of "shift left". In our industry we repeatedly learn that we should slice vertically and not horizontally. I'm a full-stack developer. I like to traverse stack strata. Having what would normally be called a "security team" fold in to the development team seems like a good idea. The traditional separation can be a communication barrier, and often feels like an "us versus them" situation.

    - title: Industry leadership experience
      prompts:
      - prompt: Describe your speaking experience at industry events and conferences
        answers:
        - response: |
            I've spoken at a few conferences, and ran quite a few hackathons. I think my biggest talk was at NodeConf, where I spoke negatively about Kubernetes. haha. I have since conceded the victory to Kubernetes.

            I was on the local speaking circuit a bit w/regards to IoT, and ran a few workshops sponsored by Intel to get the word out about their IoT offerings.

            Personally, I would teach Javascript in a monthly meetup, and go to local hackerspaces to teach some people to use the tools I was using, and they taught me theirs. I would also go to local meetups and talk about the things I was working on. I also taught kids to program a couple of times, using [Processing](https://processing.org/).

      - prompt: Are you a thought leader in any particular area of technology?
        answers:
        - response: |
            I have a lot of thoughts about software development practices, but I don't tell many people about them, and I don't know how unique they are. tl;dr "no".
      - prompt: Describe any experience working with startups. What did you draw from that experience that would be relevant for this application?
        answers:
        - response: |
            I've worked at a few startups, and in general I enjoy the fast pace and ability to make big decisions quickly. I like the ability to make a big impact on the company, and to have the creative freedom to come up with solutions to problems. There's a kind of trust implicit in startup culture. The lack of red tape and focus on people over process is nice.

            I'll go into a little more depth on the first one than I usually would, because what were doing actually informs some of the ideas I have re: enabling the clear, succinct writing of OPA Policies in Rego.

            I first worked at a startup created by one of my advisors in my graduate studies at the University of Hawaii. We had developed code for the U.S. Navy with the intention of using social graph and semantic analysis to trace the origins of extremist ideas. Or, as I used to call it, "finding terrorists on the internet". He and a few others in that department wanted to bring the technology to the private sector to do trend analysis and prediction, and so I worked on that with them for a while. It failed to gain traction, which is unsurprising. But it was very fun.

            At a high level, we had an interesting problem in which we had a lot of different algorithms related to social graphs we could use to surface properties; we could trace the similarity of the content of the message, or trace our way back in to find "supernodes"; people who seemingly amplified the messages or attitudes we were trying to track down. We could also do things like use Google's original PageRank algorithm to rank the "importance" of a person or message online in terms of links back to it. We also had a simple-ish semantic analysis algorithm that could purportedly categorize the "emotion" of a message, along with the strength of that emotion. This might be important when it comes to tracing the origins of extremist ideas.

            We also had a web crawler that needed to decide which link to follow next which crawling the web for this content. But what algorithms should we use? And how should they be weighted against each other?
            My solution was to expose these algorithms to look like simple data structures within a Python script, and let the user (usually academics) decide how to weight them. We had a single textbox that the user could use to type in a single line of Python. We used Jython, as the rest of the application was in Java, and injected objects that represented the algorithms. This effectively created a domain-specific language for the Terrorist Crawler's search heuristic. The "code" specified by the user looked something like this:

            ```inlinks/10 + (angerLevel * 100) + outlinks + pagerank```

            The web crawler would then evaluate that line of Python and use it as a way of scoring which links were the most promsing ones to follow in it's quest. This made the academics very happy, because they could tweak those basic statements, let the crawler run for a while, and see where it went and what it came back with.

            I'd like to explore a similar solution with OPA, in which authn/z data would be exposed as a simple entity in the language, allowing the policy to read clearly without having to care about the implementation details of a request.
            Imagine a scenario in which you could write a policy like this:

            ```rego
              default false
              allow = true {
                some person in sales
                person.id = identity
              }
            ```
            _Note: this is likely not valid Rego; it's been a bit since I've written it. Hopefully this gets the point across._

            In both of these cases, the user is able to convey their intention clearly in a focused, constrained DSL. I'm interested in doing this sort of thing with OPA, and I think it follows the philosophy of how it should be used.
            In OPA's case, we have a few options when it comes to representing that data to a policy. We could use the [ETL approach I described earlier](#authorization-adapters), or perhaps utilize OPA's plugin system. Or a combination of the two.

            Since we're talking about it here, I have been thinking about whether we should prevent OPA from being able to modify a message that passes through it - or to call external services via REST or the like.
            We may be able to further constrain policies to make it very difficult to leak auth data.

            Besides this I worked with the startup "Octoblu" I've mentioned earlier, and details about it's product Meshblu in the [Meshblu section](#experience-with-meshblu). I refer to it [elsewhere](#describe-your-experience-building-large-systems-with-many-services---web-front-ends--rest-apis--data-stores--event-processing-and-other-kinds-of-integration-between-components--what-are-the-key-things-to-think-about-in-regard-to-architecture--maintainability--and-reliability-in-these-large-systems-) in this document to illustrate my thoughts and experience when it comes to scaling and distributed software design.

            I also worked with a medical startup, and worked a little on "Computes", a decentralized computation platform bought by Magic Leap.

      - prompt: Describe any experience working in a public company. What is important for your colleagues to know about being a public company?
        answers:
          - response: |
              I worked at a public company for a few years. In my mind, "large company" and "public company" are somewhat intertwined. But thinking about it more, I did have some experiences that were specific to public companies.

              Not long after Octoblu was acquired by Citrix, Elliott Management bought a large stake in the company. As part of their restructuring they let go of their CEO Mark Templeton. Mark is an interesting kind of guy: the type that would wear fun, expressive semi-formal clothes while speaking at conferences. He was excited about new technologies, and encouraged some experimentation within the company. He really liked out our open source IoT platform after watching some of our demos. I'm sure he was the reason we were acquired.

              His demeanor was in stark contrast to Elliott Management, a [famously ruthless activist investment company](https://www.newyorker.com/magazine/2018/08/27/paul-singer-doomsday-investor). Elliott Management is the kind of company that would buy defaulted debt at a huge discount from countries with fragile economies, then engage in legal battles at the __soverign nation__ level to force the governments to pay the debt in full.

              Elliott Management focused on getting Citrix's stock price up quickly, and the most obvious strategy to do so is to cut divisions of the company that weren't profitable. Surprisingly, Octoblu survived these initial cost-cutting measures. Perhaps because we were acquired so recently. But our mission dramatically changed. We were no longer a startup, and we were no longer part of a company that valued experimentation. Instead we were to develop "smart conference rooms". This was still very interesting work. But our relationship with the company changed. Our developer evangelist was instructed "not  to tweet about robots" anymore. I felt like we were stuffed in the back room of a closet somewhere, to be wheeled out during conferences to show off the fun things we could do.

              It was a surreal experience to have your stock value go up as your parent company became hostile to your mission.

              This is the only experience I can think of in which I can differentiate between "large company" and "public company". That said, one experience doesn't equal a trend. I'd be open to working for a public company again.

    - title: Education
      prompts:
      - prompt: How did you fare in high school mathematics, physical sciences and computing? Which were strengths and which most enjoyable? How did you rank, competitively, in them?
        answers:
        - response: |
            I was a good student in high school. I was an 'A' student in physical sciences. They were fun! I wasn't at the time a fan of math; I was too busy writing calculator games on my ti-89 (I was fancy) in Basic in class. My family had a kind of 'anti-math' bias. I was taught at the time that it was useless. I have since became pretty frustrated about this, and read up on math theory in Wikipedia sometimes.

            I was, however, very good at programming.

            The computer science curriculum in high school coincidentally began my freshman year. Well, they had "Basic Computer Training" before then.
            I was under the mistaken impression I had to take this class to graduate. A classmate overheard me complaining about this, and told me to take "Visual Basic" instead. I wish I could let them know what a difference that made in my life.

            I was part of the first generation to take computer science classes in the school. There weren't many of us. But we were all there because we wanted to be.

            I don't know if I ever got anything less than 100% on any assignment. It was so much fun. I had arms races with my friends as to who could make the most annoying program. I learned DirectX in VB6. I made programs to "spy" on the library computers. I couldn't get enough. I took literally every CS class offered in the school, every year. As electives.


      - prompt: What sort of high school student were you? Outside of class, what were your interests and hobbies? What would your high school peers remember you for, if we asked them?
        answers:
        - response: |
            I was a good student in high school - people thought of me as an academic. I was extremely in to computers, and so I went to LAN parties a lot with my fellow techies. I liked paintball. I took Wing Chun Kung Fu. I worked as a programmer after school at a place I [talk about later](#describe-some-high-school-achievements-considered-exceptional-by-peers-and-staff). I went to punk rock shows. I liked cars.

            My high school peers remember me as a funny, good-natured, intelligent person who occasionally wrote very deep poetry in his notebooks. And obviously, yes, they remember me as a computer nerd. But they remember my attitude first, which I appreciate.
      - prompt: In languages and the arts at high school, what were your strongest subjects and how did you rank in those among your school peers?
        answers:
        - response: |
            I did well in language arts in high school. I was picked to be the "Best at English" for the yearbook my graduation year, and have been able to brag about it at parties ever since. I got 5s in my AP English classes, and my teacher in Honors English actually remembered me when I ran into her years later.
            I was an 'A' student in anything language arts. But if "Best at English" in a high school yearbook means anything, then I was the best at English.

            (Honestly, though: This was chosen by the English teachers. I was actually the top student with regards to English).
      - prompt: Describe some high school achievements considered exceptional by peers and staff
        answers:
        - response: |
            Did I mention I was the best at English?

            The English teachers really were pretty impressed by me. As were, of course, the Computer Science teachers. They employed me to do networking in classrooms and help various staff members with their computers.

            I was recruited by a subcontractor to Honeywell who was working on flight management systems for Airbus A320 aircraft and looking for high school kids he could pay $11/hr to to do some of his work for him when I was 16. This was awesome.
            The staff were pretty impressed, as were my friends and parents. I had a great time, learned a lot, and made a lot of mistakes that I hope were caught by somebody.

      - prompt: Which university courses did you perform best at? How did you rank in your degree?
        answers:
        - response: |
            I did the best in computer science. I went to a liberal arts school, but I think I took every CS class they offered. It is a small school, and I bet the teachers remember me. I insisted on taking 3d graphics my first semester freshman year, and the school was flexible enough to allow me to do it. It was pretty rough going in to that class without taking any algorithm or datastructure classes yet, but I had been programming for fun and work long enough that I did pretty well.

             I don't think there were 'rankings' for my degree.
      - prompt: Outside of class, what were your interests and where did you spend your time?
        answers:
        - response: |
            I spent a lot of time driving around, getting lost in temperate rainforests, and - obviously at this point - messing with computers.
            My school had a program in which they would give you a \*house\* instead of a dorm room if you had an academic theme and put on a few events related to that theme per semester.

            The program was similar to the fraternity/sorority system.

            My friends and I founded "Robothouse", with 8 of us living in a smallish 2-story house, with me in the basement. We had a permanent LAN party, and sometimes big ones that would fill the house when friends would stop by.

            I also worked part-time for the school, writing software that interfaced with smart routers via SNMP. This was during the age when viruses were running wild in Windows-world. All the students on campus were connected to the same LAN, and were all pirating software for years. When the semester started, thousands of these computers would plug in at once and viruses spread like wildfire.

            My job was to work on this software that would scan each computer when they signed up to use the network for the first time using Nessus. If Nessus found any critical vulnerabilities, we'd lock their computer in to a vlan where they could only access Windows Update (and maybe a few other services). Then they could update, try again, and the cycle would potentially repeat.

      - prompt: What did you achieve at university that you consider exceptional?
        answers:
        - response: |
            I took the hardest possible computer science classes first, then wrapped around and took the 1-200 levels to finish my degree when I needed a break. Founding Robothouse was pretty cool, and it went on for a couple of years after I graduated. Which made me pretty happy.

            I received an automatic 'A' in assembly class, without writing much assembly. My professor was making a homemade image manipulation program, and one of the class's assignments was to write the algorithm for some subset of that program. I think it was color normalization for my year.

            There was a competition where, if your algorithm was faster than the teacher's, you would get an automatic 'A'. So I made a faster algorithm in C, using tons of compiler flags, and optimization levels for the particular CPU we were running it on. I got an 'A' without writing too much
            assembly.

            I then made a header file and a compiler script, and sent it to everyone in the class, saying they could use it if they kept my name in the comments of the header

      - prompt: At high school or university, what leadership roles did you take on?
        answers:
        - response: |
            Besides founding Robothouse, nothing readily comes to mind. I preferred organizational structures that didn't require an explicit 'leader' role. I liked to have leadership be an emergent property of the group, rather than something that is explicitly assigned.
    - title: Context
      prompts:
        - prompt: Outline your thoughts on the mission of Canonical. What is it about the company's purpose and goals which is most appealing to you? What do you see as risky or unappealing?
          answers:
          - response: |
              Ubuntu's mission statement aligns closely with my own, and feels like an extension of the classic open source software movement. I haven't really thought about localization or accessibility as part of the open source movement, and it's interesting to think about. Ubuntu's mission even more of an evangelical twist to it when taking these under consideration. I literally can't find anything I disagree with.

              That said, I'm trying to find out of Canonical's is different in some way. If it isn't, the corporate mission statement is hard to find.
              But I'd imagine it's pretty similar, with the addition of the 2 mantras on Canonical's landing page: "deliver, maintain, secure and sustain" and "drive down infrastructure cost". I'm aligned with these as well!

              When it comes to risk, obviously security is a major one - and I'm not just saying that because of the role I'm applying for! If 'enterprise' customers are paying for Canonical to maintain, secure, and sustain infrastructure, then there is major reputational risk if security is breached - even if it's not Canonical's fault. If only there were some experimental cluster that can make strong guarantees about security in distributed systems...

              _Joking above. I can't make those "strong guarantees". Just felt fun to put in._

               When it comes to driving down costs or increasing performance, there is something to be said about a higher-tech or experimental culture that a company like Canonical can provide that pure algorithmic muscle can't. In order to make a huge quantitative increase in performance, something qualitative has to have changed under the hood. Those optimizing C compilers I talked about in the [personal achievements in college](#what-did-you-achieve-at-university-that-you-consider-exceptional-) and later as an anecdote in the [performance section](#outline-your-thoughts-on-performance-in-software-engineering--how-do-you-ensure-that-your-product-is-fast-) have been around for a long, long time. I feel that the anecdote can apply to performance in general.

               But this could be investigated in parallel to an initiative to increase performace by more traditional means.

               I don't see anything unappealing about any of this!


        - prompt: Who are Canonical's key competitors, and how should Canonical set about winning?
          answers:
          - response: |
              The obvious answer, as always, is Microsoft. They position themselves in the same space as Canonical according to Canonical's landing page. Microsoft has historically been trusted by enterprise clients vs. open source, and they have tons of money to spend on marketing and R&D. They also have a huge installed base of Windows users, and a lot of those users are in the enterprise space.

              Canonical can set about winning by being the best at what they do. They can't compete with Microsoft on marketing, but they can compete on performance, security, and reliability. They can also compete on price, but that's a bit of a double-edged sword when it comes to reputation.

              Open source is a huge advantage for Canonical, even if Microsoft is embracing it lately. The entire open source stack is so configurable and flexible by nature that it has many more "dimensions" in solution space, which increases the odds of finding interesting and powerful approaches to problems. Canonical can win not just by doing the best, but by doing the best __differently__.

              Another obvious area to think about is other Linux distributions and their associated professional services. I don't know if anyone comes close to Canonical in this regard. Does Red Hat still have a large market share?

              Amazon may be a competitor if they start offering managed solutions in their cloud that encroach upon Canonical's territory. This would be a tough one to fight out. I know they have their own flavor of Linux now. Canonical could position itself as being more flexible in terms of the underlying infrastructure. Canonical could, for example, have on-prem cluster installations. Or by being the (ahem) __canonical__ professionals when it comes to open source software. Or being more flexible of a solution than this hypothetical Amazon offering.

        - prompt: Why do you most want to work for Canonical?
          answers:
          - response: |
              The first thing that comes to mind, of course, is the ethos of open source that Canonical is built on. The mission statement (for Ubuntu, at least) is very similar to some of my own. I've gone on long, passionate talks about my thoughts on the open source movement and feel strongly about it.

              Canonical is arguably the most influential company in the open source movement, and I'd like to be a part of that.

              Canonical is at the forefront of so many bleeding-edge technologies and that is very exciting to me. I went to graduate school in computer science because I __liked__ learning in depth about interesting concepts in our field.

              In summary: Canonical lives in the intersection of my interests and my values. I'd like to be a part of that.

        - prompt: What would you most want to change about Canonical?
          answers:
          - response: |
              I don't know enough about Canonical to have an opinion on this. I'm too far away from the organization. Now, when it comes to Ubuntu, I have some thoughts...
              :)

        - prompt: What gets you most excited about this role?
          answers:
          - response: |
              I don't know if you've noticed the [huge description of this cluster I worked on](#the-totally-naive-services-cluster--open-policy-agent-experience) where I talk a lot about my interest in the theory of security in distributed systems. I'm clearly in to this stuff :). I am also interested in anything involving research & development or interesting new technology, and it sounds like Canonical is a great opportunity to work on a lot of that stuff.
