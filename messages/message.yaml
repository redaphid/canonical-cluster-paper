doc:
  logo: images/interview-icon.png
  title: Written Interview
  miro: https://miro.com/app/board/uXjVMeJA43k=
  repo: https://github.com/redaphid/resume-canonical
  intro: |
    #### Structure of this Document
    When responding to these prompts, I found that answering them in a discrete fashion was not always the most effective way to communicate experience and expertise in some technical matters. I have therefore organized some responses into descriptions of concrete solutions in my career that utilize the technologies in question. I provide a detailed description porkbarreled in to answering the questions as I encounter them, and refer to these descriptions in subsequent prompts. I have also included some diagrams to help visualize the concepts I am trying to communicate.

    I hope this format makes sense to you, though I imagine I won't be able to clarify anything, due to the anonymity of this process.
  main:
    - title: Identity and Authorization Technology
      prompts:
      - prompt: Describe your experience with authorization systems, specifically Open Policy Agent and OAuth.
        answers:
        - response: |
            I have extensive, varied experience when it comes to authorization and authentication systems.
            I will give some brief bullet points of my experiences over time with various authorization systems, and then I will give some more detailed examples of my experience with Open Policy Agent and OAuth.

            * The earliest "exotic" auth system I encountered in my career was a strange one called [OACC](http://oaccframework.org). It is a open-source system created by a colleague, and I remember it to be something like RBAC, with some permission delegation mechanism.

            * Next, I worked for Unicon and used [Shiboboleth](https://www.shibboleth.net/) (a SAML-based identity provider)

            * After this, I began work on the open-source Internet of Things platform "Meshblu", and the closed source application that ran on top of it called "Octoblu". Octoblu was later acquired by Citrix.

            #### Experience with Meshblu
            Meshblu is capable of configuring and messaging hardware or software "devices" hardware being things like hue bulbs, or DIY Arduino projects, and software being things like web applications. My team inherited Meshblu with a [permission system](https://meshblu.readme.io/reference/whitelists) that was baked directly in to the json document that represented the configuration of the device. This system was a simple one - arrays of concrete uuids that are allowed to perform certain actions. It was only capable of dealing with concrete entities - no roles, groups, etc. My colleagues and I eventually replaced it with a [more flexible system](https://meshblu.readme.io/reference/whitelists-2-0) that is still very simple and easy to understand.

            In short, it is a 2-dimensional "permission matrix", where the first dimension is the actions that can be taken on a device {broadcast, discover, configure, message}, and the second dimension usually has 2-3 permissions that were relevant to the action domain. Critically, we added an 'as' permission, allowed for impersonation, which makes groups possible without being baked in to the system.

            On top of this, we have many microservices that exist in "userland" with respect to Meshblu; they services could implement different authorization systems on top of Meshblu if necessary or desired. For example, we had a microservice that translated between OAuth and our permission system.

            We had many authn/authz systems of our own to do this permission translation. For example, a microcontroller may need to authenticate via MQTT, the protocol it is using to communicate with Meshblu. Often communications happened over http, and we allowed for basic auth if necessary. Since we had many protocols, I ended up having doing this translation a lot.

            We had many microservices that would "assume the identity" of a device, and bridge the gap between Meshblu and other services, which often required the use of OAuth. In some cases, I remember having to do the "OAuth handshake" between 4 different oauth providers (including at least 1 of ours), though the details escape me at the moment.

        - response: |
              Below describes some of my experience  with authentication and authorization systems. I will describe the architecture of an experimental cluster design to illustrate my knowledge and expertise in this problem domain. I know this is probably overkill, but bear with me!

        - response: |
            #### The "Totally Naive Services" cluster & Open Policy Agent experience
            I later designed an experimental cluster to be used in environments with complex and unknown-to-us security requirements, potentially including governmental or military applications. The cluster would be deployed in both single-tenant, on-prem environments and multitenant cloud environments. The cluster design factored out multitenancy with respect to the services it contained; e.g. the underlying services did not have a concept of organizations, or really even users (from a security standpoint). I used Open Policy Agent to the point of absurdity when it came to this cluster design, with the expectation that we'd eventually hit a wall of impracticality after which the services inside the cluster would have to become more complex as they became more aware of their environment.

            This approach to cluster development allowed us to build the cluster in parallel to the teams developing the services, and the development roadmaps of these services could be organized such that there was no wasted code if some of the more novel ideas did not work out.

            _aside: I wrote a wrote a Javascript poc that used OPA's ability to return a partially-solved abstract syntax tree from a policy, and then execute it using function composition with [Ramda](https://ramdajs.com/). Should you do this? No. But it was interesting!_

            #### Cluster Design from a Security Standpoint
            Seeing how I now have a captive audience (dear reader), I'll use the cluster as an object lesson in how I design things when in "Research and Development mode", while minimizing risk during the development of production code.
            I am recalling this from memory, and recreating the diagrams, so I may be wrong about some of the details.

            One of the goals of this architecture was to separate all authentication and authorization concerns from the services themselves, and end up with a "naive service", which I will here define as:

            1. trusts that actions requested to be performed on a resource are allowed.
            1. trusts that any information requested is allowed to be returned.

        - response: |
            At the highest level, the cluster is divided into 3 concentric circles of trust:

        - response: |
            1. No trust (outside world)
            1. Authentication trust (within cluster)
            1. Authorization trust. "Naive" (within pod)

          figure:
            figure: naive-cluster-overview
            miro: '3458764548628056239'

        - response: |
            The barriers between these circles were enforced by OPA.
            Outside of the authentication barrier, we are in a no-trust environment.
            Once we pass through the authentication barrier, communications between the services can trust that the request was from who it says it is.
        - response: |
            #### Authentication Barrier
            An OPA service was installed before Envoy at the ingress gateway of the cluster. OPA would evaluate the request after obtaining information necessary to reason about authentication legitimacy. This information could be provided as a result of an OIDC workflow, some kind of Active Directory authentication, or a variety of other authorization methods we supported via an adapter pattern described [later in this document](#authorization-adapters). This policy could also be a passthrough, and assume whatever security handshake that happened earlier in the request flow validated user identity correctly.

            _note: the authorization data in the diagram is separated logically from the request, but the information in practice would likely come from the request._
          figure:
            figure: authentication-barrier
            miro: '3458764548747400083'
        - response: |
            #### Authentication Workflow
            This workflow diagram llustrates the flow of data through the authentication barrier. Note the 'Envoy' nodes represent the same Envoy service on the edge of the cluster.

          figure:
            figure: authentication-workflow
            miro: '3458764548844659047'

        - response: |
            In the proof of concept, I used Envoy to insert an "Identity" header into the request after the request was validated through whatever mechanism the client wanted to use. We offered a Keycloak instance as an IdP  (Identity Provider) with OIDC capabilities, but we also would accept OIDC credentials from a trusted 3rd party; or many other authentication mechanisms - we followed an adapter pattern for normalizing auth requirements and data, which I talk about [later on](#authorization-adapters).

            Regardless of the mechanism used, in the end an OPA policy always gated access to the cluster. The client's authentication data was evaluated against OPA policies, which had the final say. This, like all autoinjected systems in the cluster, could be opted out of via configuration.

            This implementation, however, has a potential vulnerability in that we would need to scrub an "Identity" header off of the incoming request. Since all requests were coming in through the ingress controller that Envoy was manipulating the headers from, in theory this wouldn't be a problem - but I would have rather encapsulated the request in an envelope larger than the request itself. I had learned with Meshblu that separation between data and metadata is very important, especially with regard to security.

            In addition, this header could be manipulated by a malicious service within the cluster. The header solution was good enough for a proof of concept, but further research would be needed to create an envelope for the request that sufficiently separated "userland" requests and cluster metadata. Anyway, that's a mini self-audit of this implementation.

            Past this barrier, the services could trust that the request was from who it said it was, but they could not (yet) trust that the request was authorized to do what it was trying to do.

        - response: |
            #### Authorization Barrier
            After the authentication barrier is passed, any subsequent OPA policy evaluation can assume that the identity presented to the policy for evaluation is accurate, which allows for simple, decoupled policies to govern authorization on a per-service pod basis. You may notice below the similarity between the cluster-protecting authentication barrier, and this authorization barrier.

        - response: |
            An OPA service was injected into each Kubernetes pod, layered above Envoy. Work was being done on embedding OPA as a plugin to Envoy (also injected by default in Istio).
            Also by default, the OPA service would only do the authorization gating on ingress traffic - though evolutions of this design allowed for OPA to filter egress traffic as well. But we'll get to that later.
          figure:
            figure: authorization-barrier
            miro: '3458764548747400083'
        - response: |
            #### Authorization Workflow
            This diagram shows the flow of data through the authorization barrier. Note that all 'Envoy' nodes refer to the same service.

            At this point, the request is coming from inside the cluster, with the rewritten identity header.
          figure:
            figure: authorization-workflow
            miro: '3458764548844659047'

        - response: |
            Past this point, we have what I'd consider "half" of a naive service: the service can trust that actions requested to be performed on a resource are allowed. If this is as far as I could take the cluster, it still provides a lot of value. But the service still needs to do some logic as to what a particular user can see, which is unsatisfying. But let's take a little break from these barriers, go depth-first, and talk about the origin of the authz/n information.

        - response: |
            #### Authorization Adapters
            One of the goals of the cluster experiment was to support existing security infrastructure transparently. OPA was used as the only gate for both trust boundaries, but the clients may need information from their existing IdP (identity provider) in order to reason about access control. This entails using data or existing rules from Active Directory, Okta, or various other IdPs within the logic of a policy.

        - response: |
           I accomplished this by making several microservices that used the adapter pattern to ETL (extract,transform,load) any information necessary for policy evaluation from various IdPs to OPA's internal, ephermal datastore.

            I preferred this method to OPA policies doing http requests to our adapters, as I feel doing http requests and parsing the results make policies much more difficult to understand, as the policy now has to know and deal with implementation details, which defeats the purpose of having a restricted policy language.

          figure:
            figure: identity-provider-data-to-opa
            miro: '3458764548738949056'

        - response: |
            #### Authorization Egress Filters
            _warning: we are entering the area in which performance could be a concern. Using the OPA service is a good way to reason about this approach, but other methods can be used to accomplish the same effect. An example of this is [generating SQL from OPA policies](https://blog.openpolicyagent.org/write-policy-in-opa-enforce-policy-in-sql-d9d24db93bf4), but I am not a fan of this as it requires services to have special knowledge re: both OPA and the database. But still, it's better than nothing!_


            You may have noticed an asymmetry in the [authorization barrier](#authorization-barrier). While the request to perform an action is gated by an OPA policy, the response (egress from the pod) is not. As a result, a service within a pod may need to make assumptions regarding the security model.

        - response: |
            One way to think about policies is that they are 'gates' that control access to a resource. This is the most common use case of an OPA policy.
        - figure:
            figure: opa-policy-gating-a-single-resource
            miro: '3458764548858771011'
        - response: |
           We can, however, use the same gating policy over the set of all resources. This returns all the resources a user is allowed to see, based on the 'gating' policy via [partial evaluation](https://blog.openpolicyagent.org/partial-evaluation-162750eaf422)

        - figure:
            figure: opa-policy-filtering-resources
            miro: '3458764548861602424'

        - response: |
            _this, of course, has implications when it comes to pagination, but I had some wild ideas that are outside of the scope of the experiment_.
        - response: |
            With the addition of this filtering, the authorization barrier is now symmetric, with OPA both gating access to resources and filtering the response. At this point the pod architecture allows for a service to be fully naive.
          figure:
            figure: authorization-barrier-with-filter
            miro: '3458764548866168330'

        - response: |
            We can even run this policy as a filter on a different microservice, as long as enough variables are applied via partial evaluation. This is a bit trickier though, and this answer is already ludicrously long. Just keep in mind that the resources returned by this microservice + selected policy would filtered __again__ by the egress filter on the microservice pod.
        - response:
          figure:
            figure: opa-policy-filtering-other-domains
            miro: '3458764548863225249'
        - response: |
            Finally, the OPA policy that filters responses doesn't have to be the same policy that gates the requests. They can be totally unrelated. I reused the gate as a filter in the above examples to demonstrate how much mileage you can get out of a single policy using this architecture.
        - response: |
            #### Ok, well hopefully that was enough
            I hope that the above description of my experimental cluster design is enough to demonstrate some of my security engineering experience - particularly with regards to OPA. I plan on referring to this answer in subsequent prompts.

            I hope somebody took the time to read this, and that it was at least somewhat understandable. It was a lot of work recreating this from memory! Thanks for giving me a soapbox.

      - prompt: Describe your experience integrating OpenID Connect providers or using OpenID Connect libraries in your projects.
        answers:
        - response: |
            I both consumed OIDC responses from external identity providers, and run Keycloak as an OIDC provider in the cluster described above. Besides that, I can't think offhand of other work I've done regarding OIDC except to study the workflow in order to build the cluster.
      - prompt: Describe your experience with container technologies such as Docker, LXD and Kubernetes.
        answers:
        - response: |
            I have used Docker extensively throughout my personal and professional life. I'm running a Minecraft server, a duckdns image, and a Plex media server on this machine as I type this! I also have Podman, but ironically I have to run Podman with sudo, and I can run Docker without. Which, come to think of it, is a pretty big security vulnerability on my home machine...

            I often set up development environments for people that use docker compose liberally if we're working on a nontrivial project that requires multiple services. I also use Docker to run tests in CI/CD pipelines. In fact, I often use the same docker-compose.yml file for both development and CI/CD, spinning up the entire stack for testing before deployment.

            I have a few personal repos that at least demonstrate my interest in Docker that I might as well list off:
             - [here's a repo](https://github.com/redaphid/docker-compose-ecs-test) of me messing around with Docker Compose deploying to AWS! There is almost literally nothing in it, but if you're curious you can go through the commits and see what I was trying out.
             - [here's an older repo](https://github.com/redaphid/jad) of me using Docker in various ways on a home server. In the `services/plex` directory you can see me do a little fanciness where I encrypt my Plex server preferences so I don't leak my token and can still have a public repo.
             - [here's an older repo](https://github.com/redaphid/docker-ntp-server) Where I was running a very accurate ntp server in this effort to make "indoor gps" via ultrasonic emitters and detectors.

              _tl;dr: I think it was 3 Raspberry Pi's that each had their own ultrasonic frequency. I was trying to triangulate the position of a device in a room that had an ultrasonic detector. It didn't work. But I learned a lot about how bats use ultrasonic frequencies to navigate!_

            It looks like after a while I started using ZFS and relied on automatic snapshots to keep my Docker configs at home.


            As you probably can imagine from the cluster experiment, I am at least somewhat familiar with Kubernetes. To be honest I usually have to spin up and look in to using Kubernetes whenever I get my hands dirty with the copius YAML config. Kubernetes is the pragmatic choice for all enterprise development right now, but personally I find it to grossly violate the Unix Philosophy with it's complexity. That said, Kubernetes is important. I have used it in the past, and will again. Personal disagreements notwithstanding.

            I prefer Docker Swarm, but I know I'm a minority at this point.

            Regarding LXD: I just looked in to it, and it sounds pretty awesome! I haven't tried it, but it sounds like it might be helpful for me.

            I (like any normal person) run a gaming vm inside my Linux box. Just because I don't want to have a "real" machine in my house that runs windows.

            ```
            <username>@mortal ~> sudo virsh list --all
            Id   Name          State
            ------------------------------
            1    nx3           running
            2    bf3           running
            3    win7-2009     running
            -    bt2_testbed   shut off
            -    face          shut off
            -    face2         shut off

            ```
            The little I just read about LXD sounds like it might be a rabbithole I should avoid until I fill out the rest of this document.

        - response:
    - title: General Software Engineering Experience
      prompts:
      - prompt: What kinds of software projects have you worked on before? Which operating systems, development environments, languages, databases?
        answers:
        - response: |
            I pride myself in being a generalist. To that effect, I have worked on a huge variety of different systems, languages, etc. ranging from microcontrollers, to the open-source Internet of Things platform [Meshblu](https://github.com/octoblu) I talk about a little [here](#experience-with-meshblu), to giant fancy clusters that you may remember me describing in a previous response :).

            I helped out with developing a new  closed-source browser recently, which was interesting. [Here](https://github.com/loqwai/experiments-messageport-react) is a repo where I experiment with distributing state via Message Ports to be rendered with React. Electron has some crazy way of fusing node-side and browser-side MessagePorts together, which is important, because when making a browser you have to deal with a lot of different processes, and you really need to have a structured way for them to communicate with each other.


            I like working on platforms when I get the chance - though this is no means required. Working at Octoblu on the IoT platform and the flow-based execution engine that ran on top of it was by far the most satisfying work of my career so far.

            I have a feeling I'll need to talk about Octoblu and Meshblu a lot more further on in this document, so I won't go in to this any more right now. But [here](https://www.youtube.com/watch?v=g9sLUn_lPaQ&list=PLugWVgJZBNjY_jtZBLZnQmtwr402bC6We) is a YouTube playlist featuring the flow execution engine if you're interested in seeing it in action.

            As far as languages go, I've used a ton of them. I'm a language nerd. I soapbox about Lisp when I see an opening in conversations.

            These days I'm doing a lot of Typescript, but:

            - [here](https://github.com/loqwai/juniper-gardens-twig-debug)'s a repo where I helped an open source project by fixing a bug in their implementation of the ESPHome protocol that allows esp32 microcontrollers to get wifi credentials over BLE. (Javascript. On a microcontroller. This was not my idea)
            - [here](https://github.com/loqwai/extra-color-perception)'s a repo where I make hardware pendants that each glow a different color, and that color changes depending on how physically close you are to someone else wearing a pendant with a different color. (C++ on esp32s, using platform.io).
            - [here](https://github.com/loqwai/carbonaria)'s a repo where I learn Rust by making a game with my friend!
            - [here](https://github.com/redaphid/fish-shell-stuff)'s a repo of all my Fish shell functions I clone down when I set up a new machine or VM.
            - [here](https://github.com/redaphid/resume-canonical)'s the repo I used to generate this document. Mostly using Javascript, HTML, Github Actions, and madness.

            I've used Go somewhat recently for professional, closed-source work. I'm catching up with Python to learn more about the latest AI craze. I coded in Java and C# in the distant past, when I didn't know any better.

            As far as databases are concerned, I've used Firebase, Postgres, and MongoDB recently-ish. If Redis counts, I've done some of that. But it probably doesn't.

            The particulars of a given database haven't affected my career much so far. I try to get to the point where I'm abstracting the details as soon as possible, and rarely think about it again unless some problem crops up.

            Regarding development environments, I'm all-in on vscode and linux. I bought Red Hat 6.0 at a Gamestop in high school by saving up my lunch money. I had to buy a Macbook for the aforementioned browser project, and I'll use it occasionally because the hardware is fantastic. Please someone update the linux kernel to support this hardware! I want out of Mac-land.

      - prompt: Would you describe yourself as a high quality coder? Why?
        answers:
        - response: |
            Yes, I would describe myself as a high quality coder.
            I think it all boils down to philosophy, with some experience mixed in.

            #### Philosophy
            I have a syncretic coding philosophy that I've developed over the years, and love to talk about. Not much of my philosophy is novel, but I steal a lot from others. To wit:
            * Unix Philosophy
            * Worse is better
            * The Zen of Python (with the exception of 'flat is better than nested')
            * Pragmatic Programmer
            * Some of Uncle Bob's Clean code.
            * Test Driven Development
              - I have a game that makes this fun, I swear!
            * Strong opinions, weakly held.
            * Some of "xtreme programming"
            * "premature optimization is the root of all evil"
            * Ping-Pong pairing.
            * "The best code is no code at all"
            * The "[Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise)" as a heuristic for code complexity.

            ...and many others.

            I think about minimizing the "cost of code change" constantly by simplifing interfaces exposed to whatever is consuming my code as much as possible, exposing the bare minimum surface area necessary for the consumer to do its job. I try to use the simplest data structure for inputs and outputs.

            When it comes to code complexity, I believe "There are either obviously no errors, or no obvious errors". Defects hide in complexity. Keep your code as simple as possible - but no simpler.

            I apply the scientific method to my coding philosophy, and allow it to evolve over time, based on new information and experiences.

            I try to use the Socratic Method whenever possible during architectural or procedural debates.

            I really value collaboration. My brain probably can't come up with the best solution to a problem on its own. I like to bounce ideas off of other people.

            I like finding out I was wrong about something! I enjoy well-intentioned critique of my work, and am happy to do it for others as well. I foster a culture where people "like it when they are wrong", and are open and eager to hear criticism. Because how else can we evolve?

            I feel a major part of our job is the fight against entropy, and finding the simplest way to accomplish our goals.

            I think critically about the solution I am developing, and focus on developing what we actually need by checking my assumptions regularly.

            I've mentioned a few times that I pride myself in being a generalist. And one of the reasons for this is I think it's good for you. It keeps the mind flexible, and just knowing what's out there in our huge field of work gives you more "tools" in your mental toolbox. I don't have to remember how to do a binary search, but I know that it exists, and what it's for. Knowing things like that helps you to design things better.
            For example, learning about Rust's "match arms" and "match guards" has caused me to think about switch statements and guard clauses in other languages in a different way. I know if I see a problem that looks like something a match arm would make easier, there might be a development pattern I can use to imitate them, even if I'm not in Rust.

            #### The Game
            The first thing that always comes to mind when I think about my good developer habits is this "game" I play that was originally for pairing. But I "play it" with myself when I can't pair, assuming both roles.

            The game goes like this:
            1. "Member 1" of the pair writes the simplest possible test that fails, given the current codebase.
            1. "Member 2" of the pair writes the simplest possible code that will cause the test to pass - even if it's a function that returns a constant present in the test!
            1. The roles in the pair are reversed, and the loop continues.

            As a heuristic for what constitutes the "simplest possible code", I use the [Transformation Priority Premise](https://en.wikipedia.org/wiki/Transformation_Priority_Premise), moving from simplest->most complex code on this scale, increasing in complexity only when forced by a test to do so.


      - prompt:  Would you describe yourself as an architect of resilient software? If so, why, and in which sorts of applications?
        answers:
        - response: |
            Yes, I would describe myself as an architect of resilient software.

            The statements re: being a [high-quality coder](#would-you-describe-yourself-as-a-high-quality-coder?-why?) apply here. I'm guessing that is a given, but I figured I'd refer to it just in case.

            One way characterize architectural resiliency is by how well the system recovers from failure. And failures will happen.

            The domain in which I deal with the importance of resiliency the most is (perhaps obviously) cloud services. But I think the principles I refer to below apply to any system.

            One of the things I do is paradoxically make sure a system crashes out early if some critical expectation is not met. By making things crash as soon as possible, you an expectation that things may be crashing all the time, which causes resiliency to be a concern from the beginning.

            Detecting failures and replaying any actions attempted during the failure helps - as long as the actions don't have side effects that can't be undone.

            Things like blue/green deployments are great for resiliency, along with easy rollbacks to previous versions of subsets of the system.

            I'm excited by the idea of [Principles of Chaos](http://principlesofchaos.org/).

            Things like [circuit breakers](https://martinfowler.com/bliki/CircuitBreaker.html) help.

            I think of resiliency not as a "feature" of a system, but rather a property of the system. It's not easy to add to a system after the fact. It's something you have to design into the system from the beginning.

      - prompt:  What is your most senior role in a software engineering organisation? Describe your span of control, and the diversity of products, functions and teams you led.
        answers:
        - response: |
            I was Staff at Citrix for 3 years, during which time I would sometimes manage multiple teams in order to accomplish a goal. At Octoblu (startup acquired by Citrix), I co-architected and managed a few different initiatives -  the major ones being the Meshblu 2.0 permission system referenced earlier [here](#experience-with-meshblu), and the "Flow Engine"
      - prompt: What is your proudest success as an engineering leader?
      - prompt: Outline your thoughts on open source software development. What is important to get right in open source projects? What open source projects have you worked on? Have you been an open source maintainer, on which projects, and what was your role?
      - prompt: Describe your experience building large systems with many services - web front ends, REST APIs, data stores, event processing and other kinds of integration between components. What are the key things to think about in regard to architecture, maintainability, and reliability in these large systems?
      - prompt:  How comprehensive would you say your knowledge of a Linux distribution is, from the kernel up? How familiar are you with low-level system architecture, runtimes and Linux distro packaging? How have you gained this knowledge?
      - prompt: Describe your experience with large-scale IT operations, SAAS, or other running services, in a devops or IS or system administration capacity
      - prompt: Describe your experience with public cloud based operations - how well do you understand large-scale public cloud estate management and developer experience?
      - prompt: Describe your experience with enterprise infrastructure and application management, either as a user running enterprise operations, or as a vendor targeting the enterprise market
      - prompt: Outline your thoughts on quality in software development. What practices are most effective to drive improvements in quality?
      - prompt: Outline your thoughts on documentation in large software projects. What practices should teams follow? What are great examples of open source docs?
      - prompt: Outline your thoughts on user experience, usability and design in software. How do you lead teams to deliver outstanding user experience?
      - prompt: Outline your thoughts on performance in software engineering. How do you ensure that your product is fast?
      - prompt: Outline your thoughts on security in software engineering. How do you lead your engineers to improve their security posture and awareness?
      - prompt: Outline your thoughts on devops and devsecops. Which practices are effective, and which are overrated?
    - title: Industry leadership experience
      prompts:
      - prompt: Describe your speaking experience at industry events and conferences
      - prompt: Are you a thought leader in any particular area of technology?
      - prompt: Describe any experience working with startups. What did you draw from that experience that would be relevant for this application?
      - prompt: Describe any experience working in a public company. What is important for your colleagues to know about being a public company?
    - title: Education
      prompts:
      - prompt: How did you fare in high school mathematics, physical sciences and computing? Which were strengths and which most enjoyable? How did you rank, competitively, in them?
      - prompt: What sort of high school student were you? Outside of class, what were your interests and hobbies?  What would your high school peers remember you for, if we asked them?
      - prompt: In languages and the arts at high school, what were your strongest subjects and how did you rank in those among your school peers?
      - prompt: Describe some high school achievements considered exceptional by peers and staff
      - prompt: Which degree and university did you choose, and why?
      - prompt: Which university courses did you perform best at? How did you rank in your degree?
      - prompt: Outside of class, what were your interests and where did you spend your time?
      - prompt: What did you achieve at university that you consider exceptional?
      - prompt: At high school or university, what leadership roles did you take on?
    - title: Context
      prompts:
        - prompt: Outline your thoughts on the mission of Canonical. What is it about the company's purpose and goals which is most appealing to you? What do you see as risky or unappealing?
        - prompt: Who are Canonical's key competitors, and how should Canonical set about winning?
        - prompt: Why do you most want to work for Canonical?
        - prompt: What would you most want to change about Canonical?
        - prompt: What gets you most excited about this role?
